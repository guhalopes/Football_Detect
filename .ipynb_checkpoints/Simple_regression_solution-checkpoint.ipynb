{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QjlEFdXpMgvw"
   },
   "source": [
    "# Deep learning programming I-A: Regression\n",
    "Felix Wiewel, Institute of Signal Processing and System Theory, University of Stuttgart, 23.05.2019\n",
    "\n",
    "## Introduction\n",
    "This programming exercise is the first of a series of exercises, which are intended as a supplement to the theoretical part of the Deep Learning course offered by the ISS. The goal is to introduce you to basic tasks and applications of methods you have encountered in the lecture. After completing the exercise you should be familiar with the basic ideas and one, possibly simple, way of solving the respective task. It is worth mentioning that most of the tasks can be solved in many different, not necessarily deep learning based, ways and the solution presented here is just one of them.\n",
    "\n",
    "## Regression\n",
    "\n",
    "In this exercise we consider the problem of regression, where we are interested in modeling a functional dependence between different variables with, possibly noisy, observations of input-output pairs. Mathematically such a dependence can be formulated as\n",
    "\n",
    "$\\mathbf{y}=f(\\mathbf{x})+\\boldsymbol{\\epsilon}$,\n",
    "\n",
    "where $\\mathbf{y}\\in\\mathbb{R}^{M}$ and $\\mathbf{x}\\in\\mathbb{R}^{N}$ are the input and output observations, $f:\\mathbb{R}^{N}\\rightarrow\\mathbb{R}^{M}$ is the function mapping inputs to outputs and $\\boldsymbol{\\epsilon}\\in\\mathbb{R}^{M}$ is a random vector, which models noise in our observations. Note that this assumes additive noise that only acts on the output and not on the input variable, which might not be true in all practical applications but is a reasonable approximation. For regression we are now interested in estimating the functional relationship $f$ between the inputs and outputs. This can be done in many different ways, not just with neural networks, but for this exercise we focus on approximating this relationship with a neural network $g_{\\boldsymbol{\\theta}}:\\mathbb{R}^{N}\\rightarrow\\mathbb{R}^{M}$ with parameter vector $\\boldsymbol{\\theta}$. The task is now to choose the parameters of the neural network in a way that results in a \"good\" approximation of $f$ with $g_{\\boldsymbol{\\theta}}$.\n",
    "\n",
    "In order to quantify how \"good\" our neural network can approximate $f$, we adopt a probabilistic view. For this we make the assumption that the noise $\\boldsymbol{\\epsilon}$ is a random vector drawn from a known dustribution, which enables us to derive a suitable cost function for training our neural network and also for quantifying a \"good\" approximation.\n",
    "\n",
    "### Mathematical formulation\n",
    "If we assume that the noise $\\boldsymbol{\\epsilon}$ is drawn from a gaussian distribution, e.g. $\\boldsymbol{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\sigma^{2}\\mathbf{I})$, we can use\n",
    "\n",
    "$\\mathbf{y}=g_{\\boldsymbol{\\theta}}(\\mathbf{x})+\\boldsymbol{\\epsilon}\\Rightarrow \\mathbf{y}-g_{\\boldsymbol{\\theta}}(\\mathbf{x})=\\boldsymbol{\\epsilon}$\n",
    "\n",
    "to derive a log likelihood. Since the probability density function (pdf) of a multivariate normal distribution is given by\n",
    "\n",
    "$p(\\mathbf{x})=\\dfrac{1}{\\sqrt{(2\\pi)^{D}\\vert\\mathbf{C}\\vert}}\\mathrm{e}^{-\\dfrac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})\\mathbf{C}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})^{T}}$,\n",
    "\n",
    "we get\n",
    "\n",
    "$\\ln{p(\\boldsymbol{\\epsilon})}=\\ln{\\dfrac{1}{\\sqrt{(2\\pi)^{M}\\sigma^{2}}}\\mathrm{e}^{-\\dfrac{1}{2\\sigma^{2}}\\Vert\\boldsymbol{\\epsilon}\\Vert_{2}^{2}}}=-\\dfrac{1}{2\\sigma^{2}}\\Vert\\boldsymbol{\\epsilon}\\Vert_{2}^{2}-\\dfrac{1}{2}\\ln{(2\\pi)^{M}\\sigma^{2}}$.\n",
    "\n",
    "Replacing $\\boldsymbol{\\epsilon}$ by $\\mathbf{y}-g_{\\boldsymbol{\\theta}}(\\mathbf{x})$ yields the log likelihood for one particular input-output pair:\n",
    "\n",
    "$\\mathcal{L}(\\mathbf{x},\\mathbf{y},\\boldsymbol{\\theta})=\\ln {p(\\mathbf{y}\\vert\\mathbf{x},\\boldsymbol{\\theta})}=-\\dfrac{1}{2\\sigma^{2}}\\Vert\\boldsymbol{\\mathbf{y}-g_{\\boldsymbol{\\theta}}(\\mathbf{x})}\\Vert_{2}^{2}-\\dfrac{1}{2}\\ln{(2\\pi)^{M}\\sigma^{2}}$\n",
    "\n",
    "This log likelihood measures how likely the input-output pair is and we can use it to train our neural network. For this we maximize the expected log likelihood over all input-output pairs under the assumption that the noise is idependent and identically distributed (i.i.d.) over all input-output pairs. This corresponds to finding the parameters $\\boldsymbol{\\theta}^{\\star}$ of our neural network, which maximize the the expected probability for observing the corresponding input-output pairs. Mathematically the optimal parameters for our neural network are given by\n",
    "\n",
    "$\\boldsymbol{\\theta}^{\\star}=\\arg\\max_{\\boldsymbol{\\theta}}\\mathbb{E}\\left[\\mathcal{L}(\\mathbf{x},\\mathbf{y},\\boldsymbol{\\theta})\\right]=\\arg\\max_{\\boldsymbol{\\theta}}\\mathbb{E}\\left[-\\Vert\\boldsymbol{\\mathbf{y}-g_{\\boldsymbol{\\theta}}(\\mathbf{x})}\\Vert_{2}^{2}\\right]\\approx\\arg\\min_{\\boldsymbol{\\theta}}\\dfrac{1}{N_{D}}\\sum_{i=1}^{N_{D}}\\Vert\\boldsymbol{\\mathbf{y}_{i}-g_{\\boldsymbol{\\theta}}(\\mathbf{x}_{i})}\\Vert_{2}^{2}$,\n",
    "\n",
    "where all terms, which are independent of $\\boldsymbol{\\theta}$, are ignored and the expectation operator is approximated by the mean over all $N_{D}$ input-output pairs. In other words we are maximizing the log likelihood by minimizing the mean squared error loss over all input-output pairs in our dataset, hence this approach is called Maximum Likelihood (ML) estimation. For solving this optimization problem and obtaining the optimal network parameters, stochastic gradient descent (SGD) or one of it's many variants is typically used.\n",
    "\n",
    "It is worth noting, that choosing different distributions for the noise $\\boldsymbol{\\epsilon}$ leads to different log likelihoods and therefore different cost functions for training the neural network. Another commonly used distribution for modelling the noise in regression tasks is the laplace distribution. Deriving the log likelihood and the corresponding costfunction leads to the mean absolute error, which is given by the $l_{1}$-norm of the difference between observations predictions of the neural network. This cost function is considered more robust against outliers since these have less influence on the averall loss compared to the mean squared error.\n",
    "\n",
    "###  Implementation\n",
    "\n",
    "In the following we consider a simple regression task, implement a neural network and train it based on the mathematical fomrulation above. For this we first need to create a set of input-output pairs, which then needs to be partitioned into a training, validation and test set. We also define some constants to be used for partitioning the data and the hyperparameters for our neural network.\n",
    "\n",
    "But before we can start, we need to install tensorflow and import the necessary packages tensorflow, numpy and matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49510,
     "status": "ok",
     "timestamp": 1562072556416,
     "user": {
      "displayName": "Felix Wiewel",
      "photoUrl": "https://lh3.googleusercontent.com/-XBgpqQQzeMU/AAAAAAAAAAI/AAAAAAAAAZY/12qzl-EFXfM/s64/photo.jpg",
      "userId": "11509421729312238102"
     },
     "user_tz": -120
    },
    "id": "9JL0d398Mgvz",
    "outputId": "9ee9f778-714d-4a72-8909-4a4a0b8b4b46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu==2.0.0-beta0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/7e/87c4c94686cda7066f52cbca4c344248516490acdd6b258ec6b8a805d956/tensorflow_gpu-2.0.0b0-cp36-cp36m-manylinux1_x86_64.whl (348.8MB)\n",
      "\u001b[K     |████████████████████████████████| 348.9MB 76kB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.16.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (3.7.1)\n",
      "Collecting tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 (from tensorflow-gpu==2.0.0-beta0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/dd/99c47dd007dcf10d63fd895611b063732646f23059c618a373e85019eb0e/tf_estimator_nightly-1.14.0.dev2019060501-py2.py3-none-any.whl (496kB)\n",
      "\u001b[K     |████████████████████████████████| 501kB 41.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (0.2.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.12.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.0.8)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (0.7.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (0.1.7)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (0.8.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.11.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (0.33.4)\n",
      "Collecting tb-nightly<1.14.0a20190604,>=1.14.0a20190603 (from tensorflow-gpu==2.0.0-beta0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/96/571b875cd81dda9d5dfa1422a4f9d749e67c0a8d4f4f0b33a4e5f5f35e27/tb_nightly-1.14.0a20190603-py3-none-any.whl (3.1MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1MB 26.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-beta0) (41.0.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-beta0) (2.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta0) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta0) (0.15.4)\n",
      "Installing collected packages: tf-estimator-nightly, tb-nightly, tensorflow-gpu\n",
      "Successfully installed tb-nightly-1.14.0a20190603 tensorflow-gpu-2.0.0b0 tf-estimator-nightly-1.14.0.dev2019060501\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow-gpu==2.0.0-beta0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CPuVp2lyNK2J"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J84v9uucMgv5"
   },
   "source": [
    "Next we define our constants and set the random seeds of tensorflow and numpy in order to get reproducable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xwC-1OnHMgv7"
   },
   "outputs": [],
   "source": [
    "N_train_samples = 600\n",
    "N_validation_samples = 100\n",
    "N_test_samples = 100\n",
    "N_samples = N_train_samples + N_validation_samples + N_test_samples\n",
    "noise_sig = 0.1\n",
    "N_epochs = 150\n",
    "batch_size = 8\n",
    "learning_rate = 0.01\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ngFFSyG-MgwA"
   },
   "source": [
    "We create $600$ training samples, $100$ validation samples to optimize our hyperparameters and $100$ test samples, which are used to check if our model can generalize to unseen data. Furthermore we set the level of noise added to the observations. For training the model we plan to train it for $150$ epochs with a batch size of $8$ and a learning rate of $0.81$. Next we create the actual signal and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 51149,
     "status": "ok",
     "timestamp": 1562072558082,
     "user": {
      "displayName": "Felix Wiewel",
      "photoUrl": "https://lh3.googleusercontent.com/-XBgpqQQzeMU/AAAAAAAAAAI/AAAAAAAAAZY/12qzl-EFXfM/s64/photo.jpg",
      "userId": "11509421729312238102"
     },
     "user_tz": -120
    },
    "id": "4s8AtsdQMgwB",
    "outputId": "1523233a-6972-4113-aa80-665aff5e6568"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzsnXd4HNXVh987s7uqlqvci1xk3Htv\nGGxjgwE7YBxaKA69ExI+qgMkdBIIhBI6htCJweCGe8Ndcm9yt9ybetndmfv9sUXbZBVL2pV03+eR\nvTtzZ+estPubM+eee46QUqJQKBSK2oUWbgMUCoVCUfUo8VcoFIpaiBJ/hUKhqIUo8VcoFIpaiBJ/\nhUKhqIUo8VcoFIpaiBJ/hUKhqIUo8VcoFIpaiBJ/hUKhqIVYwm1AcTRq1EgmJSWF2wyFQqGoVqxf\nv/6UlDKxpHERK/5JSUmsW7cu3GYoFApFtUIIcaA041TYR6FQKGohSvwVCoWiFqLEX6FQKGohSvwV\nCoWiFqLEX6FQKGohSvwVCoWiFqLEX6FQKGohSvwV5ySrwIFhqlafiurP3pM5/Lb7VLjNiBiU+Ndy\nftl0hHcW7w65L99u0OOZX3l+5vYqtkqhqHgu/scSrv9wdbjNiBiU+FcjpJT8sulIhXri932Zyitz\ndobcl2t3AvDThsMVdj6FItLYczKHb9cdCrcZVY4S/2rE9NTD3PdlKp+s2Fcl5zOl6yIjRJWcTqGo\nNLYeySx235VvLefR7zchZe0Kbyrxr0acyikE4GhmQdC+PLuTPLenXh5CffAdhmebUn9F9Wb8m8uL\n3ZdrNwAodJpVZU5EoMS/GqG5XfBQDkr3Z36l21/nlvu18x1G0DaH+8ugPH9FbSDfHvwdqMko8a+G\nSILV3zAlpZ0KmLZyPyezC/22ZRcE3zU4DLf4l9lChSJyKS68E8oBqsko8a9G+Hr+Ww5n8tWag2V+\njd0ncpj601bu/yrFb3so8ffcBmvK9VfUIIrCmS50zfX5zlOevyLcbDmcybr9Z4K2ezRYSsnlby3n\n8f9tLvNr292CnpHn8NueXeAIGuv1/ENov5SSr9ccpKCWeUuK6o/nc+3BI/4q7KOodByGyYmsAgoc\nBh8u28vB03l++y9/azmT3lsZdJzHA6+MNVcnswuDboft5/D8Z20+xmP/28xbC9Mq3hiFohIJFH+r\n1/Mvf8JEdSRiO3nVRPLtBnl2J33/Pt9v+2cr97Ps0YtLPN4jzuY5UtKklLz2606u7d+aVg1ig/fj\nSd/0F/Q7Pl/P1Mu7MGVYW++2wNtjX45m5gOQW1i7vCVF9WfetuP85ftNNEmIok/r+t5sn7/O2MrM\nB4Z77wRqOsrzryKklHSeOofbpgW3pszMCw65hMLu9lh8Jflsrt0rxABpJ3J4e9Ee7vh8fTF2uP4X\nBE98/S81HYDv1h3ibK79nGGfnEKXl1Qnumz+g91pcixEqqpCUVW8Md91t3o8q5DZW455t+84lk3q\nwbPhMqvKqbXin283cBqVk9f7r/lpPPfzNu/zY5kFtH18FgCpBzOCxpc2iuPxxOdvO+7dNuCF+Qx+\ncaHPGNd72n40iyMZ+QTiFX8RvF4gI8/BgdO5/OX7TfT+2zzvhG8o8c91i398lEv8CxwGK3afKnGh\nzBPTNzPoxQVqrkBR6WQXOELG8Q+H+F54cBiSz1cd4HROYbFjago1Vvx/2nCYtOPZxe7vPHUOd/83\npdj958Pr83fxsc8q3LQTxdsBRYJ8NDOfoS8tLHacR4xP+KRpBoZmHv1+k/fxkBCv5TCL4vi/bj3m\nty/9bL7f6y3ZdcI7NhCP5x/nFv+v1xzkhg9XMz313KUgFmx3Xbjy7Abj3ljKlE/XnnO8QlFeuj/z\nK92fmcu+U7mlPmbnsSye/nELD3+7sRItiwxqrPg/+PUGxry+9Jxj5vl40L6cyCog/WxeyH2Vgcdb\nnp562M8rCazhYy/FCsStR7L8nic9NtMr1J2ens37S/Z69/0vhFDvPpHjfezxmkJFQD2poVbdtdcT\nJ12x+/Q57bPoro+c0zDZcSybhTtOnHO8QnE+OE3JRa8tLvV4T/z/TK7y/Ksl51ujY8ALCxj28qJy\nHeubSeCxQ5SwTMpjrRHgxQcuOimN+IfiSEY+pikpcJjMcXv7mw9nsik9uN7J2Ty797HhDREJZm46\n6ncL7bHlfymHOZFV4I0NhVqA5osns8JeTMjtcEY+Z3PtIfcpFKUlMKOntHgyfmpDmZ8KEX8hxMdC\niBNCiC3F7BdCiDeFELuFEJuEEH0q4rzFUZywVAWZ+UWTtwWO0pVHKHSaZBU4yMj3n/gNjFfajfLF\nyaV0eUClYZGPJ57ltmffqVzu/TKF534pmsfwvN7qfWd49IdNGO7fuVnCeTyef3EXsqEvLQwZrlIo\nysKpcsbs3160p4ItiVwqyvP/FBh3jv2XAsnunzuAdyvovEFk5jm47v1V5xxTmdX7fEM1360/RMcn\nZ3snR891TI9nfuWj5f7VOgscBj9tOMyrc3e4n5fvoiaRpS4D/atPKOzQGf/Q15GMfHIKnczefNQv\nXGOY0nsxMKXr95t2PDvkhcBSgucPtW+ZvaLiOZ1zfnePW49k8eGyvSUPrMZUSJ6/lHKpECLpHEMm\nANOkS3VXCSHqCSGaSSmPVsT5A0kJkVHji68QbjmcSbcWdSvs3L4e9tSftgKw/ei5J3yLI+XgWR7+\nZgOmhEu7NeOHlPRyvY7DKXGaHrGVJJBHPPnEiQLqkIdNODGkhonAiU4mcZyVddh3ysTXP1iy6yQj\nX11M/6T6fq/fNCHa+75nbT5K+8R4Xp+/izsvbMfjl3b2G2txzxEU+lzIFu88wYUdE4PWHigU5eVc\nzoWOQbI4TDx5HJBNOUm9kOP+PnM7tw1vV1kmhp2qWuTVAvDtlpDu3uYn/kKIO3DdGdC6detynSjK\nGnwzczyrgLgoC1+sOsCI5ETaJcZ5913+1nLeu7EP47o1K9f5fttziuNZBfyud0sgOG4Pruyf8vDg\n1xv87CwtNhxM6WRwaNcGkrV0Tn/+AYlxOSy2HaSZOEOUKN26AkMKjtGA/WZT9smm7JStSMlNxnD2\nDxhXdGfhNKX3/a7YfYovVh3gxkFtvGMtmjvs4/PlvOWTtbx1XW+u6Nm81O9RoTgXjhBhxTjyucvy\nMzfq86kvihIb1pgX8LpzEivNriW/rmFyNs9O4zrRFWpvOIioFb5SyveB9wH69etXrthMlMVf/KWU\nDHxhAa0bxHLwTB7/nLeL1KfH+I1JPZjB2K5Nycx3UC/WVqbzXf+Bqy3c73q3xDSld6FUVRFLAd3F\nPnpoe+ip7aGzOEiSOIa+X4LNJeBHCxpyML8hx2Q75pj9OSnrkkMsOTKGXKIpxIqGiYbEgkFdcmkg\nsqknsmkhTtFWHONybRU3igUAFB6IZo21AwvMPsw3+2CYzXGGuOhtOZzFU4e3MDy5EW0aui641hCe\nP5w791qhKCuBKdCdxEHes75OknacWcYA5hr9yKAOXcQBrrcs4Cvb87zrvIKXndfim98mpfS7I330\n+01MTz1M2vOXYtWrd75MVYn/YaCVz/OW7m0VTmDowBOFOeiOX9udZtDkpykli3ed5NZP1vLchJKv\n/sUxa8tR7+rByqKlOMkgbRv9xU56antIFunowvV+DpmJbJFJ/GIOonXH3ry/3cpe2YxCir+gPX15\nF/7mM5EL8MCoZN5cEPg+JC04RR8tjdF1DtDVWM8z1mk8wzQO7etAWuEE6tGRDOoEncNhmK5VzMJn\nwjdg8lo1iVdUJL7ZPt3FXv5re4E8orimcCprZSfvviX05Fv9Ml6I+Yq7C36mLjk84bwNzwXAYUhs\nliJNmbnZFawwTIlVr5r3UllUlfjPAO4TQnwNDAQyKyveH0hRrNuFEMFC4zQl6e6Lw8ZDxbd7CyRw\nQjOU93u+eMR+kLadQdo2WopTAJyR8WwwOzDH7M8Gsz2bzPacIcF73IPNktm+reQL0diuTYLE3+pT\n20QTnguo4DCJHDYT+TlzCHAdbcQxRmvrudJcycX7/sHqKAs/m0P4j/Ny0mRL72tICT2f+xWbrtGn\njSu+Guj5Hzydx8IdodddKBRlxRNWbMFJPrO9RKaM4/f2pzlCo6CxhiWGsf/3FW8/fRP3WmaQLhN5\nx5gIQKHTwGbR2HYki05N63jzsmtCKmiFiL8Q4itgJNBICJEO/BWwAkgp3wNmAZcBu4E84NaKOG9p\nKLAHlG8VIuiCUOg0vbW8y3In9+9Fu/2e142xls9IH4oT+9OyDqvNznwkL2eF0Zk02QJ5jmQtT9mF\nkrBZNNY9NZoDp/O4+t3fAHD4XNTaJcb7Lfzy5YBsykfGeD4yxtNJHORafSGT9SVMilrKPKMP/3BO\nZods7c38txumtxBc4ITcN+sO8U0tbKKtqDg+WLqXkRck0qFxPHd+vp4o7Lxnex0LBn9wPBZS+AFs\nugZC8Krz97QQp/iz5TvWmxewWnbG7jTZdTyby95cxv0Xd/CuYzlXccXqQkVl+1xXwn4J3FsR5yor\ng19a4Pdc0wQzNhzx21bgMHzEv8jrTXpsJp9NGcCFHRNDvvaszf43L+UJXZQk9u+b41lldvGKfaxN\nJ89ZcipkbFTp7kmjLDp1Y6x+C6t8ax61rB9TrPj7skO25hnnLbzhvJqb9HlMscxmlu1xfjCGk3ms\naPJ+82HXnVVpirvZnSYHz+TRoXF8qd6LovbiMEyen7WdtxamMfuhEQD8xfIN3bX9/NH+CPtl8Qkd\nRbF7weOO2+hp28M/bO8ytvBl7IbpXe/iql3lGmko8Y98Arvz6ELw95nb/bYVOkxvbnngRNGPqYdD\niv+inSfYcawohfPbtYdKVeHyXGGcVWaXILEPpLRdtUrr+XsmyC0+tzxdmheFj+qXcQI8gzq8aVzF\np8Yl3Gv5iVv0uYifxnCzPpnPjTGY7vf04uwdJb7WMz9v5cvVB1nz5KgakV2hqDw8399Cp8nOY1n0\nEruZos9hmnMMC8y+3nG6JoKcNJtPkkg+0TziuJv/RT3DPZafsDsvJcricqQy8x3eu1hZA3q913jx\nD6QwhNfs8vyd7v3Bf1W70+TVuTuYMqwtzerGAHDrJ/4FyR79YRP/urZX0LEtOOkV+kHadlppJwF/\nsV9tdmaXbHnOME5ZaV4vplTjbG7R92ThAFzeozn3fZkKQL1Y/1BWo3gbp0qxgCaLeF503sDnxhie\nt3zMs9bPmKiv4M+OO9kjW5R4/JwtR1m911UnKDPPocRfcU4K3E6eTdfIyM7nResHHKM+rzh/7zcu\nxqqTU+ikYZyNfPcdf6zN/y45RXbkB2M4t+mzOHJmH44o1+c1q8DpXSBaEzz/6p2rVA5CRWbyfcI+\nhQGrS6enHqbjU7P5YNk+Br+4kEKn4VfCwZflO4/QQ+zhVn02b1nfZHnUA6yIfpB/2N5jrHUDh6KT\n+avjZsYWvkTfwve4x/EQ04yx7JStmfbHQaWyvzi/3xLQgKJxnSjv41eu7lHs62nu4wLT1q7q7frA\nN6/rfxGxhZgUeXBUctC2Tk1dWT/psjE3O/6PB+330EYc42fbU1yjL6akQtZ3fZHiDcGpRCBFSXg8\n/+xCJ4l7/0dn7RDPOW4iB/+GRtHudUBDOjTyLuBqVjfYUXrZcS1OdLZO+xPTftsP+Hv+KuZfQ/ht\nz2k6NnHFlUuqC5R2PIdJ7/2GwKS1OEEXcYCe2h76aGn02LaX6CjXheGIbECKmcyH5mU888BdJCR2\n5oW3V7AlKyvoNd+6rjfN6hbv2Vp1cc6uWgAxNt2vCbtvGKdF/ZLvAgIvHq9d05OXJ7kuGg3ibDzy\nnavEbSiP5/6LO/CvgNTQC5rW8QmLCX4yh7GysCtvWN/mVev7DNW28LjjNvIp/n17QlwqDVRREh7x\nj6aQjtvfIsWdCReIJ4QTY9W8WW1NElyO0ryHR3grAZ+gPh8Zl3Kf/hNvbFwDtPSrR1VSDavqQI30\n/J++vAtNE8oWJth13DWpGZiCCJBADr3Ebn6vL6LJ8qf4XExlc9RtLIn6E+/a/sWt+hwsGHxhjOYe\n+wMMKniLIYX/5j7Hg3xqjIMmXUHTik0P69GyrvdDGUiLejF8cFM/7/PEhCKPfvUTo3jtmp4ARAck\nHVs1wc/3DWP+n0YU25bu9uFFLRutAYvjNE1g1TWsusbVfYvSNkNdGy0BdwPP/66bt86/Lyeoz42O\nJ/iHYxJXaCv53vYszSi+BHRRz+Lq/0VTVC6eIoi36HNpwhleclwHCF66qjs3DS5aYe7x/KPd4R+A\nJm6tSG5Sh6V/uYgpQ13fi0+c48jHxr2Wn4LOVwO0v2Z6/n8c1pbGdaK4/6vUUo2Pwk4TcZZmnGFA\nTgG99YO0FUdpqx2jrThKQ1E0sevcHc8+WvC9MYJtsg3bzDakyZbnXEjlobgPjJQQZSsS0D+N6UhC\ntIXpqYf56b5hbD9adLfwxR8HeqteNkmIplsL1+RsxybxnPRp8qJrgu4tXTWLjmed8jvftf1bYdEF\nT47v4t1m1UrnB5RGiK26RkyIFTDDkxuxLO0UbxlXsVm25S3rv/kp6mlut/+JjbJD0HiPScrzV5RE\nvsMglgLutPzCQqMXa6SrptTkfq14d0lRpc4Ym8fz19nm/l5d0LRoYWLrhrHedM6zJPCFMZrb9Fm8\nLiZxUDbxjqsJDknNE397Hmz8kvbpp7ld348VAwsGVuGkDnnUFbkkeP/PpZHIpIFPnQ+yACscl/XY\nJ5sx1+jHPtmMvbIZu2ULnph0KXd+UbqLCsD/jStaTVhcNVFDSj/P/wF3DP0WtwfiW7IicCK3U9ME\nPrm1P0kN4ziTW8jV764EimroAPRq5V+46m8TuwXF+C166bKIPEL868MjuOQczXJCrXmY1Lcly9Jc\nF6LFZm+usj/LR9ZX+dr2d+50PMxSs6ffeM88THlrsytqDwUOg9/ri6gvcnjL+Tvvdk0Tfne+nu+Z\nzaJxRY/mLEs7xYCkBn6v1cLnO/aR8zKm6HP4gz6P5503erfXBIek5om/Iw9mPkIXoIuv/gidDDOa\nTBlHFrFkyTh204I1ZieOyoYcpz5HZQOi67dk1elocgkdJy+L8APcOjTJ+zhQ+/u0rkfKwQyirXpQ\nTSJfSkq3vOiCxgC0bVRUsE73EfO4KAudfGLwoWqSeGL+fVqHrnDo4eHRyTzz8za/cwWSbzdCin9g\naCpNtuR39ueYZnuJD62v8aDjPmabA7379550td8rbxMbRe3AaZjk5hfwR8tsVpudSJX+CQi6T3q0\nJx07u8DJ5P6tuKZfy6CSMLcObUv/pAZMnbGVjYdgjtmfyfpi/uG8hgJcYdca4PjXQPGPaQCP7GLR\n7gzu+2YzDiy8fl1/xvdsQa/HZpZ8/Lm7EJYZX6H17XK1YeoYrLpGysGztKgX470rSGoYG/QagemW\nn9zS3xuvDOSJyzrxwqwdQRcTT/z84dEdQx4nhGDmA8No1SD4/AC/3D8MIaBr87reO5LiyLMbNK8X\nPOcSKhR0mrpcZ3+Kj2yv8m/rm/zZcRfTzeF+YwpL8PyPZOSz5XAml3Rtes5xippJhydnM0FbzhW2\nU0x13BK039fzb58Yz+KdJ719fUOVEdc1Qc9W9fj2zkGMe2MZn52+hCuiVjFB/41vjIsAleoZmWga\n1GmC3VaXXGJo3rAu43u60hZ/undo0PCPb+kXtK0i8f3g+d4papogLsrC8GTXAjIhBNOmDODbuwYH\nvUbgB/SiTo2LLX98x4j27H9pfLFhnQsvCL1aGVzCnhAdukRFtxZ16dq8dH0P8uzOkJ5/jC30pHYW\ncdxkf4yVZhdes77HeM2/Gc+tn6zlWGYB369P5+kfg5vFTXx7BXd8vr5UtilqIpLbLLNIM1uwyCxa\na+NJhvD9Dt42vC3N6kZz14XtS3zVKItOn9b1WScvYLvZmhv0+d59NSHmX/PE343Hk+7YpGgyp0fL\nYPFqFF+UPeOb/VIZ+H5gQq0QHNExsdjFTE9c1olnrugScl9pqKq0ycHtGvKHwW1Ch3185jXuv9h/\ngjefaG53PMJ62ZE3rG8zSvMX82/XHeLP323k81UHgl73hHuiuyak3ynKTg+xl+7afj4zLvEulOzT\nuh6T3FlqnrUsNwxsTbO6Max8fBSD2zcs1Wu7soME3xoX0kPbR7JwlWw3TUmh02DnsfI1aooEaqz4\ne3TAtxxCqFs831DEBU0Tgvb7suaJUd7smvLgEd6OTeJJiClbxO2OEe1LDLecC4/3U1niP2VoW24f\n3pav7hhE4zrRtA4RPor2abQTqkxFPtFMsf+FrbIN71j/xQBRVIbjn/NKbohTE27FFWXnen0BeTKK\nn4yiO3vfu0z9PBwfzzzVDGMITqlxlb4McOnLk9O3MPaNpeXuFxxuarD4u/7QJWUw+mbZNIyznbPZ\neuOEaCb1aem37ao+rpCSb1YPwNyHRvDzfcP8tjWMc03cfjZlQJW3LCxN3aHzYeoVXfxSRxv63FF5\n8J3w9UwwJ9aJ4gKfu7McYrnZ/hjpMpH3bf+knTgS9DrFURMyMBTnptBp+BUeNPIyuFJfyQxjMNk+\nq3ljrEWfd/08UoY9Dstp6rLY7MlEfQUaJmdy7azdfwbAb3FldaIGi7/r/5JE1rftY5RVK7EgWnxA\nTNwj6JqAv0/s5t3eJCHKm2fv4V/X9ubbOweHXE5e2bwyqQcPXNyBfm3qlzy4lPRqVY/Luhc/yfq/\ne4Yw56GiyVtf8ffcik/q25JHx13gd1wm8dzieBQnOp9YX6EBwauiQ6HEv+ZzwVNzuPq9ld7n5sZv\niRWFfGmM8hvn6/l7Q57luDP0DVX+zxhOM3GGwdpWrvtglbfUSnWN/9dY8ffE/EuqgumbFRNl0UsW\n/4BSyZ6J11GdG3PjoDbe47UQq2pbNYhlQNsGQdurgsZ1ovnTJReEtKu8/HjvUN65oW+x+/u0rk8n\nn1Ca3624Txgq1N/okGzCbfY/00Sc5UPba0RRcjG5wA5tiprJxkMZrgdSoqV8yhYziU3Sv9F6jI9T\n50l2KM+ckK/DssDsQ5aM5Srd1U/b87mtptpfc8W/v3vhxvUDzt0I3uYn/ppfQTQP7RrFebMDYm3+\nF4ceLeux/6XxdGjsCl00iCtbCeTahO+F1i8O69b+oR0a+qW6bpAdeMhxL3203Txr+dS7vdjFckr8\naw1vLkjjtpc/Rj+5la+Ni7i4UxO//b7fU49In69zUIiNX81+jNHWY8Xp/dwW93mMdGqs+DevF8P+\nl8aXOKvvW6XSZtFo7W40HufjpS7880geu9QV0w9cqBTIf28byLNXdi02ZbI241s8TvPx/D0VQH/f\nv7X3juDf1/cGYI45gLecE7nWspjf64uA4J4LHgI7tClqLv+ct4uBOfORmpWfjcGM6eIv/r7f06Lq\nsGUXaU/BuHsvak+j+ChmG/1JEHkM0bZ6LyolFV2MVGqs+JcW36JkURaNVu4KmJ5yr4HYzrESF1yh\nnZuHJFWYfTWBn+8bxiNjOvrNv3gWIJtS0qyu60J9Zc/mFLgL6/VsWbTS+HXnJJYa3XnO8indxd5i\nRV55/rUHDZMJ+m+caj6STOKDSo37ZvGdT7aPR/xjrDqncgpZbnYnW8YwTlvjjflX1/IjtV78fbFZ\nNG+KYvrZ/JBjfEMXX99Ruhr8tZ3uLetyf0DN/+JSTz1NdXznB0w0HnTcy0nq8q7tDZzZoZdhO6up\nB6YoO0O0rTQWGUzLcZUDCaxK69ugpegus+zn8XzfPdlrhdhYaPZmrL4Wnepde6rmlXcoJZ9NGRDU\nGMWmF4n/wTO5PDW+MwkBi5U6NqnDzYPbcNOQJNonqt6y5UUr5lb8rev68N6SPUH1jM6SwN32h/jB\n9lc2fngr8X/4is4BK46V5197+J2+nCwZy/vHXIsFAz1/3x7WnhBso/iyz8fddWF7Yqw61/Rtyemc\nQl77dRezjQFM0H+jp7GVHbQrsQdIpFJrPf82DWIZEdCbN8qq06dNfQa3a8jTl3fhtuHtmNyvld8Y\nXRM8O6GbEv7zxBP/D/TWhyU34ovbBobsQXAw+gJec06mf/4Kfvzk5aD9s7YcrRxjFRHBnpOu6rvR\nFDJOX8tMY6C3lLrvHfmUoW250qf8yaB2DXhlUg+evrzsK+SjrTp3Xtgei655F1kuNnuSL230zV8B\nqJh/tSOUuNh0jWirzld3DKJHy3NXt1ScHy3ru+6w2jcu+SLqWZsQZ9P5wBjPCqMrD9g/pODYLta5\nF9oAvDJnp/dx0mMzeX7mtgq2WhEuTFMy6h9LABitpRBHAT+ZRSt6fWtZTb2iC3V8Ei6EEEzu1ypk\ng6Gy4EkCKSCKFWZXBjrXA9Jv0Vl1Qok/eEM91lLWtFecP0M7NOK7uwZzRzET6754JtljbDoSjUcc\nd+HAwo53fs+17y3zG+tb/vmDZfsq1mhF2PANrVyqr+aMqM8a06dXRgk9oSsC34SFRWZv2mgnaC+O\nVNuYf60Tf4/m+6Ydfn/XYD69tX+Vl1yo7fRPalCqRWeejCxP7vYxGvK44zZ6aXu5R5/hN3bhjhMV\nb6gi7BT49Oi9SNvImughmD7yVSfaynUDWodcp1OReMqrLzZcFUNHahuwq7BP9cDj8ft6/o0Tohnp\nboiiiByeuKwTr0zq4W207ZvBMdscyAxjMPdZptNRHPJuv+uL9aq6Zw3jxVnbmZ56GIALtY3EikJW\nRhfVzVryl5H0alWPF6/qzponR1eqLdOmDADgMInsNFtykbaBb9ceqpafuVon/p6FGZZS9qxVVDwl\nldDwcMeI9kzu18obz/XEbFs1iOHJyzrzjONmsonlFev7aBTdejvUYq8axX+W7uXZn13zN5fqazgj\n49lm7e7d36Zh8V3lKhotIPQzQNtB6u5DzNhY+gKEkUKtU0CPxy9q3TuPDH577GKW/99FZTrGU5vF\nE6prVjeGujFWzpDAs46b6aXtYYo+2zs+r9CoOIMVYcV3DseGg1FaKnON/hhhki4/8Td6YRMGw7Qt\nPPTNhrDYcz7UOgls5c4yUdH98NC8Xgz1SuhJHMiwDo0AyMh3AK4G23XdsdcZ5mDmGX34s+Vb2ohj\nAPT+27wKtFgRTi57s2hCf5j4D7+EAAAgAElEQVS2mToinznmgLDZ4xswWC+TyZKxjNRcwn8iu4BJ\n7/7GiayCMFlXNmqd+H/+xwH869pefqlgisjm9/1b8cPdQ5jqztO+bkBr6nkX3wmeckzBgYVnLJ9B\nFWR9KKqO3SdyvI8v1daQKWP5zewKwMtXd+c/fyi+qmxl4NsM3omFZWY3RuobAckXqw6y7sBZvlh9\nsEptKi+1TvwbJ0QzoVeLcJuhKANCCPq2qU+3FnXZ/9J4BrRtQNO6Re0uj9OAN5xXc5G+kbHaOr9j\nq2vFRYU/FpyM0dcz3+yLAwsSVyHAsV2L7ydRGQRmBC43u9NMnKGdOEqhOyPJWoFl0yuTWif+ippB\nYJvIT42xbDdb8bT1c2Iouu0udKrJ35pAP20X9UQuvxr9wmpHoDOxwnQ1cBqibeU/S/cC/sUiI5kK\nsVIIMU4IsVMIsVsI8ViI/bcIIU4KITa4f26riPMqai9CCKbfM8T73EBnquNWWopT3Gv5ybu9utZd\nUfgzSkuhUFpYZrqyfMJ1QxeY0XlQNiZdNmKYtsW7rbosFj1v8RdC6MDbwKVAF+A6IUSoIhrfSCl7\nuX8+PN/zKhSdmyX4PV8rO/GDMZw79F+8vX8LHUXiv2rvac7kltwRTBEZ+ObOX6ylssrsQh6ucF+4\ngnnBPQEEy41uDNa2etONLbUo7DMA2C2l3CultANfAxMq4HUVinMSqrHOi47rKcDGE5b/AkWev5SS\na99fxfUfrKpSGxXlJ7vQVd67rThKe+0o880+vHhV9xKOqlw84n9Bkzrebb+Z3agr8ugq9gO1K+zT\nAjjk8zzdvS2Qq4UQm4QQ3wshWoXYr1CUm1uHJgFwirq845zAaD2VwdpW7yScp9zzjmPZ4TJRUUay\n3Km9o7QUABYavakbE94sPY/jr2vCu17Fk33kCf1Ul9LiVXWJ+hlIklL2AOYBn4UaJIS4QwixTgix\n7uTJk1VkmqI688RlnbhpcBvuGdnBu+0TYxzpshFPWb7A7nAJiGruXv3I9Ip/KtvNVhwmsag9apiC\n/h0ax9OzZV3+NrErLevHsu6p0Vw/qh/bzVYMcYt/dSn0VhHifxjw9eRburd5kVKellIWup9+CIRM\nzpVSvi+l7Cel7JeYmBhqiELhxx0j2vPchG5+zTwKsfGy41q6ageI3f4dHy/fR7e/zg2jlYrykJXv\nIIEc+ms7WGD2AfybtISDaKvOT/cNo2+bBgA0io8iqWEsv5nd6K/tJAp7tUkyqAjxXwskCyHaCiFs\nwLWAX6lFIUQzn6dXAtsr4LwKhRerxX+S7WdzMKlmB5que5VXf0lRnn81JDPfwUhtExZhssBwib+n\nvEIk/TUtusZvZheihYOeYg9frDwQbpNKxXmLv5TSCdwHzMUl6t9KKbcKIZ4TQlzpHvaAEGKrEGIj\n8ABwy/meV6HwxRo0ySb4m+NGbPknuNPyS1hsUpwfmfkOLtZTOCUT2CjbA0VlWSJp7Z5FE6w1L8CU\nggHaDo5kFnC2GmSVVUjMX0o5S0rZUUrZXkr5vHvbVCnlDPfjx6WUXaWUPaWUF0kpd1TEeRUKD6HS\n61JkR463upTb9Zk0JDMMVinOhzM5eYzUNmK0H+NXuz/SsGiCLOLZKVsxQHNJ28EzeWG2qmQi9zeq\nUJQBIQSvXN0jaPuqpLuJwsE9lhkhjlJEEoYp+XbtIZyGybYjWSyZ9wv1RC4FbceE27Rz4qk6u9rs\nRF9tFxacHFDir1BUHb1aF/VdTmoYi64JHpyXww/GCG7U59GcU2G0TlESX689yKM/bOLT3/azMT2D\nkfoGHFJHTx7lHROJzfY8vUHWmJ2IE4V0FfvJyKslYR+FIhLwzfix6hoJ0a7mL286fwfA/ZbpYbFL\nUTo8cfIzuXakhBHaJtbLjrRoksjNg9vw6LgLvNV42ydWXQOXkvD0CFnr7ik8UNvu14cgUlHir6gx\nWC3+H+coiyst8KTehC+NUVyjLyFJHAVcpQMOnM5VVT8jkOwCJ18uXEdX7QBLjR4IIXh2QjfuGdmB\nto3i+PyPA3ghzCt9Q3GSeuwxmzFA21EtCgoq8VfUGHw9fyHAMwf8zg19eNs5ETtWHrb8QMrBs7R7\nYhYXvrqYr9YcKubVFOHi81UH6JC9FoClZrDID09OJNZWulagVYGv/7DG7MQAbad3cWEko8RfUWOw\n+Xj+AuGtvV4v1sop6vKJMZYJ+m+c2pPqHbdm3+kqt1NRMsP1TZyWddgqk8JtSon4FntbY3YiQeSR\nkJUWRotKhxJ/RY0hKiDs42m5l+CuB/Mf5+Vkyxg67nzXO8ZQUZ+IwXOxFpiM0Daz3OyOrAYS5fsR\nWuOO+zfLSAmPMWUg8n+zCkUp8V3odUXPZrRrFA9ArM0V+88inmnGGNocm0d74apAYqqVvxFHJ3GI\nRJHJUiM4dTcS8fX8D5PIURrROjv1HEdEBkr8FTUG3Weh170XdeDNa3vzzg19aFm/qOvXh87LcGjR\n3Gf5EQCnGfkTc7WNEdomAG/jlkjHqvnL6AatC00zN0bWMuQQKPFX1EiEENSNtXJZ92Z+28+SwLrE\n33Gl9htJ4ijVpAZXrcBzFzZc28R2sxUnqB9mi0rHkPYNuXtke+/zFYXtacRZVq5PJbsgcid+lfgr\nagW+8wEPHBiOAwv36j+RUxi5X87aht0wiaGA/tpOVms9efzSTrxzQ59wm1UimiZ4cFSy93mK6Xr8\n1f++Z9wby8JlVoko8VfUCjxxf3A1fPnSGMXv9OVoGQfDaJXCF7vTZKC2gyjh5FST4dx5YfugO7dI\nxRNyFAJ2ylbkyGj6ars4nJHPC7O2k/TYzDBbGIwSf0WtIDAv/D/Oy0HTuTzrKw6czqXQabBm35kw\nWacAKHSajNA2USCt7LJ1Dbc5ZcKiCSb3a8lXtw/CQGeD2Z6+mivd8/2le8NsXWiU+CtqHMmN44O2\nxdj8m4AM7dOdDYlXMElfyqezV/DirB1M/s9KdhzLqiozFQEUOk2Ga5tZbXYmV4a3XWNZEULwyqSe\nDGrXEID1siOdxQFiKfCOibT2jkr8FTWKrc+O5ZcHhgVtjw0Q/0u6NKXdhCfQkFyU8QM73b19T+dE\nfkGumkps/lGStcMsNbtXi9o4xfHchK6kmB3RhaSntse7PdLaOyrxV9Qo4qIs3po+vtw+vJ3f82ir\nRoMWySyLGk7/0z+xda8r9h9p3lltol3WagCWmj2rtfj3T2pAqunqKd1X7PJuj7Ruckr8FbWCK3o2\nZ/9L46kX6wonxFhdF4glDa8jRuZzoz4fACPCc7NrGt+vT+fNBa7Y+AU5azlBQ/bQgnsv6hBmy8qP\nVdfIIo6dZkv6aEVlHpzK81cowodH26Pd4k+zHiwxenCrZQ5R2Ek5cJZD1aARR03hz99t5J/zdoFp\n0Ck/hU1Rfdj74uVc0rVpuE0rN5604vVmMn20NAQu0XdEWC0RJf6KWolnArh943jeM64gUWTyO305\nby3czfBXFvHf1dWjCXdNYXvKEuLNbDZHR35ef0l4yoykyI7UE7m0c5cRVzF/hSKMeOr3R7vnBQa2\nbcBKswubzLbcrs9Ec3tpT07f4j0m0m7XayKzpv8XE8HO2H7hNuW8sXk9/44A9NVccX+n8vwVivDh\n+fpF21wf/Y5N6gCC/zivoL12lDHaOr/xS3edpMOTs9mUnlG1htYyRuib2K13wB5VPUo6nAuP+O+T\nTcmgDn2FK+7viLA6Ukr8FbULt/oHZgTNNgdwwGzM3Zaf8S3Su2TXSQC1AKwSqUMevcVu5hZ2RYvA\nHr1lxap73oRgi9bJ6/mrsI9CEUbuGOFK+QzM+zfR+MAYTy9tDwPEDu92z7L9SEvTq0kM0bZiESbL\njO4cyyoo+YAIx7ej3Da9Ex20I9QjW4V9FIpwcv+oZPa/NN6v9r+H740RnJHxTLHM8W7ziL/K/688\nhmubyJHRpMhk8u1GuM05bzxNaQDWGq6U1d7abuX5KxSRSgFRLEu4gku0dbQSx3l+5jZ0ocS/snBN\nvktGaJtYaXbFiYUCR2QJZHmZMrQt06YMYHleK5xSo7eWFnF3j0r8FQofFiVciYHGLfqvfLBsnwr7\nVCJ2w6SNOE5r7SRLTFfXrjy7M8xWVQxTr+jCiI6J5BPNdtmaPiINR4StWlbir6j1xPnE/8/qjfjF\nHMRkfTHx5GFxi79q91ixbD2SyTuL9gR17cp3VP+wTyCpZjK9tD04nJF1YVPir6j1/PLAcADqxlhx\nGCYfOy+ljshnsr4ETXn+lcL4N5fzrwVpjNA2c8BszPXjRgIwqnOT8BpWCaSYycSLAqLO7ip5cBWi\nxF9R62nbKI6dfx/H2idH4zBMNst2rDU7cos+B6tw3aobEZajXROw4mSwtpVlZnd0TbDy8Yv55+Se\n4TarwkmRrs5e8Scjq6m7En+FAlfev82ieeuvfOS8jNbaScztswBUr99KoI9II14UsNQd729WNyZk\nRdbqzkHZmNOyDgmnlPgrFBGL0+3hzzP7ki4b0fvoVwB8t+4QAGnHs/nX/DRvmQhF+Rmub8IpNVaa\n1atrV1lonxgHCFLMZOqd3hBuc/xQ4q9Q+OBwukTdQOcT51gGajvoKvaRXejENCWXvbmM1+fvIrsw\nsibvqiMjtE2kyGSyifXLja9JzHpwOBumjiHVTCYhdx/ZZ0+E2yQvFSL+QohxQoidQojdQojHQuyP\nEkJ8496/WgiRVBHnVSgqGt+FON8aF5Ejo5limQ1AnsPwhoWy8h1hsa8mIKWkAVl0E/tZZnQPtzmV\nSpRFp16sjVTpWuyVmbYyzBYVcd7iL4TQgbeBS4EuwHVCiC4Bw/4InJVSdgBeB14+3/MqFJWBb/Gt\nbGL5zriQK7SVJHKW/adyvfsylfiXG8OUDNO2oAnpjffXdDaa7TGkYOWSueE2xUtFeP4DgN1Syr1S\nSjvwNTAhYMwE4DP34++BUaKm3ucpqjWesI+HT42x2ITB9fpCNqVnercr8S8/DkMyXNvEWRlP006D\nAKjpYiBsceyQrWmStSncpnipCPFvARzyeZ7u3hZyjJTSCWQCDSvg3ApFhRJYf+WAbMoioyc3WBZw\nNivHuz0rX8X8y4vDMBiub+ZskyE0qx8fbnOqhE+nDCDV7EAvbTfSNOj39/m8u3hPyQdWIhE14SuE\nuEMIsU4Ise7kyZPhNkdRCwlVfOsz4xIaiwwaH57n3aZi/uXHPLaNpuIsRxsN9mZN1fQ4QIxVJ8VM\nJkHkYz+2nVM5hbw8Z0fJB1YiFSH+h4FWPs9bureFHCOEsAB1gdOBLySlfF9K2U9K2S8xMbECTFMo\nysar1wQvMlpi9uSA2Zg+x7/zbqspNWjCgbZvEQAnGw8LsyVVR7RV9y72ytu7KszWuKgI8V8LJAsh\n2gohbMC1wIyAMTOAm92PJwELpUqUVkQgY7s25cqezf22STSmGWNon7+ZLmI/AIURVqSrOmHbt5Bd\nZgsc8c24bmBrdE0wpkvNK+vgS6xNZ79syhkZj3lwdbjNASpA/N0x/PuAucB24Fsp5VYhxHNCiCvd\nwz4CGgohdgN/AoLSQRWKSCGUVzJLv5h8aeMPuiv0syk9k4w8e9UaVgPIz83GengVS80e2CwanZom\nsOeFy2hZPzbcplUqMVYdEKSayUQdSwm3OUAFxfyllLOklB2llO2llM+7t02VUs5wPy6QUl4jpewg\npRwgpdxbEedVKCqDxnWigrY1bdqM6cZQJuorqEsOMzcfZcLbK8JgXfVm2lf/RTftLDV7YNEiasqx\nUolxV45NMZOJz9pNArklHFH51J7fvkJRSv4y9oKgbfFRFj43LiFG2LlGXwLAgdN5VW1atafRieUU\nSCurzc4+vW5rPlHupu6exV69tN3hNAdQ4q9QBBFtDS4uJoRgu2zDarMTN1vmo+GK+Tvd2UGmKTl0\nRl0MSmKwuYHVZmcKsWG11B75EUIw+8Hh3sVevYUSf4WiWuDJT5jmvIRW4jgXahsB6PDkbLILHLy9\naDfDX1nktwpYEUDGIZo7D3lX9dpC9FGuydgsGrnEsEu2oo+WFm5zlPgrFGVhrtmPE9TnZv1X77aM\nPAe/7XFlLh/OyA+XaRFPwU7XZLmnZaOnS1ptwRv6MTvQW9uNILwZY0r8FYpzcGHHRP5vXCc8iclO\nLHwvxjBS30iSOApArt2JRVcdv85Fvt0gZeH3HJEN2C1dBQBqU9gHXJ4/uJq7JIg82omjZBWEb7Fg\n7frtKxRl5LMpA7h7ZHukTwLodDEGu9T5gz4fgJwCp7fRu+r1G5o/fb2ObgWpLDV64KnkU9vCPlF6\nUcYPQB8tjXX7z4TNntr121coSsmXtw/kwVHJ3ueeYp/v3tCHM6I+s82BXKMvIZYCDp7JY8OhDADS\nTmTz0fJ94TA5YjlwOpcT21eQIPJYYhatoLbUomwfgCirS273yaZkyDj6iDQKHOEL/VjCdmaFIoIZ\n0r4RQ9o38j73eP4JMVaEgM+clzAh6jcm6iv407fR3nEvzHLVa7m2fyviotTXC+DgmTxG6JswpGCF\nT9euhGhrGK2qejx3OhLNG/ffGcb+oMrzVyjKgHD/myKT2WImcZP+K6HWBOeqTl9edE1wobaJDbID\nWRRV8awfawujVVWP5jPBnWIm01GkIwuywmdP2M6sUFQjvJWoBMTYNEDwmXEJnbRDDBTB1RlVm8ci\noh2Z9BB73fH+IjyrXmsjqTIZTUh+/GUGRpjmiZT4KxSloE+b+gA0rhNNnM0VzplhDOGsjOcmS3B3\nJuX5F1H3yPJa1bWrNGww22NKQQ+5i70nc0o+oBJQ4q9QlIJHxnRkzkPD6dA43hvLL8TGN8ZIxmrr\naBpQoTxHib+XOoeXkCHj2Cjbh9uUiCGHWHbJlvTW0sgqCM9nRYm/QlEKLLqrAiW4yvN6+MIYg4bk\nessCv/E5YfpCRxxSUvfIMpab3TF95OaX+2tPLf/i8Ez6vrtwV1jOr8RfoSgj7ROLJi3TZSILzD5c\npy/ERtGCnVzV7MXFsU1E5Z9gkdHLb3O3FnXDZFB4Wf3EKO/jFJlMPZHLvl0bw2KLEn+Foow8flkn\nLuxY1GnuM+MSEkUWl2pFTTpUj19YuOM4//38fUwpWGwGd0irjTRJKEoL9iz26q3tJjsMK32V+CsU\nZSTKojOhV1G3rxVmV/aYzbjZUlTv50R2QThMiyimfLqOLjmr2CTbcZoiT//WoUnhMyqC2CubkSlj\n6SPSwlIeXIm/QlEOfJuQuto8XkIfbTfdhatP0fGswjBZFjk0IIueYg8Ljd5+2/96RddijqhdSDQ2\nmB3oraVR4DCq/PxK/BWKcuDRfs+6nR+M4eTIaK/3fzxLef4jtQ1oQrLQLIr3h2qUU9tY99Ro1rhj\n/ylmMheIdJwFWRzJyKfQWXUXASX+CsV5UDfGVaIgh1gKu0zmSn0lAxubZOSFr1pjJHDoTB4X66mc\nkPXYKpMAWPPkKO69qEN4DYsAGsVH0dgd+0+VHdCEJPr4Boa8tJAHv9pQZXYo8VcoyoGnuYtH/HVN\n0PCie7HhYLK+GKcpGf3PJdwxbV04zQwLacezueiVeYzQNrHQ6IV0y0xtq+JZGjaYrothzHFXU/d5\n249X2bnVX0OhKAeesI9H/A1TQuNO0HYEI7NnIA0Hu0/k8Ou2qvsyRwrbj2XTT9tFgshnkVkU77co\n8Q8iizh2mS2ocyoV8NSOqhrUX0OhKAeeipS+Of8ADLiThs4T9LevCYNVkYFpSi7SUrFLnRRLUYpn\nbWrYXhZSzWQanN0ESEQV/oqU+CsU5WBs1ya8MqlHcOZKx3GcsTRhgn1meAyLAAxTMkpLZbXZmeuG\ndfFut2pKbkKRIpOJdmTQVhxDVKHvr/4aCkU5EEIwuV8rEmICavbrFlY2mEA/cxPtxeHwGBdG8uxO\n0vdto4N2hEVmb2wWzRvr12pZz96S+PvEbvz1ii5Fi71EWpXGfZT4KxTngXDfpyfWifJuS2l4OYXS\n6m3ybpoSuzO8zbqrige+2kBG6gwAFpi9seoasx4czktXdQ+zZZHHjYPacGXP5uyWzcmSMfTR0lTM\nX6GoTvz68AhmPzjc+zzf1oCfzcFcrS+lDnk8/dMWOj4125shVFM5cDqXRTtPMFZfx06zJQdkU5ym\npEPjeK4d0Drc5kUkVovmXezVR9tdpedW4q9QnCcdm9ShUXyR56+52zzGiUJuS1jFf1cfBCA/DKs4\nqwrTlFz46mISzEz6ix3MNfsBhGXlanXCExJLlclcIA4SJ6pucaASf4WigpESNst2pJoduLxgJgJX\nyCczv2Yu/Dqckc+sLUcBGK2noAvJr4ZL/PPtSvzPhdUj/mYHdCHpzp4qO7cSf4WigvF05fvMeQnt\ntaMM1bYCsDk9s0aK4RVvLee+L1156pdo60iXjdgi2wJQUIXlCqojunsSPMW92Ku3SKuycyvxVygq\nHJf6zzIHclImeCd+7/h8PQ99kxpOwyqFM7l2AGIpYIS2mXlGXzxpK+O7Nz/HkQoPWcSz22xOTyX+\nCkX1xTOva8fKV8bFjNJSaClOAjB363HMMDXsrmxGaJuIEg5+dcf7d/xtHIPbNwyzVdWHVLMDPdnl\nXzK2Ejkv8RdCNBBCzBNCpLn/r1/MOEMIscH9M+N8zqlQRDqmz5f3S+coTAR/0Itq/X+55mA4zKp0\nLtHXcUbGs8bsBBSFNBSlI0Um00Bkw5m9VXK+8/X8HwMWSCmTgQXu56HIl1L2cv9ceZ7nVCgiGl/H\n7RgNmWMO4Dp9EXHkA3A0Mz9MllUeFpyM0lJYYPTBwNXjWK/KWgXVmE9u7Y/NonkXe2WmrayS856v\n+E8APnM//gyYeJ6vp1BUewJv2j9wXkaCyGOyvhiomaI4SNtOXZHnDfmAWtFbWi66oDEdm8STJluS\nLWP4eeaPVXLe8xX/JlLKo+7Hx4AmxYyLFkKsE0KsEkKoC4SiRmMGxGw3yg6sMS9gij4HHSOkKH6/\nPp3pqelVZWKFc5m2ihwZzVKzR7hNqZa0aRiHicZGsx29qmjSt0TxF0LMF0JsCfEzwXecdC1fLG6m\noo2Ush9wPfCGEKJ9Mee6w32RWHfy5MmyvheFIjII8S34yHkZrbSTjNXWYkpIemwmL87e7t3/5+82\n8vA3G6vQyIrDgpNL9bXMN/tQiC3c5lRLPOUvUmQyncRBKMyp9HOWKP5SytFSym4hfn4CjgshmgG4\n/z9RzGscdv+/F1gM9C5m3PtSyn5Syn6JiYnlfEsKRXgJ5QHNM/uy32zC7ZZZZOS6+vv+Z0nVTOxV\nFoYpkVIyTNtCfZHDL8Zg6sdaw21WtaROtJWLOzVmrdkJizDh0OpKP+f5hn1mADe7H98M/BQ4QAhR\nXwgR5X7cCBgKbDvP8yoUEUtg2AfAROMj41J6a7uxHV0bBqsqnjumrWP0P5dwub6KLBnLUrMH79zQ\nN9xmVVsKHAbrzY44pQYHVlT6+c5X/F8Cxggh0oDR7ucIIfoJIT50j+kMrBNCbAQWAS9JKZX4K2os\nxaXxf2+MIFvEM/zUNyW+xuVvLePzlfsr1K6KZsGOE6SfPMsl2lrmGv2wY8VmUZO85aV+rI08ol2r\no/dXvvhbSh5SPFLK08CoENvXAbe5H/8GqHquilpDcdU784lmYdx4rsj+ltbiOCctxa9+3XI4iy2H\nt/KHwUmVZGXZuPuL9Vh1jTev84/YjtA2kSDy+cUcDBTVqlGUnUHtGjBz81Heck7kg6H9K30FrvpL\nKRQVjG+Fz0DWNJ6EE40p+mzyHQar956uQsvKz+wtx5ix8UjQ9sv1VZyR8awwXR3NlPiXnxsHtQFg\ngdmXd48mV/r51F9KoahgHru0Ey9f3Z3+ScEL3i11m/OzOYTJ+hLqksMLs3eQ9Jh/y8fqUPff7jSJ\noYDR2nrmGANwuoMIqk9v+RFC0LlZAgC/bjte6edT4q9QVDDRVp3f92+NERD8H9i2AQ3ionjfOZ5Y\nUcgt+lw2HsoIOr46lP6ZnprOOG0tcaKQH42h3u1G7WhYVmkUuvsf2KrgIqrEX6GoJAwfEf/DoDZ8\nc+dgEmIs7JStmWf05VbLHJLrBh/nNCNbQVfuOc3//bCZq/WlHDQTWSsv8O5rVi86jJZVfzLcPR+q\nInymxF+hqCQ81Tun3zOEv03sBkBclCs88rZzAvVELtfp84KOcxqR6/rn2w2u+2AVzTjNEG0b/zOH\nI90y0j4xjoRoled/Ptx7kauuv1YFJUCU+CsUlYQn39+iFX3N6rjF/1idbiw3ujIxfzpR2P2Oc0Zw\n3Gfb0UwAfqcvQxOSH4yi3sW+71NRPv44rC0Xdkwkq6Dyu76pv5ZCUUl4NNxXE+OjXeLfIM7G28ZE\nGsgMrtGX+B0XOFcQSZzOsQOSSfpSVpudOCSLynmpEs4Vw0tXd+ejm/tX+nmU+CsUlYQn7ON7C+8J\n+2garDS7sN5M5i7Lz1hwesdEcsw//Ww+fUQa7bRjfG+M8NtnUZk+FUKzujEk1ik+XbiiUOKvUFQS\nhjvs4+sR29wTeS59F/zbOZGW4hQT9aIVnZHk+efbDb8QxN5TOVxvWUiOjGaWMZABSQ28+5TnX71Q\n4q9QVBKhPP9Ym6vRSesGsQAsMnux2UziqfhfsOBEShk04fvpin30eu5XwsFFry2mxzNF5z585CiX\nayv50RhKLjE8Ob4zNw92LU6yKPGvVijxVygqidFdXPHwBnFFZY7bJcbzzg19eOUaT917wT+c11Cv\n8DCT9SU4DBk04fvMz9vIyHPgKCaJ/uLXFvPa3J2V8h6OZRX4Pe99ZhbRwsF/jdEAxNh0xnZtChRN\n+I7u3Ji+bUJ2dFVEEEr8FYpK4v/GdWLNk6P8xB/gsu7N/FIiF5u9OJbQk/st03l9ziaMgJi/x6PO\nLnASir2ncvn3ot0VbH0oJFc453Awthvbpcvbt+oaDvfFyhPz//Dm/vxw95AqsEdxPijxVygqCV0T\nNK5TmkVPgvUd7qOZOFQYmsUAABffSURBVIN95QfYnf6ef4zVFSrKroL0v3MxWNtGW46yoenV3m1W\nXdCvTX26tUjgsUs7hdE6RVlR4q9QhIlHxxWtjD3ZsD/LjG7cbZnBiVOn/MZFu+cJ1u4/G/QalVkH\nqMBdasDDLfpczsp49jcZ491m0zXioiz8cv9wujYPsVxZEbEo8VcowsQ9Izt4H+u6xmvOyTQSWUSn\nvO83Ltrq+pr++bvgNo+FzopNC912JItCp8HMTUfp9PQc7/a24ihjtPV8bowmKjrWu11V8ay+nFc9\nf4VCUTFYNcFG2YE5Rn+G7fuERLpzknrAuVfOBnrn58Pcrce48/P1NE2IDprovV2fiQML05xjedB9\nJwJgtSjxr66ov5xCEUYeHt2Rib2aez3oF53XYcPBI5ZvvWMaxRdNGM/ZcgyA41kFFDiMCvX8d59w\nNQ0PFP5GZHK1vowfjOGcoi4xtiKfUZVwrr4o8VcowsiDo5N549reXg/6gGzKp8Y4JutL6Cr2A9Aw\nrmi1511frAdg4AsL+MNHqyvU8y+OmyxzseLkA2M8UDQBDWBV9XyqLeovp1BEADaf2Pm/nRM5SzxP\nWz/n6ndWkB8g8JvTXcXV1u4/S4GjcktB1CGPm/R5zDP7sk82A4oWqgFoamFXtUWJv0IRAUT5xM6z\niON15yQGadtplj6b/adz/cZe8e/l3seV7fn/0TKLeiKXN51XebfF+Ii/ovqixF+hiAACs2a+Mi5m\ns5nEVOvnRDtzij2utDH/zHwHX64+WKbU0Hpk80d9NjONAWyVSd7tsUr8awRK/BWKCMAWkDVjoPO4\n4zYakskdjmnFHpdrD73qN5DHftjEE9M3s/lwJlJK0s/mYZqSnMLij7/T8gtxFPC6c5Lf9libzqI/\nj+TDm/qV6tyKyESJv0IRAQSKP8AW2Y5PjHFcbf5KP7Ej5HFr950BIKlhbMj9Ho67M3jsTpOPV+xn\n2MuLuPfLFLr9dS6ZecErh1uKE0zR5zDdHMpu2dJvX4zNQttGcd7aRYrqiRJ/hSICCEyZHN/DNbn6\nT+c1HJaNeNX6H2IpCDpuz0lXSCjWFrxkx+402XU8GyhqLCOEYO5WV7robHfaaGZ+sPg/ZfkvTjRe\ncVwbtC/WqsI+NQEl/gpFBOA74Tv9niEkuDt+5RHNn+x300acYKolOPzjyfYJ7AGQU+ik41OzueT1\npZzNtXtj/ZqAs7mBbSNdr+EZM0zbzDh9LW87J3KcBgSiJnxrBtVqha/D4SA9PZ2CgmAPSFH1REdH\n07JlS6xW1bT7fLHpLkG16oLerevz+aoD3n2rZWfeM67gHssMFpm9mWsWtfgrdLqyfTwC3vPZXxna\noSHjuzf3jnGYptfzNyWczfMXf08qqcOQxFDA3ywfc8BszEfGpSFtjVKremsE1Ur809PTqVOnDklJ\nSYgq6G6vKB4pJadPnyY9PZ22bduG25xqj9Xi+jx7knGW7jrpt/915ySGaZt52fo+2+2tOejunZvv\n9vw9PQAy8x3M2nyMmwYneY8tsJtsPuxaG+AwTM4GxPgLHAZSSo5k5POo5Rvaase5zv4khfiXovag\nvns1g2p1CS8oKKBhw4bqwxcBCCFo2LChugurILztHd3q/7cJ3fz2O7Bwn+MBhID3rf/0xv9PZRcC\nBHX/8u0etnjXiaLXMcygEFG+3eTfC3eTnjqXWy1z+cQ5lpVmVwDuv7gDippJtRJ/UF5HJKH+FhWH\np7yDR5Z7tKoXNOagbMLTlkdIFum8YX0bHYPDGflAcMzf6dP1K7ewaCFYqG5g+Q6DJes28qb13+wx\nm/Gys2iS13dZwLQpA5h6eZcyvzdFZFLtxD8SSE9PZ8KECSQnJ9O+fXsefPBB7HY7n376Kffdd1+4\nzePHH39k27Zt3udTp05l/vz5YbRIURIez98jtrZiSiWnWHvznPMmLtHX86LlQzyXC6dp+gm7w+di\nkFNYFOaxh1gUVliQy9N5LxFLAXc5HqYAVy2h3/drhXS//sWdGjOiYyJThqkQX01BiX8ZkVJy1VVX\nMXHiRNLS0ti1axc5OTk8+eSTlXI+p7N0i3h8CRT/5557jtGjR1ekWYoKxiP2HZvEAxBl9f9qDmjr\nyrqREj4zxvIv51VMtizhWcunCEycpvSbJ/D1/E9kFXof2wPCQxac9Fn1EN3ZzSOOu0nzyel/6eru\n3oli1ZO35qHEv4wsXLiQ6Ohobr31VgB0Xef111/n448/Ji8vj0OHDjFy5EiSk5N59tlnAcjNzWX8\n+PH07NmTbt268c033wCwfv16LrzwQvr27cvYsWM5evQoACNHjuShhx6iX79+PP/887Rp0wbTnc2R\nm5tLq1atcDgcfPDBB/Tv35+ePXty9dVXk5eXx2+//caMGTP4y1/+8v/tnXlwVFW+xz+/dDobgSAk\nkMSEJCBByQpZQJaAyUQUYpQdhYEeFUZQFHVQChkYBovBpWbqIco8HyDIQ2EEQYTweIgDgSpHlggM\ni4xIBcngEkAWH1sSzvujO51O0kk3ZOnc5HyqUnX73JN7f797km+f+zvn/A7Jycl8++23WCwW1q5d\nC8D27dvp0aMHCQkJPP7441y/bhWG6Oho5syZQ8+ePUlISODrr50vKtI0DF5ewqone/HBxN6Akxk1\nNhEuD+/8pXQ4/1k6hAne23jL/Bamsus8sWKfvXqJg8h/f7FiXMax5+/LDRaaFxH+005ml1r4n5vp\nlW4pIvYxCB3ha37UabaPiIwE/gDcA6QrpfbVUO8B4D8AE7BEKbWgLvcFmPvpEY6euVTXy1Sie3gb\n5jwUV2udI0eOkJKSUqmsTZs2dOrUidLSUvbs2cPhw4cJCAggLS2NIUOGcOrUKcLDw9m8eTMAFy9e\npKSkhKlTp/LJJ58QEhLCmjVreOWVV1i2bBkAN27cYN8+6+MsKChg586d3HfffWzatIlBgwZhNpsZ\nNmwYEydOBGDWrFksXbqUqVOnkpubS05ODiNGVF6Wf+3aNSwWC9u3byc2Npbx48ezePFipk2bBkBw\ncDAFBQW88847vPnmmyxZsqTuD1XjNn3vCrYfO4Z9vL2ELh0C2VN4nookmsKfSsdSrNoyy7yKKFXM\nVHmaQlvmzVKHTeBPFlfkBlqwxfql3pHzLPJZSJrXv/hjya/577KKrRkrYfsO8dLq3+yoa8//MDAM\nyK+pgoiYgLeBB4HuwKMi0mxHjbKzs2nfvj3+/v4MGzaM3bt3k5CQwLZt23j55ZfZtWsXQUFBHD9+\nnMOHD5OdnU1ycjKvvvoqRUVF9uuMHj260nH528Lq1avt5w4fPkz//v1JSEhg1apVHDlypFbbjh8/\nTkxMDLGxsQBMmDCB/PyKphs2zJq5MSUlhcLCwnp5Hprbw3Ew3ctLmPNQd1Y+kU5wa99K9ZaUDWHS\njeeJ5EfyfGbynGkdAVyrNPvnjEPP/8Iv/8cY0+f8r+9LxEshU248y7KyB+kVU30xF1TMPtLS3/yo\nU89fKXUMXM76SAdOKKVO2uquBh4Gjtb2S65w1UNvKLp3724PoZRz6dIlvvvuO7y9vas9CxEhNjaW\ngoIC8vLymDVrFllZWQwdOpS4uDi++OILp/dp1aqV/Tg3N5eZM2dy/vx59u/fT2ZmJgAWi4UNGzaQ\nlJTE8uXL2bFjR5188/W1CovJZLqtsQZNw2ASwc9son/XEGas+2e181+16scDlzsz27yS583reMI7\nj+IDDzHYK5RCFcoNvBnRBcoKd5Pr9QWRXsXsvRnL9JLf2t8U7g5tzZe2PEGO3NQ9/2ZLY8T87wRO\nO3wuspUZkqysLK5cucL771uX2peVlfHiiy9isVgICAhg27ZtnD9/nqtXr7Jhwwb69u3LmTNnCAgI\nYNy4cUyfPp2CggK6detGcXGxXfxLSkpq7LkHBgaSlpbGc889R05ODibbatDLly8TFhZGSUkJq1at\nstdv3bo1ly9frnadbt26UVhYyIkTJwBYuXIlAwYMqNfno6l/TA4bpgxPiah2PqZ9K36gPVNKpjH0\n+lw+u5lC1OmNvOOzkDzfmXzm+xJPFb3Eb02bOKU68Jsb0xl5Y45d+AG6dAh0em9lzwlUvz5pPI/L\nnr+IfAaEOjn1ilLqk/o0RkQmAZMAOnXqVJ+XrjdEhPXr1zNlyhTmzZvHzZs3GTx4MPPnz+fDDz8k\nPT2d4cOHU1RUxLhx40hNTWXr1q1Mnz4dLy8vzGYzixcvxsfHh7Vr1/Lss89y8eJFSktLmTZtGnFx\nzt9oRo8ezciRIyv17ufNm0evXr0ICQmhV69edsEfM2YMEydOZOHChZXeUvz8/HjvvfcYOXIkpaWl\npKWl8dRTTzXo89LUHUfxf/5XXTlUdIEdxytm9nQOacWeQmuv/aegRF640JWf7utM3rZthMk5fCll\nYFoyf/jyJpdwLvLR7Vs5LVcop+Ua4+NS/JVSdZ0j+G8g0uFzhK3M2b3eBd4FSE1NbbJ/dZGRkXz6\n6afVyi0WCxaLpVr5oEGDGDRoULXy5OTkSjH3cpyFb0aMGFFtI47JkyczefLkanX79u1baarn8uXL\n7cdZWVl89dVX1X7HMcafmppa5xCSpv5oG1CRO0lE8K6yb250cIVwZ97dgZX/OMXWr89zSHXhkOoC\nQFpYPNe8j0INm790bOPntFzpsE+zpTHCPnuBriISIyI+wBhgYyPcV6NpFqx8vFet5x177f26BuNv\nNvHVdxcq1Wnt600bv5oT8IXYBpL73tW+UnlqtHV+f1x4m1uyWdP0qetUz6HAW0AIsFlEDiilBolI\nONYpnYOVUqUi8gywFetUz2VKqdqnpWg0Gv5rfCqt/bzpVGWjlvJOuK+3F9dLb5IYEWQ/18bPTLfQ\n1hw4XUX8/bwJ8vfm7C/XcUaQv5nPXsggLMifuDlb7eU5ieGkR7ejQw1vBhrjUtfZPuuB9U7KzwCD\nHT7nAXl1uZdG09LIdrFT1p+GJRAW5E94W3/uDm3N1z9cJsDHRIcq00EBAn29CfKvuedv8hLu6tDa\n6Tkt/M0TvcJXozEY5dH3AB8T93axhmkCfa39uDKlnG6w3sqF+GtaHlr8NRqD4Wzs9S+jk3msVycS\n7wwiwLf6C72f2csu/sGBlfP0Z8SGNIidmqaNFn+NxqA4Tv6KbBfA/KEJeJu8nO6x6+ttsov/gNgO\n9vJHksN5//H0avU1zR8t/rfIjz/+yGOPPUbnzp1JSUnh3nvvZf36asMeDU50dDRnz56tVj5//vzb\nul7VTKADBw605xbSGAtnYR8/s4lA277Abfwr3gz0ngwtFy3+t4BSikceeYSMjAxOnjzJ/v37Wb16\ndaWcPOV4Kj1CTeKvlLJnBnVGVfHXNF3ERaYdfx+ruPs4ZAb1M3thsq0P8PbSgq/R4n9LfP755/j4\n+FRaFRsVFcXUqVMB62Kq3NxcMjMzycrKQinF9OnTiY+PJyEhwZ6cbceOHeTk5Niv8cwzz9gXYtWU\nWvncuXPcf//9xMXF8eSTT1Zb8AUwY8YMrl69SnJyMmPHjqWwsJBu3boxfvx44uPjOX36NIGBFSs8\n165di8VicZoGGuCjjz4iPT2d2NhYdu3aVb8PU1NnaloFWd7zD3WYpeNnNtkzgjr29p19DTyUFF5t\nvr+m+WGoDdwrsWUG/FA9yVWdCE2AB2vONn3kyBF69uxZ6yUKCgo4dOgQ7dq1Y926dRw4cICDBw9y\n9uxZ0tLSyMjIcGmGs9TKc+fOpV+/fsyePZvNmzezdOnSar+3YMECFi1axIEDBwDrqt1vvvmGFStW\n0Lt37xrv16dPH6dpoMtTVOfl5TF37ly9G1gTwVWkpnwvgPC2fnx3/goAZpOXfZWuq37/W4/2qKuJ\nGgOge/514OmnnyYpKYm0tDR7WXZ2Nu3aWdPj7t69m0cffRSTyUTHjh0ZMGAAe/fudXldZ6mV8/Pz\nGTduHABDhgzhjjvc21kpKiqqVuG/VTs0TZ877/AHYERKZKXyrrbkbd1CHebz6whQi8W4Pf9aeugN\nRVxcHOvWrbN/fvvttzl79iypqan2MsdUzDXh7e1dKf5+7dq1SufrM7VyVXscX/mr3rcqOsVz08ZJ\n5A+AfncFU/D7bNq18uF3Hx20lz+YEMYnT/clMSKIF/5mLXc1fqBpvuie/y2QmZnJtWvXWLx4sb3s\nypUrNdbv378/a9asoaysjOLiYvLz80lPTycqKoqjR49y/fp1Lly4wPbt213eOyMjgw8++ACALVu2\n8PPPPzutZzabKSkpcXoOoGPHjhw7doybN29WmqVUUxpoTdOjfEGX2eRcuEWEdq18nJ5LimxbOeav\ntb/FosX/FhARNmzYwM6dO4mJiSE9PZ0JEybw2muvOa0/dOhQEhMTSUpKIjMzk9dff53Q0FAiIyMZ\nNWoU8fHxjBo1ih49XMdY58yZQ35+PnFxcXz88cc1pryeNGkSiYmJjB071un5BQsWkJOTQ58+fQgL\nq8jnPmbMGN544w169OhhH/DVNE1+/1B3fnd/LL+6p/b0D+6QmxReDxZpjIg4mzXSFEhNTVVV55kf\nO3aMe+65x0MWaZyh26Rpc/aX61y9UUZku8rJ4aJnWPeTLlwwxBNmaRoQEdmvlEp1Vc+4MX+NRuOS\n4MDqSd4A5ubGkRLl3qQBTfNEi79G0wKZ0Cfa0yZoPIyO+Ws0Gk0LxHDi31THKFoiui00GuNiKPH3\n8/Pj3LlzWnSaAEopzp07h5+f3uhDozEihor5R0REUFRURHFxsadN0WD9Mo6IiPC0GRqN5jYwlPib\nzWZiYmI8bYZGo9EYHkOFfTQajUZTP2jx12g0mhaIFn+NRqNpgTTZ9A4iUgycqsMlgoHq+xwaj+bi\nB2hfmirNxZfm4gfUzZcopVSIq0pNVvzriojscye/RVOnufgB2pemSnPxpbn4AY3jiw77aDQaTQtE\ni79Go9G0QJqz+L/raQPqiebiB2hfmirNxZfm4gc0gi/NNuav0Wg0mpppzj1/jUaj0dSAocVfRB4Q\nkeMickJEZjg57ysia2znvxSR6Ma30j3c8MUiIsUicsD286Qn7HSFiCwTkZ9E5HAN50VEFtr8PCQi\nPRvbRndxw5eBInLRoU1mN7aN7iAikSLydxE5KiJHROQ5J3UM0S5u+mKUdvETkT0ictDmy1wndRpO\nw5RShvwBTMC3QGfABzgIdK9SZwrwV9vxGGCNp+2ugy8WYJGnbXXDlwygJ3C4hvODgS2AAL2BLz1t\ncx18GQhs8rSdbvgRBvS0HbcG/uXk78sQ7eKmL0ZpFwECbcdm4Eugd5U6DaZhRu75pwMnlFInlVI3\ngNXAw1XqPAyssB2vBbJERBrRRndxxxdDoJTKB87XUuVh4H1l5R9AWxEJq6W+x3DDF0OglPpeKVVg\nO74MHAPurFLNEO3ipi+GwPasf7F9NNt+qg7CNpiGGVn87wROO3wuovofgb2OUqoUuAi0bxTrbg13\nfAEYbnslXysikY1jWr3jrq9G4V7ba/sWEYnztDGusIUNemDtZTpiuHapxRcwSLuIiElEDgA/AduU\nUjW2S31rmJHFv6XxKRCtlEoEtlHRG9B4jgKsS+mTgLeADR62p1ZEJBBYB0xTSl3ytD11wYUvhmkX\npVSZUioZiADSRSS+se5tZPH/N+DY+42wlTmtIyLeQBBwrlGsuzVc+qKUOqeUum77uARIaSTb6ht3\n2s0QKKUulb+2K6XyALOIBHvYLKeIiBmrWK5SSn3spIph2sWVL0Zql3KUUheAvwMPVDnVYBpmZPHf\nC3QVkRgR8cE6GLKxSp2NwATb8Qjgc2UbOWliuPSlSvw1F2us04hsBMbbZpf0Bi4qpb73tFG3g4iE\nlsdfRSQd6/9Tk+tc2GxcChxTSv25hmqGaBd3fDFQu4SISFvbsT+QDXxdpVqDaZihdvJyRClVKiLP\nAFuxzpZZppQ6IiJ/BPYppTZi/SNZKSInsA7cjfGcxTXjpi/PikguUIrVF4vHDK4FEfkQ62yLYBEp\nAuZgHchCKfVXIA/rzJITwBXgN56x1DVu+DICmCwipcBVYEwT7Vz0BX4N/NMWXwaYCXQCw7WLO74Y\npV3CgBUiYsL6BfU3pdSmxtIwvcJXo9FoWiBGDvtoNBqN5jbR4q/RaDQtEC3+Go1G0wLR4q/RaDQt\nEC3+Go1G0wLR4q/RaDQtEC3+Go1G0wLR4q/RaDQtkP8HD6slijCClA8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0.0, 3.0, N_samples, dtype=np.float32)\n",
    "y = np.expand_dims(np.sin(1.0+x*x) + noise_sig*np.random.randn(N_samples).astype(np.float32), axis=-1)\n",
    "y_true = np.sin(1.0+x*x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.plot(x, y_true)\n",
    "plt.legend([\"Observation\", \"Ground truth\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y3wx8vJlMgwI"
   },
   "source": [
    "With the input-output pairs created, your first task is now to partition the data in the training, validation and test sets. Keep in mind that we have created the data in a structured way, i.e. the input-output pairs are ordered. This means you need to shuffle the data before partitioning it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kDIMUZs0MgwK"
   },
   "outputs": [],
   "source": [
    "shuffle_idx = np.arange(0, N_samples)\n",
    "np.random.shuffle(shuffle_idx)\n",
    "x_shuffled = x[shuffle_idx]\n",
    "y_shuffled = y[shuffle_idx]\n",
    "x_train = x_shuffled[0:N_train_samples]\n",
    "y_train = y_shuffled[0:N_train_samples]\n",
    "x_validation = x_shuffled[N_train_samples:N_train_samples+N_validation_samples]\n",
    "y_validation = y_shuffled[N_train_samples:N_train_samples+N_validation_samples]\n",
    "x_test = x_shuffled[N_train_samples+N_validation_samples:]\n",
    "y_test = y_shuffled[N_train_samples+N_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ucvKRhOMgwN"
   },
   "source": [
    "In order to feed the data to our model, we will use the Dataset class provided by tensorflow. This class is simple to use and provides all the functionality we need for shuffling, batching and feeding the data to our model. It is also tightly integrated into the tensorflow framework, which makes it very performant. Performance is not an aspect we need to worry about in this exercise, but it is important in more demanding applications.\n",
    "\n",
    "In this exercise we instantiate a separate Dataset object for the training, validation and test data sets, where we shuffle and repeat just the training data set. Shuffling the validation and test data sets is not necessary, since we only evaluate the loss on those data sets and do not perform SGD on it. Please fill in the missing part of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2129,
     "status": "ok",
     "timestamp": 1562072575637,
     "user": {
      "displayName": "Felix Wiewel",
      "photoUrl": "https://lh3.googleusercontent.com/-XBgpqQQzeMU/AAAAAAAAAAI/AAAAAAAAAZY/12qzl-EFXfM/s64/photo.jpg",
      "userId": "11509421729312238102"
     },
     "user_tz": -120
    },
    "id": "HifQ63iPMgwP",
    "outputId": "4fa6e6d3-0d1f-4bf7-bb07-4df30348a48c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0702 13:00:43.758037 139937634293632 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(N_train_samples).batch(batch_size).repeat()\n",
    "validation_ds = tf.data.Dataset.from_tensor_slices((x_validation, y_validation))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Crr6fIkMgwT"
   },
   "source": [
    "In this exercise we will create a a simple neural network with two hidden layers containing $10$ neurons. For creating a model and keeping track of its weights a class called MyModel is used. When initializing an instance of this class the necessary variables are created and stored in a list called \"trainable_variables\". This makes it easy to get all trainable variables of the model. We also override the \\__call__ method of this class in order to implement the forward pass of the neural network. This method should accept the inputs to the neural network and should return the result of the forward pass as an output. Please fill in the missing part of the code and select suitable activation functions for the different layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nq8ri416MgwX"
   },
   "outputs": [],
   "source": [
    "class MyModel(object):\n",
    "    def __init__(self):\n",
    "        # Create model variables\n",
    "        self.W0 = tf.Variable(tf.random.normal([1, 10]), name=\"W0\")\n",
    "        self.b0 = tf.Variable(tf.zeros(10), name=\"b0\")\n",
    "        self.W1 = tf.Variable(tf.random.normal([10, 10]), name=\"W1\")\n",
    "        self.b1 = tf.Variable(tf.zeros(10), name=\"b1\")\n",
    "        self.W2 = tf.Variable(tf.random.normal([10, 1]), name=\"W2\")\n",
    "        self.b2 = tf.Variable(tf.zeros(1), name=\"b2\")\n",
    "        self.trainable_variables = [self.W0, self.b0, self.W1, self.b1, self.W2, self.b2]\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        # Compute forward pass\n",
    "        output = tf.reshape(inputs, [-1, 1])\n",
    "        output = tf.nn.tanh(tf.add(tf.matmul(output, self.W0), self.b0))\n",
    "        output = tf.nn.tanh(tf.add(tf.matmul(output, self.W1), self.b1))\n",
    "        output = tf.add(tf.matmul(output, self.W2), self.b2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uf6m8zXoMgwb"
   },
   "source": [
    "Now after the model class is defined we can instantiate a MyModel object by running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nSfI8wjLMgwb"
   },
   "outputs": [],
   "source": [
    "mdl = MyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KraeH6MsMgwh"
   },
   "source": [
    "We can now use the model to make predictions by calling it. In the following we predict on the inputs an plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 986,
     "status": "ok",
     "timestamp": 1562073286275,
     "user": {
      "displayName": "Felix Wiewel",
      "photoUrl": "https://lh3.googleusercontent.com/-XBgpqQQzeMU/AAAAAAAAAAI/AAAAAAAAAZY/12qzl-EFXfM/s64/photo.jpg",
      "userId": "11509421729312238102"
     },
     "user_tz": -120
    },
    "id": "K1at5RObMgwi",
    "outputId": "3fe92ed8-ba5c-4527-95b8-8fd7d675f364"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUqrEaLArpVSJ+X/grc/y4QeBWQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = mdl(x)\n",
    "plt.plot(x, y)\n",
    "plt.plot(x, y_true)\n",
    "plt.plot(x, y_pred.numpy())\n",
    "plt.legend([\"Observation\", \"Target\", \"Prediction\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "utFkv4F3NULQ"
   },
   "source": [
    "Since we have initialized the variables of the neural network randomly, it's prediction is also random. In order to fit the model we need to minimize the expected mean squared error over all input-ouput pairs in our training data set. For this we need to create a function, that performs a training step when provided with the model, an optimizer and a batch of input-ouput pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o4zFN0-kOdu7"
   },
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        y_pred = model(x)\n",
    "        loss_val = tf.reduce_mean(tf.square(y-y_pred))\n",
    "    grads = tape.gradient(loss_val, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ax-Kfd-tOm7I"
   },
   "source": [
    "This function uses the GradientTape to record the operations for which gradients have to be calculated. In our case this is the forward pass through our model and the computation of the loss function. After these operations are recoded we can get its gradients and apply these through the use of an optimizer. Finally we return the loss value in order to print it.\n",
    "\n",
    "With the training step function defined we now need to choose a suitable optimizer. Tensorflow offers a wide variety of optimizers but in this exercise we will use the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PahsqMscPcKD"
   },
   "outputs": [],
   "source": [
    "opt = tf.optimizers.RMSprop(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8_WjHrAwQB9e"
   },
   "source": [
    "We now have everything we need to start training the model. For this we repeatedly sample a batch of input-output pairs from our training data set and use the train_step function to minimize the loss function over this batch. We repeat this until we have iterated over the complete training data set once. After this we compute the loss on the validation data set and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 451334,
     "status": "ok",
     "timestamp": 1562073736676,
     "user": {
      "displayName": "Felix Wiewel",
      "photoUrl": "https://lh3.googleusercontent.com/-XBgpqQQzeMU/AAAAAAAAAAI/AAAAAAAAAZY/12qzl-EFXfM/s64/photo.jpg",
      "userId": "11509421729312238102"
     },
     "user_tz": -120
    },
    "id": "x3LqS3d_QrX6",
    "outputId": "604f7614-7aea-404e-cf02-7143bdaf4261"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train loss: 2.9776 Validation loss: 0.08793\n",
      "Epoch: 1 Train loss: 0.33951 Validation loss: 0.18942\n",
      "Epoch: 2 Train loss: 0.38259 Validation loss: 0.16884\n",
      "Epoch: 3 Train loss: 0.36105 Validation loss: 0.012762\n",
      "Epoch: 4 Train loss: 0.25058 Validation loss: 0.0077904\n",
      "Epoch: 5 Train loss: 0.2866 Validation loss: 0.00041251\n",
      "Epoch: 6 Train loss: 0.22018 Validation loss: 0.0051587\n",
      "Epoch: 7 Train loss: 0.20754 Validation loss: 0.016262\n",
      "Epoch: 8 Train loss: 0.22146 Validation loss: 0.0023657\n",
      "Epoch: 9 Train loss: 0.18422 Validation loss: 0.024248\n",
      "Epoch: 10 Train loss: 0.22048 Validation loss: 0.00013367\n",
      "Epoch: 11 Train loss: 0.15635 Validation loss: 0.0027906\n",
      "Epoch: 12 Train loss: 0.15185 Validation loss: 0.0011151\n",
      "Epoch: 13 Train loss: 0.18631 Validation loss: 0.0027761\n",
      "Epoch: 14 Train loss: 0.1252 Validation loss: 0.012725\n",
      "Epoch: 15 Train loss: 0.10215 Validation loss: 0.001674\n",
      "Epoch: 16 Train loss: 0.12482 Validation loss: 0.023147\n",
      "Epoch: 17 Train loss: 0.18792 Validation loss: 0.00083994\n",
      "Epoch: 18 Train loss: 0.1532 Validation loss: 0.0056872\n",
      "Epoch: 19 Train loss: 0.1078 Validation loss: 0.013261\n",
      "Epoch: 20 Train loss: 0.14279 Validation loss: 0.0061174\n",
      "Epoch: 21 Train loss: 0.10567 Validation loss: 0.0010076\n",
      "Epoch: 22 Train loss: 0.14338 Validation loss: 0.013676\n",
      "Epoch: 23 Train loss: 0.13358 Validation loss: 3.3653e-05\n",
      "Epoch: 24 Train loss: 0.10571 Validation loss: 0.0088233\n",
      "Epoch: 25 Train loss: 0.10971 Validation loss: 0.0068132\n",
      "Epoch: 26 Train loss: 0.099991 Validation loss: 0.020415\n",
      "Epoch: 27 Train loss: 0.12459 Validation loss: 7.9858e-06\n",
      "Epoch: 28 Train loss: 0.10756 Validation loss: 0.00042963\n",
      "Epoch: 29 Train loss: 0.10436 Validation loss: 0.01331\n",
      "Epoch: 30 Train loss: 0.11232 Validation loss: 6.2978e-06\n",
      "Epoch: 31 Train loss: 0.091506 Validation loss: 0.012752\n",
      "Epoch: 32 Train loss: 0.11459 Validation loss: 0.0062118\n",
      "Epoch: 33 Train loss: 0.075607 Validation loss: 0.0045607\n",
      "Epoch: 34 Train loss: 0.10746 Validation loss: 0.0002243\n",
      "Epoch: 35 Train loss: 0.097207 Validation loss: 0.018697\n",
      "Epoch: 36 Train loss: 0.067343 Validation loss: 0.00088865\n",
      "Epoch: 37 Train loss: 0.083182 Validation loss: 0.00037273\n",
      "Epoch: 38 Train loss: 0.11461 Validation loss: 0.00015492\n",
      "Epoch: 39 Train loss: 0.083019 Validation loss: 0.001469\n",
      "Epoch: 40 Train loss: 0.075676 Validation loss: 0.0020108\n",
      "Epoch: 41 Train loss: 0.081737 Validation loss: 0.0026533\n",
      "Epoch: 42 Train loss: 0.070065 Validation loss: 0.00039961\n",
      "Epoch: 43 Train loss: 0.061918 Validation loss: 0.0044641\n",
      "Epoch: 44 Train loss: 0.096076 Validation loss: 0.0016123\n",
      "Epoch: 45 Train loss: 0.064354 Validation loss: 0.0047202\n",
      "Epoch: 46 Train loss: 0.067867 Validation loss: 0.0047844\n",
      "Epoch: 47 Train loss: 0.088537 Validation loss: 0.0046069\n",
      "Epoch: 48 Train loss: 0.07072 Validation loss: 0.00011505\n",
      "Epoch: 49 Train loss: 0.067871 Validation loss: 0.010236\n",
      "Epoch: 50 Train loss: 0.083724 Validation loss: 0.0017334\n",
      "Epoch: 51 Train loss: 0.065301 Validation loss: 0.0053421\n",
      "Epoch: 52 Train loss: 0.068514 Validation loss: 0.011248\n",
      "Epoch: 53 Train loss: 0.045622 Validation loss: 0.0020145\n",
      "Epoch: 54 Train loss: 0.055964 Validation loss: 5.1488e-06\n",
      "Epoch: 55 Train loss: 0.07278 Validation loss: 0.00097597\n",
      "Epoch: 56 Train loss: 0.043508 Validation loss: 4.8035e-05\n",
      "Epoch: 57 Train loss: 0.075647 Validation loss: 0.0043081\n",
      "Epoch: 58 Train loss: 0.055203 Validation loss: 1.3442e-05\n",
      "Epoch: 59 Train loss: 0.058818 Validation loss: 0.00079198\n",
      "Epoch: 60 Train loss: 0.059383 Validation loss: 0.0052973\n",
      "Epoch: 61 Train loss: 0.03943 Validation loss: 0.00067058\n",
      "Epoch: 62 Train loss: 0.059362 Validation loss: 1.7342e-06\n",
      "Epoch: 63 Train loss: 0.054069 Validation loss: 0.001156\n",
      "Epoch: 64 Train loss: 0.044087 Validation loss: 0.0029985\n",
      "Epoch: 65 Train loss: 0.063901 Validation loss: 0.0008227\n",
      "Epoch: 66 Train loss: 0.063195 Validation loss: 0.013075\n",
      "Epoch: 67 Train loss: 0.062063 Validation loss: 0.0014858\n",
      "Epoch: 68 Train loss: 0.042234 Validation loss: 0.0018719\n",
      "Epoch: 69 Train loss: 0.037752 Validation loss: 0.0095306\n",
      "Epoch: 70 Train loss: 0.049157 Validation loss: 0.0034597\n",
      "Epoch: 71 Train loss: 0.058518 Validation loss: 0.00058794\n",
      "Epoch: 72 Train loss: 0.047723 Validation loss: 0.00020098\n",
      "Epoch: 73 Train loss: 0.043991 Validation loss: 0.0017148\n",
      "Epoch: 74 Train loss: 0.038125 Validation loss: 9.3054e-05\n",
      "Epoch: 75 Train loss: 0.044432 Validation loss: 0.00030906\n",
      "Epoch: 76 Train loss: 0.044116 Validation loss: 0.0043123\n",
      "Epoch: 77 Train loss: 0.041779 Validation loss: 0.0024404\n",
      "Epoch: 78 Train loss: 0.037455 Validation loss: 0.014959\n",
      "Epoch: 79 Train loss: 0.040266 Validation loss: 0.00058501\n",
      "Epoch: 80 Train loss: 0.045922 Validation loss: 0.014635\n",
      "Epoch: 81 Train loss: 0.03381 Validation loss: 0.0015907\n",
      "Epoch: 82 Train loss: 0.040766 Validation loss: 0.016117\n",
      "Epoch: 83 Train loss: 0.050254 Validation loss: 0.00035467\n",
      "Epoch: 84 Train loss: 0.039818 Validation loss: 0.0030231\n",
      "Epoch: 85 Train loss: 0.030197 Validation loss: 0.034483\n",
      "Epoch: 86 Train loss: 0.035208 Validation loss: 0.012419\n",
      "Epoch: 87 Train loss: 0.042073 Validation loss: 7.7671e-07\n",
      "Epoch: 88 Train loss: 0.039918 Validation loss: 0.006494\n",
      "Epoch: 89 Train loss: 0.03374 Validation loss: 0.0055739\n",
      "Epoch: 90 Train loss: 0.041197 Validation loss: 0.0080457\n",
      "Epoch: 91 Train loss: 0.043368 Validation loss: 0.010545\n",
      "Epoch: 92 Train loss: 0.032198 Validation loss: 0.0011126\n",
      "Epoch: 93 Train loss: 0.035855 Validation loss: 0.0059545\n",
      "Epoch: 94 Train loss: 0.032591 Validation loss: 0.023863\n",
      "Epoch: 95 Train loss: 0.02993 Validation loss: 0.0029317\n",
      "Epoch: 96 Train loss: 0.033453 Validation loss: 0.0020028\n",
      "Epoch: 97 Train loss: 0.040598 Validation loss: 0.0044073\n",
      "Epoch: 98 Train loss: 0.037478 Validation loss: 0.0078151\n",
      "Epoch: 99 Train loss: 0.042004 Validation loss: 0.0048035\n",
      "Epoch: 100 Train loss: 0.031338 Validation loss: 0.0040935\n",
      "Epoch: 101 Train loss: 0.035755 Validation loss: 0.00022958\n",
      "Epoch: 102 Train loss: 0.038089 Validation loss: 0.0048106\n",
      "Epoch: 103 Train loss: 0.0308 Validation loss: 3.6092e-06\n",
      "Epoch: 104 Train loss: 0.030246 Validation loss: 0.0023866\n",
      "Epoch: 105 Train loss: 0.032812 Validation loss: 0.015573\n",
      "Epoch: 106 Train loss: 0.031898 Validation loss: 0.002355\n",
      "Epoch: 107 Train loss: 0.039288 Validation loss: 0.0040851\n",
      "Epoch: 108 Train loss: 0.029134 Validation loss: 0.010076\n",
      "Epoch: 109 Train loss: 0.028697 Validation loss: 0.00014153\n",
      "Epoch: 110 Train loss: 0.028274 Validation loss: 0.0092918\n",
      "Epoch: 111 Train loss: 0.03422 Validation loss: 0.0018816\n",
      "Epoch: 112 Train loss: 0.029246 Validation loss: 0.0049858\n",
      "Epoch: 113 Train loss: 0.036464 Validation loss: 0.00012991\n",
      "Epoch: 114 Train loss: 0.036126 Validation loss: 0.0029939\n",
      "Epoch: 115 Train loss: 0.029163 Validation loss: 0.0012632\n",
      "Epoch: 116 Train loss: 0.036138 Validation loss: 0.014076\n",
      "Epoch: 117 Train loss: 0.039562 Validation loss: 0.00081128\n",
      "Epoch: 118 Train loss: 0.021835 Validation loss: 0.0011391\n",
      "Epoch: 119 Train loss: 0.035971 Validation loss: 0.00095016\n",
      "Epoch: 120 Train loss: 0.028946 Validation loss: 0.021612\n",
      "Epoch: 121 Train loss: 0.029176 Validation loss: 0.0059713\n",
      "Epoch: 122 Train loss: 0.032829 Validation loss: 0.0064753\n",
      "Epoch: 123 Train loss: 0.020871 Validation loss: 0.017145\n",
      "Epoch: 124 Train loss: 0.033923 Validation loss: 0.00043171\n",
      "Epoch: 125 Train loss: 0.02944 Validation loss: 0.0010095\n",
      "Epoch: 126 Train loss: 0.027981 Validation loss: 0.0008939\n",
      "Epoch: 127 Train loss: 0.034492 Validation loss: 0.0015793\n",
      "Epoch: 128 Train loss: 0.025651 Validation loss: 0.015061\n",
      "Epoch: 129 Train loss: 0.027184 Validation loss: 0.033905\n",
      "Epoch: 130 Train loss: 0.032836 Validation loss: 0.00068319\n",
      "Epoch: 131 Train loss: 0.033472 Validation loss: 0.0083048\n",
      "Epoch: 132 Train loss: 0.032931 Validation loss: 1.0699e-05\n",
      "Epoch: 133 Train loss: 0.023506 Validation loss: 0.015387\n",
      "Epoch: 134 Train loss: 0.02909 Validation loss: 0.01034\n",
      "Epoch: 135 Train loss: 0.02844 Validation loss: 0.0028452\n",
      "Epoch: 136 Train loss: 0.033203 Validation loss: 0.0023274\n",
      "Epoch: 137 Train loss: 0.025732 Validation loss: 0.00033165\n",
      "Epoch: 138 Train loss: 0.025277 Validation loss: 0.0010537\n",
      "Epoch: 139 Train loss: 0.029571 Validation loss: 0.010883\n",
      "Epoch: 140 Train loss: 0.02864 Validation loss: 0.00029498\n",
      "Epoch: 141 Train loss: 0.027283 Validation loss: 0.0018136\n",
      "Epoch: 142 Train loss: 0.026679 Validation loss: 0.0085147\n",
      "Epoch: 143 Train loss: 0.028806 Validation loss: 0.0052808\n",
      "Epoch: 144 Train loss: 0.029437 Validation loss: 0.0068258\n",
      "Epoch: 145 Train loss: 0.031852 Validation loss: 0.011046\n",
      "Epoch: 146 Train loss: 0.035951 Validation loss: 0.0056654\n",
      "Epoch: 147 Train loss: 0.030995 Validation loss: 0.0021295\n",
      "Epoch: 148 Train loss: 0.030193 Validation loss: 0.021645\n",
      "Epoch: 149 Train loss: 0.027519 Validation loss: 0.0061295\n",
      "Epoch: 150 Train loss: 0.032213 Validation loss: 5.2681e-05\n",
      "Epoch: 151 Train loss: 0.024552 Validation loss: 0.00016457\n",
      "Epoch: 152 Train loss: 0.025702 Validation loss: 0.0053178\n",
      "Epoch: 153 Train loss: 0.030347 Validation loss: 0.0018656\n",
      "Epoch: 154 Train loss: 0.043722 Validation loss: 0.00036595\n",
      "Epoch: 155 Train loss: 0.023957 Validation loss: 0.0046728\n",
      "Epoch: 156 Train loss: 0.025858 Validation loss: 0.00077535\n",
      "Epoch: 157 Train loss: 0.024913 Validation loss: 0.017509\n",
      "Epoch: 158 Train loss: 0.02995 Validation loss: 0.0028798\n",
      "Epoch: 159 Train loss: 0.034924 Validation loss: 1.3194e-07\n",
      "Epoch: 160 Train loss: 0.030439 Validation loss: 0.0018772\n",
      "Epoch: 161 Train loss: 0.029532 Validation loss: 0.00059631\n",
      "Epoch: 162 Train loss: 0.021588 Validation loss: 0.0062452\n",
      "Epoch: 163 Train loss: 0.033979 Validation loss: 0.0067561\n",
      "Epoch: 164 Train loss: 0.027088 Validation loss: 0.0068352\n",
      "Epoch: 165 Train loss: 0.025845 Validation loss: 0.001568\n",
      "Epoch: 166 Train loss: 0.023401 Validation loss: 0.012792\n",
      "Epoch: 167 Train loss: 0.033601 Validation loss: 0.0030905\n",
      "Epoch: 168 Train loss: 0.032893 Validation loss: 0.0041215\n",
      "Epoch: 169 Train loss: 0.030157 Validation loss: 0.01807\n",
      "Epoch: 170 Train loss: 0.021996 Validation loss: 0.0037542\n",
      "Epoch: 171 Train loss: 0.028106 Validation loss: 0.00023202\n",
      "Epoch: 172 Train loss: 0.020289 Validation loss: 0.0092656\n",
      "Epoch: 173 Train loss: 0.034031 Validation loss: 0.00126\n",
      "Epoch: 174 Train loss: 0.028249 Validation loss: 0.0075006\n",
      "Epoch: 175 Train loss: 0.020406 Validation loss: 0.0013599\n",
      "Epoch: 176 Train loss: 0.029386 Validation loss: 0.00038678\n",
      "Epoch: 177 Train loss: 0.02769 Validation loss: 0.0011312\n",
      "Epoch: 178 Train loss: 0.033458 Validation loss: 0.00010857\n",
      "Epoch: 179 Train loss: 0.02577 Validation loss: 6.5036e-05\n",
      "Epoch: 180 Train loss: 0.0348 Validation loss: 0.0012546\n",
      "Epoch: 181 Train loss: 0.023881 Validation loss: 8.886e-05\n",
      "Epoch: 182 Train loss: 0.029001 Validation loss: 0.017569\n",
      "Epoch: 183 Train loss: 0.022605 Validation loss: 0.00029165\n",
      "Epoch: 184 Train loss: 0.034834 Validation loss: 0.004359\n",
      "Epoch: 185 Train loss: 0.027715 Validation loss: 0.0032237\n",
      "Epoch: 186 Train loss: 0.025154 Validation loss: 0.0037147\n",
      "Epoch: 187 Train loss: 0.020344 Validation loss: 3.9169e-06\n",
      "Epoch: 188 Train loss: 0.027345 Validation loss: 0.012035\n",
      "Epoch: 189 Train loss: 0.033937 Validation loss: 0.0020295\n",
      "Epoch: 190 Train loss: 0.028717 Validation loss: 0.0020268\n",
      "Epoch: 191 Train loss: 0.02537 Validation loss: 0.0026042\n",
      "Epoch: 192 Train loss: 0.027561 Validation loss: 0.005088\n",
      "Epoch: 193 Train loss: 0.028409 Validation loss: 0.00088708\n",
      "Epoch: 194 Train loss: 0.024671 Validation loss: 0.0018051\n",
      "Epoch: 195 Train loss: 0.027905 Validation loss: 0.0014978\n",
      "Epoch: 196 Train loss: 0.026959 Validation loss: 0.0028384\n",
      "Epoch: 197 Train loss: 0.029136 Validation loss: 0.0042318\n",
      "Epoch: 198 Train loss: 0.025092 Validation loss: 0.001671\n",
      "Epoch: 199 Train loss: 0.026499 Validation loss: 0.0033456\n",
      "Epoch: 200 Train loss: 0.026919 Validation loss: 0.00027793\n",
      "Epoch: 201 Train loss: 0.027334 Validation loss: 4.0216e-06\n",
      "Epoch: 202 Train loss: 0.022354 Validation loss: 0.0031304\n",
      "Epoch: 203 Train loss: 0.020415 Validation loss: 0.0031355\n",
      "Epoch: 204 Train loss: 0.034072 Validation loss: 0.004847\n",
      "Epoch: 205 Train loss: 0.028481 Validation loss: 0.00027817\n",
      "Epoch: 206 Train loss: 0.029191 Validation loss: 0.00047027\n",
      "Epoch: 207 Train loss: 0.02426 Validation loss: 0.012183\n",
      "Epoch: 208 Train loss: 0.024361 Validation loss: 0.00033201\n",
      "Epoch: 209 Train loss: 0.035959 Validation loss: 0.016372\n",
      "Epoch: 210 Train loss: 0.018444 Validation loss: 0.0010386\n",
      "Epoch: 211 Train loss: 0.021452 Validation loss: 0.00045077\n",
      "Epoch: 212 Train loss: 0.039903 Validation loss: 0.0022697\n",
      "Epoch: 213 Train loss: 0.022053 Validation loss: 0.00041495\n",
      "Epoch: 214 Train loss: 0.023105 Validation loss: 0.0014526\n",
      "Epoch: 215 Train loss: 0.022516 Validation loss: 0.00012473\n",
      "Epoch: 216 Train loss: 0.027211 Validation loss: 0.00031049\n",
      "Epoch: 217 Train loss: 0.023695 Validation loss: 0.012223\n",
      "Epoch: 218 Train loss: 0.027019 Validation loss: 0.0025821\n",
      "Epoch: 219 Train loss: 0.024824 Validation loss: 0.0056981\n",
      "Epoch: 220 Train loss: 0.024016 Validation loss: 0.00089998\n",
      "Epoch: 221 Train loss: 0.023341 Validation loss: 0.0059825\n",
      "Epoch: 222 Train loss: 0.027082 Validation loss: 0.0079897\n",
      "Epoch: 223 Train loss: 0.01868 Validation loss: 0.00059174\n",
      "Epoch: 224 Train loss: 0.03133 Validation loss: 0.0067771\n",
      "Epoch: 225 Train loss: 0.028613 Validation loss: 5.9595e-05\n",
      "Epoch: 226 Train loss: 0.022765 Validation loss: 0.0014461\n",
      "Epoch: 227 Train loss: 0.027492 Validation loss: 0.012312\n",
      "Epoch: 228 Train loss: 0.01629 Validation loss: 0.008943\n",
      "Epoch: 229 Train loss: 0.029763 Validation loss: 0.0013995\n",
      "Epoch: 230 Train loss: 0.023532 Validation loss: 0.0031907\n",
      "Epoch: 231 Train loss: 0.02147 Validation loss: 0.019487\n",
      "Epoch: 232 Train loss: 0.031965 Validation loss: 0.0029402\n",
      "Epoch: 233 Train loss: 0.016638 Validation loss: 0.0017202\n",
      "Epoch: 234 Train loss: 0.028521 Validation loss: 0.0097838\n",
      "Epoch: 235 Train loss: 0.024031 Validation loss: 0.0024427\n",
      "Epoch: 236 Train loss: 0.028639 Validation loss: 0.0036612\n",
      "Epoch: 237 Train loss: 0.016463 Validation loss: 0.0021077\n",
      "Epoch: 238 Train loss: 0.029331 Validation loss: 6.6548e-06\n",
      "Epoch: 239 Train loss: 0.0291 Validation loss: 0.0086449\n",
      "Epoch: 240 Train loss: 0.024354 Validation loss: 7.6126e-06\n",
      "Epoch: 241 Train loss: 0.031701 Validation loss: 0.00097836\n",
      "Epoch: 242 Train loss: 0.024106 Validation loss: 0.0021474\n",
      "Epoch: 243 Train loss: 0.026252 Validation loss: 0.0089281\n",
      "Epoch: 244 Train loss: 0.02441 Validation loss: 0.0057836\n",
      "Epoch: 245 Train loss: 0.026689 Validation loss: 0.0031192\n",
      "Epoch: 246 Train loss: 0.020364 Validation loss: 0.017277\n",
      "Epoch: 247 Train loss: 0.028889 Validation loss: 0.010376\n",
      "Epoch: 248 Train loss: 0.017555 Validation loss: 0.002883\n",
      "Epoch: 249 Train loss: 0.025186 Validation loss: 2.9659e-05\n",
      "Epoch: 250 Train loss: 0.026165 Validation loss: 0.0041035\n",
      "Epoch: 251 Train loss: 0.01822 Validation loss: 0.014474\n",
      "Epoch: 252 Train loss: 0.023935 Validation loss: 0.0058409\n",
      "Epoch: 253 Train loss: 0.021742 Validation loss: 9.4156e-05\n",
      "Epoch: 254 Train loss: 0.025759 Validation loss: 0.0043191\n",
      "Epoch: 255 Train loss: 0.028924 Validation loss: 0.0041828\n",
      "Epoch: 256 Train loss: 0.017695 Validation loss: 0.0043201\n",
      "Epoch: 257 Train loss: 0.025827 Validation loss: 4.4631e-05\n",
      "Epoch: 258 Train loss: 0.026056 Validation loss: 0.0057822\n",
      "Epoch: 259 Train loss: 0.014329 Validation loss: 0.0050076\n",
      "Epoch: 260 Train loss: 0.027636 Validation loss: 0.00047281\n",
      "Epoch: 261 Train loss: 0.0318 Validation loss: 0.0011136\n",
      "Epoch: 262 Train loss: 0.027108 Validation loss: 0.020647\n",
      "Epoch: 263 Train loss: 0.016572 Validation loss: 0.0021194\n",
      "Epoch: 264 Train loss: 0.024696 Validation loss: 0.0066569\n",
      "Epoch: 265 Train loss: 0.020657 Validation loss: 0.0021031\n",
      "Epoch: 266 Train loss: 0.027756 Validation loss: 0.0062847\n",
      "Epoch: 267 Train loss: 0.023494 Validation loss: 0.010131\n",
      "Epoch: 268 Train loss: 0.022224 Validation loss: 0.0062277\n",
      "Epoch: 269 Train loss: 0.026608 Validation loss: 0.0068892\n",
      "Epoch: 270 Train loss: 0.022754 Validation loss: 0.0029504\n",
      "Epoch: 271 Train loss: 0.022566 Validation loss: 0.00031441\n",
      "Epoch: 272 Train loss: 0.027324 Validation loss: 0.020355\n",
      "Epoch: 273 Train loss: 0.02647 Validation loss: 0.0056192\n",
      "Epoch: 274 Train loss: 0.025296 Validation loss: 0.0043923\n",
      "Epoch: 275 Train loss: 0.020251 Validation loss: 0.0013819\n",
      "Epoch: 276 Train loss: 0.018794 Validation loss: 0.0083921\n",
      "Epoch: 277 Train loss: 0.018855 Validation loss: 1.0621e-05\n",
      "Epoch: 278 Train loss: 0.024754 Validation loss: 8.7164e-05\n",
      "Epoch: 279 Train loss: 0.023222 Validation loss: 0.0018035\n",
      "Epoch: 280 Train loss: 0.027575 Validation loss: 0.00057715\n",
      "Epoch: 281 Train loss: 0.019073 Validation loss: 0.0043229\n",
      "Epoch: 282 Train loss: 0.023621 Validation loss: 0.0038279\n",
      "Epoch: 283 Train loss: 0.019995 Validation loss: 5.4598e-05\n",
      "Epoch: 284 Train loss: 0.02131 Validation loss: 0.0032837\n",
      "Epoch: 285 Train loss: 0.022819 Validation loss: 0.0046333\n",
      "Epoch: 286 Train loss: 0.028441 Validation loss: 1.1176e-05\n",
      "Epoch: 287 Train loss: 0.023792 Validation loss: 0.00024164\n",
      "Epoch: 288 Train loss: 0.025003 Validation loss: 1.8925e-06\n",
      "Epoch: 289 Train loss: 0.022436 Validation loss: 0.0051553\n",
      "Epoch: 290 Train loss: 0.024571 Validation loss: 0.00073794\n",
      "Epoch: 291 Train loss: 0.030702 Validation loss: 0.00045174\n",
      "Epoch: 292 Train loss: 0.018087 Validation loss: 0.0054342\n",
      "Epoch: 293 Train loss: 0.037346 Validation loss: 0.0023205\n",
      "Epoch: 294 Train loss: 0.016759 Validation loss: 0.002484\n",
      "Epoch: 295 Train loss: 0.030722 Validation loss: 0.0044167\n",
      "Epoch: 296 Train loss: 0.019049 Validation loss: 0.0019327\n",
      "Epoch: 297 Train loss: 0.025619 Validation loss: 0.026254\n",
      "Epoch: 298 Train loss: 0.022956 Validation loss: 0.0023907\n",
      "Epoch: 299 Train loss: 0.028271 Validation loss: 0.00049247\n",
      "Epoch: 300 Train loss: 0.024963 Validation loss: 0.002748\n",
      "Epoch: 301 Train loss: 0.02118 Validation loss: 0.00039476\n",
      "Epoch: 302 Train loss: 0.022894 Validation loss: 0.00085326\n",
      "Epoch: 303 Train loss: 0.024986 Validation loss: 0.00012338\n",
      "Epoch: 304 Train loss: 0.021366 Validation loss: 0.00043441\n",
      "Epoch: 305 Train loss: 0.02351 Validation loss: 0.00015524\n",
      "Epoch: 306 Train loss: 0.028045 Validation loss: 0.0055153\n",
      "Epoch: 307 Train loss: 0.018292 Validation loss: 0.0004089\n",
      "Epoch: 308 Train loss: 0.024241 Validation loss: 0.0019177\n",
      "Epoch: 309 Train loss: 0.023054 Validation loss: 0.0031405\n",
      "Epoch: 310 Train loss: 0.022439 Validation loss: 0.00079363\n",
      "Epoch: 311 Train loss: 0.026541 Validation loss: 0.0027312\n",
      "Epoch: 312 Train loss: 0.023099 Validation loss: 0.0091589\n",
      "Epoch: 313 Train loss: 0.029658 Validation loss: 0.00015399\n",
      "Epoch: 314 Train loss: 0.019764 Validation loss: 0.0014573\n",
      "Epoch: 315 Train loss: 0.021733 Validation loss: 0.00064236\n",
      "Epoch: 316 Train loss: 0.025298 Validation loss: 0.000997\n",
      "Epoch: 317 Train loss: 0.0233 Validation loss: 2.6146e-05\n",
      "Epoch: 318 Train loss: 0.015374 Validation loss: 0.0013728\n",
      "Epoch: 319 Train loss: 0.021254 Validation loss: 0.0017036\n",
      "Epoch: 320 Train loss: 0.02469 Validation loss: 0.0005757\n",
      "Epoch: 321 Train loss: 0.024306 Validation loss: 0.0065562\n",
      "Epoch: 322 Train loss: 0.022701 Validation loss: 0.00035966\n",
      "Epoch: 323 Train loss: 0.024858 Validation loss: 0.0031156\n",
      "Epoch: 324 Train loss: 0.026738 Validation loss: 0.00044866\n",
      "Epoch: 325 Train loss: 0.02076 Validation loss: 0.0008699\n",
      "Epoch: 326 Train loss: 0.023956 Validation loss: 0.0013495\n",
      "Epoch: 327 Train loss: 0.023518 Validation loss: 0.0004514\n",
      "Epoch: 328 Train loss: 0.019575 Validation loss: 0.00024022\n",
      "Epoch: 329 Train loss: 0.017696 Validation loss: 0.00099643\n",
      "Epoch: 330 Train loss: 0.024884 Validation loss: 0.00037111\n",
      "Epoch: 331 Train loss: 0.021025 Validation loss: 0.0016904\n",
      "Epoch: 332 Train loss: 0.024244 Validation loss: 0.0020792\n",
      "Epoch: 333 Train loss: 0.025075 Validation loss: 3.6934e-06\n",
      "Epoch: 334 Train loss: 0.021218 Validation loss: 0.0036047\n",
      "Epoch: 335 Train loss: 0.021377 Validation loss: 0.00014502\n",
      "Epoch: 336 Train loss: 0.029304 Validation loss: 0.001224\n",
      "Epoch: 337 Train loss: 0.020365 Validation loss: 0.00017128\n",
      "Epoch: 338 Train loss: 0.02216 Validation loss: 0.0096866\n",
      "Epoch: 339 Train loss: 0.024869 Validation loss: 0.003785\n",
      "Epoch: 340 Train loss: 0.019412 Validation loss: 0.011843\n",
      "Epoch: 341 Train loss: 0.025758 Validation loss: 0.00057701\n",
      "Epoch: 342 Train loss: 0.018238 Validation loss: 0.0016647\n",
      "Epoch: 343 Train loss: 0.024942 Validation loss: 0.011637\n",
      "Epoch: 344 Train loss: 0.02155 Validation loss: 0.0004046\n",
      "Epoch: 345 Train loss: 0.018491 Validation loss: 0.0073416\n",
      "Epoch: 346 Train loss: 0.019085 Validation loss: 1.1717e-05\n",
      "Epoch: 347 Train loss: 0.023439 Validation loss: 0.00088704\n",
      "Epoch: 348 Train loss: 0.019717 Validation loss: 0.0015165\n",
      "Epoch: 349 Train loss: 0.023984 Validation loss: 0.015233\n",
      "Epoch: 350 Train loss: 0.01998 Validation loss: 0.00961\n",
      "Epoch: 351 Train loss: 0.018853 Validation loss: 0.00071305\n",
      "Epoch: 352 Train loss: 0.024964 Validation loss: 0.0033149\n",
      "Epoch: 353 Train loss: 0.015475 Validation loss: 0.0032229\n",
      "Epoch: 354 Train loss: 0.026212 Validation loss: 0.00032661\n",
      "Epoch: 355 Train loss: 0.022628 Validation loss: 0.018475\n",
      "Epoch: 356 Train loss: 0.026359 Validation loss: 0.00075665\n",
      "Epoch: 357 Train loss: 0.020493 Validation loss: 0.0019496\n",
      "Epoch: 358 Train loss: 0.022967 Validation loss: 0.00014944\n",
      "Epoch: 359 Train loss: 0.022624 Validation loss: 0.0038948\n",
      "Epoch: 360 Train loss: 0.024581 Validation loss: 0.0035051\n",
      "Epoch: 361 Train loss: 0.015515 Validation loss: 0.0026923\n",
      "Epoch: 362 Train loss: 0.025736 Validation loss: 0.0046846\n",
      "Epoch: 363 Train loss: 0.022224 Validation loss: 0.0044213\n",
      "Epoch: 364 Train loss: 0.01643 Validation loss: 0.0003121\n",
      "Epoch: 365 Train loss: 0.028415 Validation loss: 0.0011976\n",
      "Epoch: 366 Train loss: 0.023911 Validation loss: 0.0025417\n",
      "Epoch: 367 Train loss: 0.02159 Validation loss: 0.0039734\n",
      "Epoch: 368 Train loss: 0.021441 Validation loss: 0.0062012\n",
      "Epoch: 369 Train loss: 0.032185 Validation loss: 0.0028925\n",
      "Epoch: 370 Train loss: 0.022041 Validation loss: 0.015979\n",
      "Epoch: 371 Train loss: 0.012815 Validation loss: 0.0032151\n",
      "Epoch: 372 Train loss: 0.024479 Validation loss: 0.0064093\n",
      "Epoch: 373 Train loss: 0.025092 Validation loss: 0.0060963\n",
      "Epoch: 374 Train loss: 0.016889 Validation loss: 1.75e-06\n",
      "Epoch: 375 Train loss: 0.019425 Validation loss: 0.0015229\n",
      "Epoch: 376 Train loss: 0.027423 Validation loss: 0.0034418\n",
      "Epoch: 377 Train loss: 0.026107 Validation loss: 0.009047\n",
      "Epoch: 378 Train loss: 0.018678 Validation loss: 1.5116e-05\n",
      "Epoch: 379 Train loss: 0.029902 Validation loss: 0.005016\n",
      "Epoch: 380 Train loss: 0.017497 Validation loss: 0.0053649\n",
      "Epoch: 381 Train loss: 0.025403 Validation loss: 0.0011124\n",
      "Epoch: 382 Train loss: 0.015139 Validation loss: 0.0063378\n",
      "Epoch: 383 Train loss: 0.022471 Validation loss: 0.001003\n",
      "Epoch: 384 Train loss: 0.020449 Validation loss: 0.013269\n",
      "Epoch: 385 Train loss: 0.024402 Validation loss: 0.0009693\n",
      "Epoch: 386 Train loss: 0.020019 Validation loss: 0.0093736\n",
      "Epoch: 387 Train loss: 0.022466 Validation loss: 0.0047416\n",
      "Epoch: 388 Train loss: 0.02764 Validation loss: 0.0058576\n",
      "Epoch: 389 Train loss: 0.017169 Validation loss: 0.0034309\n",
      "Epoch: 390 Train loss: 0.022642 Validation loss: 0.0098851\n",
      "Epoch: 391 Train loss: 0.018861 Validation loss: 0.0024112\n",
      "Epoch: 392 Train loss: 0.024357 Validation loss: 0.018272\n",
      "Epoch: 393 Train loss: 0.019341 Validation loss: 0.00035638\n",
      "Epoch: 394 Train loss: 0.021145 Validation loss: 9.7585e-05\n",
      "Epoch: 395 Train loss: 0.018925 Validation loss: 0.0028823\n",
      "Epoch: 396 Train loss: 0.024476 Validation loss: 0.015283\n",
      "Epoch: 397 Train loss: 0.018338 Validation loss: 3.3281e-06\n",
      "Epoch: 398 Train loss: 0.021828 Validation loss: 0.010933\n",
      "Epoch: 399 Train loss: 0.021902 Validation loss: 0.0085926\n",
      "Epoch: 400 Train loss: 0.026702 Validation loss: 0.0079478\n",
      "Epoch: 401 Train loss: 0.019333 Validation loss: 2.2058e-05\n",
      "Epoch: 402 Train loss: 0.027532 Validation loss: 0.0064029\n",
      "Epoch: 403 Train loss: 0.014723 Validation loss: 0.00014035\n",
      "Epoch: 404 Train loss: 0.017599 Validation loss: 0.0039258\n",
      "Epoch: 405 Train loss: 0.020106 Validation loss: 0.0020806\n",
      "Epoch: 406 Train loss: 0.019015 Validation loss: 0.0017424\n",
      "Epoch: 407 Train loss: 0.022637 Validation loss: 0.014486\n",
      "Epoch: 408 Train loss: 0.022619 Validation loss: 0.00031001\n",
      "Epoch: 409 Train loss: 0.017366 Validation loss: 0.0081685\n",
      "Epoch: 410 Train loss: 0.021033 Validation loss: 0.0092971\n",
      "Epoch: 411 Train loss: 0.019481 Validation loss: 0.001155\n",
      "Epoch: 412 Train loss: 0.02107 Validation loss: 0.00028726\n",
      "Epoch: 413 Train loss: 0.020598 Validation loss: 0.0033102\n",
      "Epoch: 414 Train loss: 0.024618 Validation loss: 0.00017643\n",
      "Epoch: 415 Train loss: 0.012573 Validation loss: 0.0033358\n",
      "Epoch: 416 Train loss: 0.024589 Validation loss: 0.001583\n",
      "Epoch: 417 Train loss: 0.016233 Validation loss: 0.00031947\n",
      "Epoch: 418 Train loss: 0.030018 Validation loss: 0.01133\n",
      "Epoch: 419 Train loss: 0.016141 Validation loss: 0.0027699\n",
      "Epoch: 420 Train loss: 0.026169 Validation loss: 0.0070626\n",
      "Epoch: 421 Train loss: 0.020731 Validation loss: 0.0026586\n",
      "Epoch: 422 Train loss: 0.021312 Validation loss: 2.7508e-05\n",
      "Epoch: 423 Train loss: 0.016084 Validation loss: 0.0077971\n",
      "Epoch: 424 Train loss: 0.020958 Validation loss: 0.0092641\n",
      "Epoch: 425 Train loss: 0.02228 Validation loss: 0.011077\n",
      "Epoch: 426 Train loss: 0.02093 Validation loss: 0.015026\n",
      "Epoch: 427 Train loss: 0.02469 Validation loss: 0.0010652\n",
      "Epoch: 428 Train loss: 0.022144 Validation loss: 0.0094672\n",
      "Epoch: 429 Train loss: 0.026343 Validation loss: 0.0037958\n",
      "Epoch: 430 Train loss: 0.01614 Validation loss: 0.0052732\n",
      "Epoch: 431 Train loss: 0.020372 Validation loss: 0.007517\n",
      "Epoch: 432 Train loss: 0.017987 Validation loss: 0.00053321\n",
      "Epoch: 433 Train loss: 0.020952 Validation loss: 0.0057196\n",
      "Epoch: 434 Train loss: 0.026283 Validation loss: 0.0039918\n",
      "Epoch: 435 Train loss: 0.015552 Validation loss: 0.00066926\n",
      "Epoch: 436 Train loss: 0.024 Validation loss: 0.0064175\n",
      "Epoch: 437 Train loss: 0.018629 Validation loss: 0.0042446\n",
      "Epoch: 438 Train loss: 0.026095 Validation loss: 0.00058624\n",
      "Epoch: 439 Train loss: 0.020143 Validation loss: 0.00051373\n",
      "Epoch: 440 Train loss: 0.019616 Validation loss: 0.00045759\n",
      "Epoch: 441 Train loss: 0.022523 Validation loss: 1.6064e-05\n",
      "Epoch: 442 Train loss: 0.013381 Validation loss: 0.0077751\n",
      "Epoch: 443 Train loss: 0.023635 Validation loss: 0.00013616\n",
      "Epoch: 444 Train loss: 0.018773 Validation loss: 0.0083324\n",
      "Epoch: 445 Train loss: 0.026437 Validation loss: 0.0086887\n",
      "Epoch: 446 Train loss: 0.020112 Validation loss: 0.0026908\n",
      "Epoch: 447 Train loss: 0.029268 Validation loss: 0.010613\n",
      "Epoch: 448 Train loss: 0.017195 Validation loss: 0.0029438\n",
      "Epoch: 449 Train loss: 0.017009 Validation loss: 6.8659e-06\n",
      "Epoch: 450 Train loss: 0.023552 Validation loss: 0.0082903\n",
      "Epoch: 451 Train loss: 0.02197 Validation loss: 0.00024111\n",
      "Epoch: 452 Train loss: 0.025516 Validation loss: 8.0352e-05\n",
      "Epoch: 453 Train loss: 0.020717 Validation loss: 0.0015403\n",
      "Epoch: 454 Train loss: 0.021309 Validation loss: 2.4044e-05\n",
      "Epoch: 455 Train loss: 0.020022 Validation loss: 0.00011946\n",
      "Epoch: 456 Train loss: 0.022494 Validation loss: 0.00055244\n",
      "Epoch: 457 Train loss: 0.015399 Validation loss: 0.014334\n",
      "Epoch: 458 Train loss: 0.022546 Validation loss: 0.00022671\n",
      "Epoch: 459 Train loss: 0.024025 Validation loss: 0.001019\n",
      "Epoch: 460 Train loss: 0.01545 Validation loss: 0.0034122\n",
      "Epoch: 461 Train loss: 0.02249 Validation loss: 0.0013051\n",
      "Epoch: 462 Train loss: 0.024666 Validation loss: 0.00083403\n",
      "Epoch: 463 Train loss: 0.015472 Validation loss: 0.0020104\n",
      "Epoch: 464 Train loss: 0.020519 Validation loss: 0.0060654\n",
      "Epoch: 465 Train loss: 0.025048 Validation loss: 0.0025507\n",
      "Epoch: 466 Train loss: 0.019834 Validation loss: 9.5548e-05\n",
      "Epoch: 467 Train loss: 0.016433 Validation loss: 0.00023117\n",
      "Epoch: 468 Train loss: 0.028858 Validation loss: 3.4461e-05\n",
      "Epoch: 469 Train loss: 0.01842 Validation loss: 0.0015249\n",
      "Epoch: 470 Train loss: 0.019419 Validation loss: 0.015822\n",
      "Epoch: 471 Train loss: 0.019456 Validation loss: 0.0093467\n",
      "Epoch: 472 Train loss: 0.019344 Validation loss: 0.0067127\n",
      "Epoch: 473 Train loss: 0.020632 Validation loss: 0.0013269\n",
      "Epoch: 474 Train loss: 0.021972 Validation loss: 0.00098759\n",
      "Epoch: 475 Train loss: 0.017406 Validation loss: 0.0016157\n",
      "Epoch: 476 Train loss: 0.020662 Validation loss: 0.0055883\n",
      "Epoch: 477 Train loss: 0.023019 Validation loss: 0.0037478\n",
      "Epoch: 478 Train loss: 0.018547 Validation loss: 0.0015961\n",
      "Epoch: 479 Train loss: 0.019092 Validation loss: 0.0018558\n",
      "Epoch: 480 Train loss: 0.020903 Validation loss: 0.0057324\n",
      "Epoch: 481 Train loss: 0.022037 Validation loss: 0.021037\n",
      "Epoch: 482 Train loss: 0.016603 Validation loss: 0.0016776\n",
      "Epoch: 483 Train loss: 0.024895 Validation loss: 0.0080569\n",
      "Epoch: 484 Train loss: 0.016362 Validation loss: 0.0061432\n",
      "Epoch: 485 Train loss: 0.019714 Validation loss: 0.010356\n",
      "Epoch: 486 Train loss: 0.015302 Validation loss: 0.0048459\n",
      "Epoch: 487 Train loss: 0.026003 Validation loss: 0.00013294\n",
      "Epoch: 488 Train loss: 0.020564 Validation loss: 0.0052555\n",
      "Epoch: 489 Train loss: 0.018693 Validation loss: 0.0034804\n",
      "Epoch: 490 Train loss: 0.013994 Validation loss: 0.0059653\n",
      "Epoch: 491 Train loss: 0.016317 Validation loss: 0.00028558\n",
      "Epoch: 492 Train loss: 0.023907 Validation loss: 0.0027051\n",
      "Epoch: 493 Train loss: 0.019235 Validation loss: 3.4597e-05\n",
      "Epoch: 494 Train loss: 0.021254 Validation loss: 0.0035977\n",
      "Epoch: 495 Train loss: 0.019182 Validation loss: 0.0012158\n",
      "Epoch: 496 Train loss: 0.022342 Validation loss: 0.00050181\n",
      "Epoch: 497 Train loss: 0.018719 Validation loss: 0.0020442\n",
      "Epoch: 498 Train loss: 0.023226 Validation loss: 0.00023293\n",
      "Epoch: 499 Train loss: 0.020279 Validation loss: 0.0017865\n",
      "Epoch: 500 Train loss: 0.01531 Validation loss: 9.0269e-09\n",
      "Epoch: 501 Train loss: 0.025281 Validation loss: 0.0072267\n",
      "Epoch: 502 Train loss: 0.017303 Validation loss: 2.1753e-05\n",
      "Epoch: 503 Train loss: 0.022235 Validation loss: 0.0022043\n",
      "Epoch: 504 Train loss: 0.021121 Validation loss: 0.00038851\n",
      "Epoch: 505 Train loss: 0.023445 Validation loss: 0.0074405\n",
      "Epoch: 506 Train loss: 0.014679 Validation loss: 0.0053081\n",
      "Epoch: 507 Train loss: 0.022177 Validation loss: 0.0027953\n",
      "Epoch: 508 Train loss: 0.020556 Validation loss: 0.0017384\n",
      "Epoch: 509 Train loss: 0.019494 Validation loss: 0.0019518\n",
      "Epoch: 510 Train loss: 0.021859 Validation loss: 0.0001474\n",
      "Epoch: 511 Train loss: 0.015413 Validation loss: 6.4387e-05\n",
      "Epoch: 512 Train loss: 0.018346 Validation loss: 0.00044156\n",
      "Epoch: 513 Train loss: 0.017229 Validation loss: 0.0061491\n",
      "Epoch: 514 Train loss: 0.022733 Validation loss: 0.012833\n",
      "Epoch: 515 Train loss: 0.01859 Validation loss: 7.7667e-05\n",
      "Epoch: 516 Train loss: 0.018352 Validation loss: 0.0056324\n",
      "Epoch: 517 Train loss: 0.026706 Validation loss: 9.1724e-05\n",
      "Epoch: 518 Train loss: 0.018628 Validation loss: 0.0038557\n",
      "Epoch: 519 Train loss: 0.01918 Validation loss: 0.00065576\n",
      "Epoch: 520 Train loss: 0.016713 Validation loss: 0.00033617\n",
      "Epoch: 521 Train loss: 0.018412 Validation loss: 0.013293\n",
      "Epoch: 522 Train loss: 0.016144 Validation loss: 1.0794e-07\n",
      "Epoch: 523 Train loss: 0.024652 Validation loss: 0.0065665\n",
      "Epoch: 524 Train loss: 0.01844 Validation loss: 0.0003096\n",
      "Epoch: 525 Train loss: 0.017989 Validation loss: 0.0037573\n",
      "Epoch: 526 Train loss: 0.026437 Validation loss: 0.0019331\n",
      "Epoch: 527 Train loss: 0.016249 Validation loss: 8.8008e-05\n",
      "Epoch: 528 Train loss: 0.015675 Validation loss: 0.023651\n",
      "Epoch: 529 Train loss: 0.021983 Validation loss: 0.00010302\n",
      "Epoch: 530 Train loss: 0.014437 Validation loss: 0.00096048\n",
      "Epoch: 531 Train loss: 0.027157 Validation loss: 0.0019277\n",
      "Epoch: 532 Train loss: 0.014592 Validation loss: 0.0064731\n",
      "Epoch: 533 Train loss: 0.022114 Validation loss: 0.012804\n",
      "Epoch: 534 Train loss: 0.018657 Validation loss: 0.0015239\n",
      "Epoch: 535 Train loss: 0.023118 Validation loss: 0.0031272\n",
      "Epoch: 536 Train loss: 0.017097 Validation loss: 0.00093924\n",
      "Epoch: 537 Train loss: 0.018327 Validation loss: 0.00056557\n",
      "Epoch: 538 Train loss: 0.017876 Validation loss: 0.0053438\n",
      "Epoch: 539 Train loss: 0.0176 Validation loss: 0.00017969\n",
      "Epoch: 540 Train loss: 0.017953 Validation loss: 0.0013196\n",
      "Epoch: 541 Train loss: 0.022793 Validation loss: 0.00020658\n",
      "Epoch: 542 Train loss: 0.02001 Validation loss: 0.0051507\n",
      "Epoch: 543 Train loss: 0.022818 Validation loss: 0.0043078\n",
      "Epoch: 544 Train loss: 0.020132 Validation loss: 0.0037446\n",
      "Epoch: 545 Train loss: 0.014052 Validation loss: 0.0079933\n",
      "Epoch: 546 Train loss: 0.022117 Validation loss: 0.0010763\n",
      "Epoch: 547 Train loss: 0.016529 Validation loss: 0.0053562\n",
      "Epoch: 548 Train loss: 0.021871 Validation loss: 0.014445\n",
      "Epoch: 549 Train loss: 0.017046 Validation loss: 8.0601e-05\n",
      "Epoch: 550 Train loss: 0.021948 Validation loss: 0.002194\n",
      "Epoch: 551 Train loss: 0.018609 Validation loss: 0.0078152\n",
      "Epoch: 552 Train loss: 0.020112 Validation loss: 0.0034096\n",
      "Epoch: 553 Train loss: 0.022396 Validation loss: 0.00051429\n",
      "Epoch: 554 Train loss: 0.021345 Validation loss: 4.9415e-05\n",
      "Epoch: 555 Train loss: 0.020779 Validation loss: 0.0043901\n",
      "Epoch: 556 Train loss: 0.013786 Validation loss: 0.0004858\n",
      "Epoch: 557 Train loss: 0.022996 Validation loss: 0.0025355\n",
      "Epoch: 558 Train loss: 0.015924 Validation loss: 0.0087461\n",
      "Epoch: 559 Train loss: 0.018369 Validation loss: 0.0064756\n",
      "Epoch: 560 Train loss: 0.021036 Validation loss: 0.0011665\n",
      "Epoch: 561 Train loss: 0.022651 Validation loss: 0.00057502\n",
      "Epoch: 562 Train loss: 0.013911 Validation loss: 0.013094\n",
      "Epoch: 563 Train loss: 0.020173 Validation loss: 0.0026665\n",
      "Epoch: 564 Train loss: 0.017421 Validation loss: 0.0014467\n",
      "Epoch: 565 Train loss: 0.016646 Validation loss: 0.0006303\n",
      "Epoch: 566 Train loss: 0.020374 Validation loss: 0.0055373\n",
      "Epoch: 567 Train loss: 0.022263 Validation loss: 0.0034581\n",
      "Epoch: 568 Train loss: 0.017635 Validation loss: 0.0030211\n",
      "Epoch: 569 Train loss: 0.026199 Validation loss: 0.0013159\n",
      "Epoch: 570 Train loss: 0.014592 Validation loss: 0.0025856\n",
      "Epoch: 571 Train loss: 0.025202 Validation loss: 0.0055831\n",
      "Epoch: 572 Train loss: 0.018102 Validation loss: 5.6793e-05\n",
      "Epoch: 573 Train loss: 0.018314 Validation loss: 8.9045e-06\n",
      "Epoch: 574 Train loss: 0.017906 Validation loss: 0.0029396\n",
      "Epoch: 575 Train loss: 0.019252 Validation loss: 0.0009035\n",
      "Epoch: 576 Train loss: 0.016874 Validation loss: 0.0098936\n",
      "Epoch: 577 Train loss: 0.022304 Validation loss: 0.00035364\n",
      "Epoch: 578 Train loss: 0.017868 Validation loss: 0.001298\n",
      "Epoch: 579 Train loss: 0.020952 Validation loss: 0.00059138\n",
      "Epoch: 580 Train loss: 0.019358 Validation loss: 0.0069046\n",
      "Epoch: 581 Train loss: 0.023315 Validation loss: 0.0022492\n",
      "Epoch: 582 Train loss: 0.018812 Validation loss: 0.00097102\n",
      "Epoch: 583 Train loss: 0.016723 Validation loss: 0.00017544\n",
      "Epoch: 584 Train loss: 0.020654 Validation loss: 0.0019274\n",
      "Epoch: 585 Train loss: 0.01849 Validation loss: 0.0018753\n",
      "Epoch: 586 Train loss: 0.02073 Validation loss: 0.005542\n",
      "Epoch: 587 Train loss: 0.016728 Validation loss: 0.0058537\n",
      "Epoch: 588 Train loss: 0.023964 Validation loss: 0.0025156\n",
      "Epoch: 589 Train loss: 0.012541 Validation loss: 0.0023301\n",
      "Epoch: 590 Train loss: 0.02539 Validation loss: 0.0021465\n",
      "Epoch: 591 Train loss: 0.012531 Validation loss: 0.0001695\n",
      "Epoch: 592 Train loss: 0.026627 Validation loss: 0.00012286\n",
      "Epoch: 593 Train loss: 0.015268 Validation loss: 0.0011433\n",
      "Epoch: 594 Train loss: 0.022599 Validation loss: 0.00013021\n",
      "Epoch: 595 Train loss: 0.015757 Validation loss: 0.009081\n",
      "Epoch: 596 Train loss: 0.022377 Validation loss: 3.2779e-05\n",
      "Epoch: 597 Train loss: 0.019034 Validation loss: 0.0037671\n",
      "Epoch: 598 Train loss: 0.017242 Validation loss: 0.00027778\n",
      "Epoch: 599 Train loss: 0.021332 Validation loss: 0.00032304\n",
      "Epoch: 600 Train loss: 0.020956 Validation loss: 0.0094344\n",
      "Epoch: 601 Train loss: 0.017981 Validation loss: 0.0067025\n",
      "Epoch: 602 Train loss: 0.01985 Validation loss: 0.00017813\n",
      "Epoch: 603 Train loss: 0.018796 Validation loss: 0.012122\n",
      "Epoch: 604 Train loss: 0.014342 Validation loss: 0.0091007\n",
      "Epoch: 605 Train loss: 0.019282 Validation loss: 0.0082434\n",
      "Epoch: 606 Train loss: 0.020882 Validation loss: 0.0019933\n",
      "Epoch: 607 Train loss: 0.015698 Validation loss: 0.00066004\n",
      "Epoch: 608 Train loss: 0.02398 Validation loss: 0.0024727\n",
      "Epoch: 609 Train loss: 0.018887 Validation loss: 0.0075465\n",
      "Epoch: 610 Train loss: 0.017159 Validation loss: 0.013767\n",
      "Epoch: 611 Train loss: 0.020117 Validation loss: 0.011199\n",
      "Epoch: 612 Train loss: 0.018562 Validation loss: 0.0080912\n",
      "Epoch: 613 Train loss: 0.017255 Validation loss: 0.0050587\n",
      "Epoch: 614 Train loss: 0.019717 Validation loss: 0.0031942\n",
      "Epoch: 615 Train loss: 0.021453 Validation loss: 0.0003824\n",
      "Epoch: 616 Train loss: 0.027171 Validation loss: 0.0006556\n",
      "Epoch: 617 Train loss: 0.014617 Validation loss: 0.00019702\n",
      "Epoch: 618 Train loss: 0.022607 Validation loss: 0.008258\n",
      "Epoch: 619 Train loss: 0.01762 Validation loss: 2.2517e-05\n",
      "Epoch: 620 Train loss: 0.016544 Validation loss: 0.00041537\n",
      "Epoch: 621 Train loss: 0.023638 Validation loss: 0.004466\n",
      "Epoch: 622 Train loss: 0.016097 Validation loss: 0.00051325\n",
      "Epoch: 623 Train loss: 0.018604 Validation loss: 0.0046231\n",
      "Epoch: 624 Train loss: 0.023336 Validation loss: 0.0013926\n",
      "Epoch: 625 Train loss: 0.021204 Validation loss: 0.0053874\n",
      "Epoch: 626 Train loss: 0.015231 Validation loss: 0.00025105\n",
      "Epoch: 627 Train loss: 0.017859 Validation loss: 0.001164\n",
      "Epoch: 628 Train loss: 0.012808 Validation loss: 0.0091917\n",
      "Epoch: 629 Train loss: 0.020213 Validation loss: 0.0078613\n",
      "Epoch: 630 Train loss: 0.018251 Validation loss: 0.0019328\n",
      "Epoch: 631 Train loss: 0.015596 Validation loss: 4.9184e-05\n",
      "Epoch: 632 Train loss: 0.021647 Validation loss: 0.00080877\n",
      "Epoch: 633 Train loss: 0.014062 Validation loss: 0.0029744\n",
      "Epoch: 634 Train loss: 0.021687 Validation loss: 0.00093629\n",
      "Epoch: 635 Train loss: 0.018804 Validation loss: 0.00031053\n",
      "Epoch: 636 Train loss: 0.013402 Validation loss: 0.00019018\n",
      "Epoch: 637 Train loss: 0.019738 Validation loss: 0.0014511\n",
      "Epoch: 638 Train loss: 0.018238 Validation loss: 0.009398\n",
      "Epoch: 639 Train loss: 0.020126 Validation loss: 8.3741e-06\n",
      "Epoch: 640 Train loss: 0.019013 Validation loss: 0.00052802\n",
      "Epoch: 641 Train loss: 0.021676 Validation loss: 4.6217e-06\n",
      "Epoch: 642 Train loss: 0.017711 Validation loss: 0.0026671\n",
      "Epoch: 643 Train loss: 0.016094 Validation loss: 0.00094258\n",
      "Epoch: 644 Train loss: 0.017527 Validation loss: 0.0016168\n",
      "Epoch: 645 Train loss: 0.021984 Validation loss: 0.00016948\n",
      "Epoch: 646 Train loss: 0.01761 Validation loss: 0.003787\n",
      "Epoch: 647 Train loss: 0.018358 Validation loss: 0.018168\n",
      "Epoch: 648 Train loss: 0.017946 Validation loss: 0.0002089\n",
      "Epoch: 649 Train loss: 0.017824 Validation loss: 0.00085146\n",
      "Epoch: 650 Train loss: 0.021727 Validation loss: 0.00067446\n",
      "Epoch: 651 Train loss: 0.016861 Validation loss: 0.00159\n",
      "Epoch: 652 Train loss: 0.019593 Validation loss: 0.0059594\n",
      "Epoch: 653 Train loss: 0.016079 Validation loss: 0.0054136\n",
      "Epoch: 654 Train loss: 0.02328 Validation loss: 0.00088871\n",
      "Epoch: 655 Train loss: 0.019067 Validation loss: 0.0068845\n",
      "Epoch: 656 Train loss: 0.017994 Validation loss: 0.0010446\n",
      "Epoch: 657 Train loss: 0.015484 Validation loss: 0.00035649\n",
      "Epoch: 658 Train loss: 0.028133 Validation loss: 0.00088987\n",
      "Epoch: 659 Train loss: 0.017212 Validation loss: 0.00027035\n",
      "Epoch: 660 Train loss: 0.014735 Validation loss: 0.00094095\n",
      "Epoch: 661 Train loss: 0.027609 Validation loss: 0.013672\n",
      "Epoch: 662 Train loss: 0.019646 Validation loss: 0.00066926\n",
      "Epoch: 663 Train loss: 0.020111 Validation loss: 0.00019168\n",
      "Epoch: 664 Train loss: 0.016282 Validation loss: 0.002063\n",
      "Epoch: 665 Train loss: 0.020602 Validation loss: 0.018435\n",
      "Epoch: 666 Train loss: 0.021177 Validation loss: 6.1281e-06\n",
      "Epoch: 667 Train loss: 0.022549 Validation loss: 0.0043737\n",
      "Epoch: 668 Train loss: 0.012897 Validation loss: 0.0056301\n",
      "Epoch: 669 Train loss: 0.019495 Validation loss: 0.014891\n",
      "Epoch: 670 Train loss: 0.020411 Validation loss: 0.00039773\n",
      "Epoch: 671 Train loss: 0.021919 Validation loss: 1.4438e-06\n",
      "Epoch: 672 Train loss: 0.019334 Validation loss: 0.0072861\n",
      "Epoch: 673 Train loss: 0.021092 Validation loss: 0.004983\n",
      "Epoch: 674 Train loss: 0.016525 Validation loss: 0.0056772\n",
      "Epoch: 675 Train loss: 0.019029 Validation loss: 0.001059\n",
      "Epoch: 676 Train loss: 0.017935 Validation loss: 0.01356\n",
      "Epoch: 677 Train loss: 0.01582 Validation loss: 0.0037764\n",
      "Epoch: 678 Train loss: 0.023576 Validation loss: 0.0072338\n",
      "Epoch: 679 Train loss: 0.020128 Validation loss: 0.0020141\n",
      "Epoch: 680 Train loss: 0.017671 Validation loss: 0.0010305\n",
      "Epoch: 681 Train loss: 0.019404 Validation loss: 0.0036639\n",
      "Epoch: 682 Train loss: 0.021981 Validation loss: 0.0028918\n",
      "Epoch: 683 Train loss: 0.020168 Validation loss: 0.0053188\n",
      "Epoch: 684 Train loss: 0.016544 Validation loss: 1.3268e-05\n",
      "Epoch: 685 Train loss: 0.024622 Validation loss: 0.00062231\n",
      "Epoch: 686 Train loss: 0.018071 Validation loss: 0.0012498\n",
      "Epoch: 687 Train loss: 0.01862 Validation loss: 4.592e-07\n",
      "Epoch: 688 Train loss: 0.020786 Validation loss: 0.0095546\n",
      "Epoch: 689 Train loss: 0.023258 Validation loss: 0.00075638\n",
      "Epoch: 690 Train loss: 0.017642 Validation loss: 0.0015223\n",
      "Epoch: 691 Train loss: 0.022014 Validation loss: 0.01074\n",
      "Epoch: 692 Train loss: 0.019962 Validation loss: 0.0045824\n",
      "Epoch: 693 Train loss: 0.019389 Validation loss: 0.0030706\n",
      "Epoch: 694 Train loss: 0.01743 Validation loss: 2.3827e-05\n",
      "Epoch: 695 Train loss: 0.019947 Validation loss: 0.0015864\n",
      "Epoch: 696 Train loss: 0.019433 Validation loss: 0.0018543\n",
      "Epoch: 697 Train loss: 0.015989 Validation loss: 0.00058739\n",
      "Epoch: 698 Train loss: 0.021294 Validation loss: 9.3704e-05\n",
      "Epoch: 699 Train loss: 0.014331 Validation loss: 0.00055277\n",
      "Epoch: 700 Train loss: 0.020263 Validation loss: 0.0039159\n",
      "Epoch: 701 Train loss: 0.022325 Validation loss: 7.9488e-05\n",
      "Epoch: 702 Train loss: 0.016944 Validation loss: 4.4516e-05\n",
      "Epoch: 703 Train loss: 0.016717 Validation loss: 0.00012189\n",
      "Epoch: 704 Train loss: 0.016801 Validation loss: 0.0020774\n",
      "Epoch: 705 Train loss: 0.023089 Validation loss: 0.00054817\n",
      "Epoch: 706 Train loss: 0.015027 Validation loss: 0.0029687\n",
      "Epoch: 707 Train loss: 0.019326 Validation loss: 0.0099378\n",
      "Epoch: 708 Train loss: 0.023902 Validation loss: 0.0032447\n",
      "Epoch: 709 Train loss: 0.015644 Validation loss: 0.0022864\n",
      "Epoch: 710 Train loss: 0.019084 Validation loss: 0.0030623\n",
      "Epoch: 711 Train loss: 0.016383 Validation loss: 0.010189\n",
      "Epoch: 712 Train loss: 0.022761 Validation loss: 0.0038176\n",
      "Epoch: 713 Train loss: 0.015818 Validation loss: 0.010054\n",
      "Epoch: 714 Train loss: 0.019575 Validation loss: 2.7317e-05\n",
      "Epoch: 715 Train loss: 0.013047 Validation loss: 0.00059144\n",
      "Epoch: 716 Train loss: 0.025901 Validation loss: 0.020476\n",
      "Epoch: 717 Train loss: 0.011242 Validation loss: 0.0013237\n",
      "Epoch: 718 Train loss: 0.022443 Validation loss: 0.00098137\n",
      "Epoch: 719 Train loss: 0.021157 Validation loss: 0.0013364\n",
      "Epoch: 720 Train loss: 0.013495 Validation loss: 0.0055135\n",
      "Epoch: 721 Train loss: 0.021388 Validation loss: 0.0034803\n",
      "Epoch: 722 Train loss: 0.014569 Validation loss: 0.0013841\n",
      "Epoch: 723 Train loss: 0.019689 Validation loss: 0.0052454\n",
      "Epoch: 724 Train loss: 0.022878 Validation loss: 1.1433e-05\n",
      "Epoch: 725 Train loss: 0.019351 Validation loss: 0.00016319\n",
      "Epoch: 726 Train loss: 0.017268 Validation loss: 0.00081326\n",
      "Epoch: 727 Train loss: 0.018735 Validation loss: 0.0019545\n",
      "Epoch: 728 Train loss: 0.016588 Validation loss: 0.0012428\n",
      "Epoch: 729 Train loss: 0.021595 Validation loss: 0.0070958\n",
      "Epoch: 730 Train loss: 0.021529 Validation loss: 0.0091693\n",
      "Epoch: 731 Train loss: 0.017554 Validation loss: 0.01234\n",
      "Epoch: 732 Train loss: 0.017292 Validation loss: 0.0028453\n",
      "Epoch: 733 Train loss: 0.019367 Validation loss: 0.00072846\n",
      "Epoch: 734 Train loss: 0.018123 Validation loss: 0.0054672\n",
      "Epoch: 735 Train loss: 0.023078 Validation loss: 0.011692\n",
      "Epoch: 736 Train loss: 0.018477 Validation loss: 7.2049e-05\n",
      "Epoch: 737 Train loss: 0.019365 Validation loss: 0.0019408\n",
      "Epoch: 738 Train loss: 0.01867 Validation loss: 8.9023e-05\n",
      "Epoch: 739 Train loss: 0.015779 Validation loss: 3.7569e-05\n",
      "Epoch: 740 Train loss: 0.022053 Validation loss: 0.000966\n",
      "Epoch: 741 Train loss: 0.016601 Validation loss: 0.0084513\n",
      "Epoch: 742 Train loss: 0.01893 Validation loss: 0.0068901\n",
      "Epoch: 743 Train loss: 0.015056 Validation loss: 0.00019531\n",
      "Epoch: 744 Train loss: 0.022325 Validation loss: 9.0618e-05\n",
      "Epoch: 745 Train loss: 0.020757 Validation loss: 0.009199\n",
      "Epoch: 746 Train loss: 0.015383 Validation loss: 0.0062758\n",
      "Epoch: 747 Train loss: 0.016375 Validation loss: 0.0037957\n",
      "Epoch: 748 Train loss: 0.016583 Validation loss: 0.00031525\n",
      "Epoch: 749 Train loss: 0.021805 Validation loss: 0.0088428\n",
      "Epoch: 750 Train loss: 0.015215 Validation loss: 0.0068126\n",
      "Epoch: 751 Train loss: 0.020466 Validation loss: 0.00025182\n",
      "Epoch: 752 Train loss: 0.020257 Validation loss: 0.0044384\n",
      "Epoch: 753 Train loss: 0.018991 Validation loss: 0.0019526\n",
      "Epoch: 754 Train loss: 0.01933 Validation loss: 0.0038758\n",
      "Epoch: 755 Train loss: 0.022538 Validation loss: 0.015952\n",
      "Epoch: 756 Train loss: 0.014307 Validation loss: 0.0067627\n",
      "Epoch: 757 Train loss: 0.01675 Validation loss: 0.0023027\n",
      "Epoch: 758 Train loss: 0.014777 Validation loss: 0.0011817\n",
      "Epoch: 759 Train loss: 0.021019 Validation loss: 0.0072699\n",
      "Epoch: 760 Train loss: 0.018227 Validation loss: 0.0062297\n",
      "Epoch: 761 Train loss: 0.018226 Validation loss: 0.00042618\n",
      "Epoch: 762 Train loss: 0.017024 Validation loss: 0.0065284\n",
      "Epoch: 763 Train loss: 0.017094 Validation loss: 0.0018789\n",
      "Epoch: 764 Train loss: 0.021308 Validation loss: 0.0086632\n",
      "Epoch: 765 Train loss: 0.01763 Validation loss: 0.0028397\n",
      "Epoch: 766 Train loss: 0.019993 Validation loss: 0.0014417\n",
      "Epoch: 767 Train loss: 0.016238 Validation loss: 0.0054979\n",
      "Epoch: 768 Train loss: 0.017127 Validation loss: 9.4068e-07\n",
      "Epoch: 769 Train loss: 0.022638 Validation loss: 0.0058214\n",
      "Epoch: 770 Train loss: 0.01278 Validation loss: 0.0012662\n",
      "Epoch: 771 Train loss: 0.022222 Validation loss: 0.0027232\n",
      "Epoch: 772 Train loss: 0.019755 Validation loss: 0.001129\n",
      "Epoch: 773 Train loss: 0.021423 Validation loss: 0.011202\n",
      "Epoch: 774 Train loss: 0.016418 Validation loss: 0.0039438\n",
      "Epoch: 775 Train loss: 0.018805 Validation loss: 0.0053122\n",
      "Epoch: 776 Train loss: 0.019235 Validation loss: 0.0015256\n",
      "Epoch: 777 Train loss: 0.017062 Validation loss: 0.00077871\n",
      "Epoch: 778 Train loss: 0.022425 Validation loss: 0.0057459\n",
      "Epoch: 779 Train loss: 0.016925 Validation loss: 8.3888e-05\n",
      "Epoch: 780 Train loss: 0.014783 Validation loss: 0.00015199\n",
      "Epoch: 781 Train loss: 0.019746 Validation loss: 0.0011107\n",
      "Epoch: 782 Train loss: 0.016218 Validation loss: 0.00059191\n",
      "Epoch: 783 Train loss: 0.023512 Validation loss: 0.0029372\n",
      "Epoch: 784 Train loss: 0.014553 Validation loss: 0.017533\n",
      "Epoch: 785 Train loss: 0.018368 Validation loss: 0.0016519\n",
      "Epoch: 786 Train loss: 0.01802 Validation loss: 1.8595e-05\n",
      "Epoch: 787 Train loss: 0.024532 Validation loss: 2.133e-06\n",
      "Epoch: 788 Train loss: 0.021522 Validation loss: 8.7872e-06\n",
      "Epoch: 789 Train loss: 0.015225 Validation loss: 0.00019525\n",
      "Epoch: 790 Train loss: 0.021371 Validation loss: 0.0054123\n",
      "Epoch: 791 Train loss: 0.015928 Validation loss: 0.0035573\n",
      "Epoch: 792 Train loss: 0.020192 Validation loss: 0.00054147\n",
      "Epoch: 793 Train loss: 0.019646 Validation loss: 0.0011502\n",
      "Epoch: 794 Train loss: 0.016238 Validation loss: 0.0047349\n",
      "Epoch: 795 Train loss: 0.020339 Validation loss: 0.0005057\n",
      "Epoch: 796 Train loss: 0.014747 Validation loss: 0.0027644\n",
      "Epoch: 797 Train loss: 0.017132 Validation loss: 0.00022407\n",
      "Epoch: 798 Train loss: 0.016737 Validation loss: 0.00093767\n",
      "Epoch: 799 Train loss: 0.018526 Validation loss: 0.021771\n",
      "Epoch: 800 Train loss: 0.020736 Validation loss: 0.00045657\n",
      "Epoch: 801 Train loss: 0.01578 Validation loss: 0.00020412\n",
      "Epoch: 802 Train loss: 0.021223 Validation loss: 2.9003e-06\n",
      "Epoch: 803 Train loss: 0.016143 Validation loss: 0.00048259\n",
      "Epoch: 804 Train loss: 0.019587 Validation loss: 0.016016\n",
      "Epoch: 805 Train loss: 0.021359 Validation loss: 0.00032928\n",
      "Epoch: 806 Train loss: 0.018764 Validation loss: 0.0075554\n",
      "Epoch: 807 Train loss: 0.022448 Validation loss: 0.001882\n",
      "Epoch: 808 Train loss: 0.014802 Validation loss: 0.010504\n",
      "Epoch: 809 Train loss: 0.017299 Validation loss: 0.0059146\n",
      "Epoch: 810 Train loss: 0.016811 Validation loss: 0.0028136\n",
      "Epoch: 811 Train loss: 0.016311 Validation loss: 0.0084218\n",
      "Epoch: 812 Train loss: 0.021521 Validation loss: 0.0080785\n",
      "Epoch: 813 Train loss: 0.018168 Validation loss: 0.00049256\n",
      "Epoch: 814 Train loss: 0.01573 Validation loss: 0.00094862\n",
      "Epoch: 815 Train loss: 0.021371 Validation loss: 0.011357\n",
      "Epoch: 816 Train loss: 0.017374 Validation loss: 0.00010587\n",
      "Epoch: 817 Train loss: 0.013137 Validation loss: 0.00013123\n",
      "Epoch: 818 Train loss: 0.022024 Validation loss: 6.9456e-05\n",
      "Epoch: 819 Train loss: 0.01653 Validation loss: 0.011878\n",
      "Epoch: 820 Train loss: 0.017519 Validation loss: 0.0014262\n",
      "Epoch: 821 Train loss: 0.019585 Validation loss: 0.0015002\n",
      "Epoch: 822 Train loss: 0.014789 Validation loss: 0.0035094\n",
      "Epoch: 823 Train loss: 0.019151 Validation loss: 0.0020308\n",
      "Epoch: 824 Train loss: 0.020273 Validation loss: 0.00015449\n",
      "Epoch: 825 Train loss: 0.0182 Validation loss: 0.0016146\n",
      "Epoch: 826 Train loss: 0.0166 Validation loss: 0.0035515\n",
      "Epoch: 827 Train loss: 0.020298 Validation loss: 0.0045512\n",
      "Epoch: 828 Train loss: 0.016319 Validation loss: 0.0069639\n",
      "Epoch: 829 Train loss: 0.020996 Validation loss: 0.016382\n",
      "Epoch: 830 Train loss: 0.019284 Validation loss: 0.00023346\n",
      "Epoch: 831 Train loss: 0.019353 Validation loss: 0.0010314\n",
      "Epoch: 832 Train loss: 0.01708 Validation loss: 3.0188e-06\n",
      "Epoch: 833 Train loss: 0.017357 Validation loss: 3.968e-06\n",
      "Epoch: 834 Train loss: 0.016743 Validation loss: 0.00049713\n",
      "Epoch: 835 Train loss: 0.022642 Validation loss: 0.00068072\n",
      "Epoch: 836 Train loss: 0.017545 Validation loss: 0.011623\n",
      "Epoch: 837 Train loss: 0.018067 Validation loss: 0.0084392\n",
      "Epoch: 838 Train loss: 0.018168 Validation loss: 0.0067875\n",
      "Epoch: 839 Train loss: 0.015557 Validation loss: 0.001553\n",
      "Epoch: 840 Train loss: 0.019456 Validation loss: 0.0054464\n",
      "Epoch: 841 Train loss: 0.02201 Validation loss: 0.0042876\n",
      "Epoch: 842 Train loss: 0.019686 Validation loss: 0.010579\n",
      "Epoch: 843 Train loss: 0.01411 Validation loss: 0.00050707\n",
      "Epoch: 844 Train loss: 0.015739 Validation loss: 1.0187e-05\n",
      "Epoch: 845 Train loss: 0.023137 Validation loss: 0.00056134\n",
      "Epoch: 846 Train loss: 0.019276 Validation loss: 0.00038631\n",
      "Epoch: 847 Train loss: 0.024642 Validation loss: 0.011466\n",
      "Epoch: 848 Train loss: 0.015641 Validation loss: 2.9618e-05\n",
      "Epoch: 849 Train loss: 0.018879 Validation loss: 0.00054237\n",
      "Epoch: 850 Train loss: 0.016124 Validation loss: 0.006732\n",
      "Epoch: 851 Train loss: 0.023819 Validation loss: 0.001672\n",
      "Epoch: 852 Train loss: 0.018932 Validation loss: 0.00099983\n",
      "Epoch: 853 Train loss: 0.011662 Validation loss: 0.0014618\n",
      "Epoch: 854 Train loss: 0.020369 Validation loss: 0.003623\n",
      "Epoch: 855 Train loss: 0.018046 Validation loss: 0.00054962\n",
      "Epoch: 856 Train loss: 0.016203 Validation loss: 0.0022059\n",
      "Epoch: 857 Train loss: 0.018918 Validation loss: 0.00044418\n",
      "Epoch: 858 Train loss: 0.018601 Validation loss: 0.0070075\n",
      "Epoch: 859 Train loss: 0.018739 Validation loss: 0.0073281\n",
      "Epoch: 860 Train loss: 0.015308 Validation loss: 0.0021986\n",
      "Epoch: 861 Train loss: 0.020702 Validation loss: 0.0042341\n",
      "Epoch: 862 Train loss: 0.014319 Validation loss: 0.0095294\n",
      "Epoch: 863 Train loss: 0.021976 Validation loss: 0.00423\n",
      "Epoch: 864 Train loss: 0.018424 Validation loss: 0.0017408\n",
      "Epoch: 865 Train loss: 0.017858 Validation loss: 0.00016337\n",
      "Epoch: 866 Train loss: 0.019764 Validation loss: 0.0046616\n",
      "Epoch: 867 Train loss: 0.01817 Validation loss: 0.0014626\n",
      "Epoch: 868 Train loss: 0.021881 Validation loss: 0.0035524\n",
      "Epoch: 869 Train loss: 0.014317 Validation loss: 0.0076192\n",
      "Epoch: 870 Train loss: 0.019283 Validation loss: 0.0072607\n",
      "Epoch: 871 Train loss: 0.016285 Validation loss: 0.0034641\n",
      "Epoch: 872 Train loss: 0.016825 Validation loss: 0.0049177\n",
      "Epoch: 873 Train loss: 0.020758 Validation loss: 0.0020729\n",
      "Epoch: 874 Train loss: 0.014533 Validation loss: 5.4999e-05\n",
      "Epoch: 875 Train loss: 0.021731 Validation loss: 0.0082861\n",
      "Epoch: 876 Train loss: 0.017373 Validation loss: 0.0039084\n",
      "Epoch: 877 Train loss: 0.014025 Validation loss: 0.0050485\n",
      "Epoch: 878 Train loss: 0.019431 Validation loss: 0.0025374\n",
      "Epoch: 879 Train loss: 0.019934 Validation loss: 0.0046416\n",
      "Epoch: 880 Train loss: 0.017924 Validation loss: 0.00045603\n",
      "Epoch: 881 Train loss: 0.013455 Validation loss: 0.0020541\n",
      "Epoch: 882 Train loss: 0.018429 Validation loss: 0.0021337\n",
      "Epoch: 883 Train loss: 0.017024 Validation loss: 0.0028907\n",
      "Epoch: 884 Train loss: 0.015794 Validation loss: 0.0048561\n",
      "Epoch: 885 Train loss: 0.01802 Validation loss: 0.0049603\n",
      "Epoch: 886 Train loss: 0.016599 Validation loss: 0.0021101\n",
      "Epoch: 887 Train loss: 0.015404 Validation loss: 0.0044705\n",
      "Epoch: 888 Train loss: 0.016 Validation loss: 0.0059861\n",
      "Epoch: 889 Train loss: 0.024561 Validation loss: 0.0044586\n",
      "Epoch: 890 Train loss: 0.018141 Validation loss: 0.00086952\n",
      "Epoch: 891 Train loss: 0.017899 Validation loss: 0.015286\n",
      "Epoch: 892 Train loss: 0.024963 Validation loss: 0.0030437\n",
      "Epoch: 893 Train loss: 0.010599 Validation loss: 0.0031978\n",
      "Epoch: 894 Train loss: 0.023085 Validation loss: 0.0079181\n",
      "Epoch: 895 Train loss: 0.017182 Validation loss: 0.0054969\n",
      "Epoch: 896 Train loss: 0.021384 Validation loss: 0.000477\n",
      "Epoch: 897 Train loss: 0.016165 Validation loss: 0.00042806\n",
      "Epoch: 898 Train loss: 0.017638 Validation loss: 0.0032322\n",
      "Epoch: 899 Train loss: 0.017846 Validation loss: 0.0038569\n",
      "Epoch: 900 Train loss: 0.019351 Validation loss: 0.0013721\n",
      "Epoch: 901 Train loss: 0.016674 Validation loss: 0.0029851\n",
      "Epoch: 902 Train loss: 0.017109 Validation loss: 0.0024689\n",
      "Epoch: 903 Train loss: 0.013881 Validation loss: 4.8479e-05\n",
      "Epoch: 904 Train loss: 0.022708 Validation loss: 0.0037889\n",
      "Epoch: 905 Train loss: 0.020176 Validation loss: 0.0081107\n",
      "Epoch: 906 Train loss: 0.01466 Validation loss: 0.006926\n",
      "Epoch: 907 Train loss: 0.019461 Validation loss: 0.0058896\n",
      "Epoch: 908 Train loss: 0.024775 Validation loss: 0.003184\n",
      "Epoch: 909 Train loss: 0.014514 Validation loss: 0.015298\n",
      "Epoch: 910 Train loss: 0.019511 Validation loss: 0.01572\n",
      "Epoch: 911 Train loss: 0.017578 Validation loss: 0.0025903\n",
      "Epoch: 912 Train loss: 0.019625 Validation loss: 0.0010262\n",
      "Epoch: 913 Train loss: 0.01565 Validation loss: 0.0010406\n",
      "Epoch: 914 Train loss: 0.021374 Validation loss: 0.0092929\n",
      "Epoch: 915 Train loss: 0.018175 Validation loss: 0.00011132\n",
      "Epoch: 916 Train loss: 0.017702 Validation loss: 0.0067207\n",
      "Epoch: 917 Train loss: 0.015559 Validation loss: 0.0090281\n",
      "Epoch: 918 Train loss: 0.01627 Validation loss: 0.00084284\n",
      "Epoch: 919 Train loss: 0.019437 Validation loss: 1.2081e-05\n",
      "Epoch: 920 Train loss: 0.015331 Validation loss: 0.026756\n",
      "Epoch: 921 Train loss: 0.01785 Validation loss: 0.0042498\n",
      "Epoch: 922 Train loss: 0.014343 Validation loss: 1.0492e-05\n",
      "Epoch: 923 Train loss: 0.021741 Validation loss: 0.001821\n",
      "Epoch: 924 Train loss: 0.017054 Validation loss: 2.1136e-06\n",
      "Epoch: 925 Train loss: 0.019355 Validation loss: 0.0011113\n",
      "Epoch: 926 Train loss: 0.019745 Validation loss: 0.0034899\n",
      "Epoch: 927 Train loss: 0.018568 Validation loss: 0.0063805\n",
      "Epoch: 928 Train loss: 0.016022 Validation loss: 0.0054142\n",
      "Epoch: 929 Train loss: 0.018915 Validation loss: 0.0042018\n",
      "Epoch: 930 Train loss: 0.015221 Validation loss: 0.010465\n",
      "Epoch: 931 Train loss: 0.017439 Validation loss: 0.016266\n",
      "Epoch: 932 Train loss: 0.015965 Validation loss: 0.00010205\n",
      "Epoch: 933 Train loss: 0.019244 Validation loss: 0.011999\n",
      "Epoch: 934 Train loss: 0.015606 Validation loss: 0.01035\n",
      "Epoch: 935 Train loss: 0.021571 Validation loss: 0.00024533\n",
      "Epoch: 936 Train loss: 0.016993 Validation loss: 0.00055062\n",
      "Epoch: 937 Train loss: 0.021005 Validation loss: 0.010129\n",
      "Epoch: 938 Train loss: 0.015667 Validation loss: 0.00067286\n",
      "Epoch: 939 Train loss: 0.019314 Validation loss: 0.0087181\n",
      "Epoch: 940 Train loss: 0.01567 Validation loss: 0.0014707\n",
      "Epoch: 941 Train loss: 0.022983 Validation loss: 0.016752\n",
      "Epoch: 942 Train loss: 0.022689 Validation loss: 8.8293e-05\n",
      "Epoch: 943 Train loss: 0.013987 Validation loss: 0.010518\n",
      "Epoch: 944 Train loss: 0.014761 Validation loss: 0.0037385\n",
      "Epoch: 945 Train loss: 0.017982 Validation loss: 2.4909e-05\n",
      "Epoch: 946 Train loss: 0.015808 Validation loss: 0.0044079\n",
      "Epoch: 947 Train loss: 0.017751 Validation loss: 0.0078213\n",
      "Epoch: 948 Train loss: 0.021942 Validation loss: 0.0061993\n",
      "Epoch: 949 Train loss: 0.016718 Validation loss: 0.0084745\n",
      "Epoch: 950 Train loss: 0.015992 Validation loss: 0.0093114\n",
      "Epoch: 951 Train loss: 0.0202 Validation loss: 0.0082988\n",
      "Epoch: 952 Train loss: 0.019789 Validation loss: 0.0014295\n",
      "Epoch: 953 Train loss: 0.015068 Validation loss: 0.0065788\n",
      "Epoch: 954 Train loss: 0.016094 Validation loss: 0.0092636\n",
      "Epoch: 955 Train loss: 0.01701 Validation loss: 0.0034429\n",
      "Epoch: 956 Train loss: 0.02201 Validation loss: 0.00094163\n",
      "Epoch: 957 Train loss: 0.016213 Validation loss: 0.0010774\n",
      "Epoch: 958 Train loss: 0.015524 Validation loss: 0.0011705\n",
      "Epoch: 959 Train loss: 0.023302 Validation loss: 0.0084027\n",
      "Epoch: 960 Train loss: 0.016734 Validation loss: 0.0011986\n",
      "Epoch: 961 Train loss: 0.020049 Validation loss: 0.00071335\n",
      "Epoch: 962 Train loss: 0.019479 Validation loss: 0.0010331\n",
      "Epoch: 963 Train loss: 0.020119 Validation loss: 0.0016046\n",
      "Epoch: 964 Train loss: 0.018456 Validation loss: 0.001725\n",
      "Epoch: 965 Train loss: 0.022111 Validation loss: 0.0032279\n",
      "Epoch: 966 Train loss: 0.012656 Validation loss: 0.00041437\n",
      "Epoch: 967 Train loss: 0.01766 Validation loss: 5.8513e-05\n",
      "Epoch: 968 Train loss: 0.01838 Validation loss: 0.0039865\n",
      "Epoch: 969 Train loss: 0.019612 Validation loss: 0.0029566\n",
      "Epoch: 970 Train loss: 0.02153 Validation loss: 0.0014935\n",
      "Epoch: 971 Train loss: 0.015227 Validation loss: 0.00054523\n",
      "Epoch: 972 Train loss: 0.025098 Validation loss: 0.00012547\n",
      "Epoch: 973 Train loss: 0.014351 Validation loss: 0.00090432\n",
      "Epoch: 974 Train loss: 0.017261 Validation loss: 0.0042075\n",
      "Epoch: 975 Train loss: 0.021194 Validation loss: 0.003571\n",
      "Epoch: 976 Train loss: 0.019098 Validation loss: 0.00041762\n",
      "Epoch: 977 Train loss: 0.017153 Validation loss: 0.0029813\n",
      "Epoch: 978 Train loss: 0.014891 Validation loss: 0.0010278\n",
      "Epoch: 979 Train loss: 0.023779 Validation loss: 0.00022526\n",
      "Epoch: 980 Train loss: 0.0187 Validation loss: 0.014735\n",
      "Epoch: 981 Train loss: 0.015268 Validation loss: 0.0024575\n",
      "Epoch: 982 Train loss: 0.020824 Validation loss: 0.007543\n",
      "Epoch: 983 Train loss: 0.019274 Validation loss: 0.0018553\n",
      "Epoch: 984 Train loss: 0.015098 Validation loss: 0.0045616\n",
      "Epoch: 985 Train loss: 0.019658 Validation loss: 2.2569e-05\n",
      "Epoch: 986 Train loss: 0.013082 Validation loss: 0.00078093\n",
      "Epoch: 987 Train loss: 0.022566 Validation loss: 0.00014191\n",
      "Epoch: 988 Train loss: 0.017193 Validation loss: 0.0017825\n",
      "Epoch: 989 Train loss: 0.018999 Validation loss: 0.0040081\n",
      "Epoch: 990 Train loss: 0.021751 Validation loss: 0.00015698\n",
      "Epoch: 991 Train loss: 0.016525 Validation loss: 0.011874\n",
      "Epoch: 992 Train loss: 0.016345 Validation loss: 6.7616e-05\n",
      "Epoch: 993 Train loss: 0.020357 Validation loss: 0.0011046\n",
      "Epoch: 994 Train loss: 0.016738 Validation loss: 0.0054988\n",
      "Epoch: 995 Train loss: 0.016031 Validation loss: 0.0044612\n",
      "Epoch: 996 Train loss: 0.023459 Validation loss: 0.0066209\n",
      "Epoch: 997 Train loss: 0.018574 Validation loss: 0.013158\n",
      "Epoch: 998 Train loss: 0.018097 Validation loss: 0.00093704\n",
      "Epoch: 999 Train loss: 0.015378 Validation loss: 0.010323\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "train_iters = 0\n",
    "train_loss = 0.0\n",
    "for x_t, y_t in train_ds:\n",
    "    train_loss += train_step(mdl, opt, x_t, y_t)\n",
    "    train_iters += 1\n",
    "    if (train_iters >= int(N_train_samples/batch_size)):\n",
    "        for x_v, y_v in validation_ds:\n",
    "            y_pred = mdl(x_v)\n",
    "            validation_loss = tf.reduce_mean(tf.square(y_v-y_pred))\n",
    "        print(\"Epoch: {} Train loss: {:.5} Validation loss: {:.5}\".format(epoch, train_loss/train_iters, validation_loss))\n",
    "        train_iters = 0\n",
    "        train_loss = 0.0\n",
    "        epoch += 1\n",
    "    if (epoch == N_epochs):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DduaR_pCQ0k-"
   },
   "source": [
    "After completion of the training process we use the test data set to test the models generalization to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 451671,
     "status": "ok",
     "timestamp": 1562073737029,
     "user": {
      "displayName": "Felix Wiewel",
      "photoUrl": "https://lh3.googleusercontent.com/-XBgpqQQzeMU/AAAAAAAAAAI/AAAAAAAAAZY/12qzl-EFXfM/s64/photo.jpg",
      "userId": "11509421729312238102"
     },
     "user_tz": -120
    },
    "id": "3ofClNnHRAUy",
    "outputId": "ed362136-0caf-44be-dd62-26f6a05ab56b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.016574\n"
     ]
    }
   ],
   "source": [
    "for x_t, y_t in test_ds:\n",
    "    y_pred = mdl(x_t)\n",
    "    test_loss = tf.reduce_mean(tf.square(y_t-y_pred))\n",
    "print(\"Test loss: {:.5}\".format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NzJ7wOZmRbez"
   },
   "source": [
    "After we have verified that our model achieves a similar loss on the test as on the validation and training data set, we can conclude that our model is not overfitting or underfitting and generalizes to unseen data. We can now predict on the inputs again and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 452331,
     "status": "ok",
     "timestamp": 1562073737703,
     "user": {
      "displayName": "Felix Wiewel",
      "photoUrl": "https://lh3.googleusercontent.com/-XBgpqQQzeMU/AAAAAAAAAAI/AAAAAAAAAZY/12qzl-EFXfM/s64/photo.jpg",
      "userId": "11509421729312238102"
     },
     "user_tz": -120
    },
    "id": "tyvFN03bR8ez",
    "outputId": "6f658618-e18d-4f76-9b98-289483ab5b2c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = mdl(x)\n",
    "plt.plot(x, y)\n",
    "plt.plot(x, y_true)\n",
    "plt.plot(x, y_pred.numpy())\n",
    "plt.legend([\"Observation\", \"Target\", \"Prediction\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WKmuAAmLSmiR"
   },
   "source": [
    "Now our model has learned to approximate the function mapping from the input to the output. The capability of neural networks to learn from input-ouput pairs alone and approximate an arbitrary function, see universal approximation theorem, can be very useful if the mapping between the input and output is too complex to be captured with model based approaches. But learning from input-ouput pairs alone implies that the model will only be able to make accurate predictions over input ranges it has seen during training. In order to demonstrate this we will predict on an interval that exeeds the $\\left[0,3\\right]$ interval the model was trained on, i.e. we will predict on the interval $\\left[-2,5\\right]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 452718,
     "status": "ok",
     "timestamp": 1562073738106,
     "user": {
      "displayName": "Felix Wiewel",
      "photoUrl": "https://lh3.googleusercontent.com/-XBgpqQQzeMU/AAAAAAAAAAI/AAAAAAAAAZY/12qzl-EFXfM/s64/photo.jpg",
      "userId": "11509421729312238102"
     },
     "user_tz": -120
    },
    "id": "ZRF8hAmmVFcH",
    "outputId": "87877fc7-4d35-4db4-d80c-9c49e0eb006a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_generalize = np.linspace(-2.0, 5.0, N_samples, dtype=np.float32)\n",
    "y_generalize = np.sin(1.0+x_generalize*x_generalize) + noise_sig*np.random.randn(N_samples).astype(np.float32)\n",
    "y_truey_generalize = np.sin(1.0+x_generalize*x_generalize)\n",
    "y_pred = mdl(x_generalize)\n",
    "plt.plot(x_generalize, y_generalize)\n",
    "plt.plot(x_generalize, y_truey_generalize)\n",
    "plt.plot(x_generalize, y_pred.numpy())\n",
    "plt.legend([\"Observation\", \"Target\", \"Prediction\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J2yr-0UCVIli"
   },
   "source": [
    "As expected, the model is able to make farely accurate predictions on the interval it was trained on but makes unreliable predictions outside this interval.\n",
    "\n",
    "### Regularization\n",
    "In this section we will explore the concept of regularization. As there is no theorem that can be used to determine the required size and structure of a neural network given a certain task, one has to find a suitable neural architecture by trial and error. This can result in choosing a architecture with a capacity that is higher than required for solving the given task and hence overfitting might occur. A common way to prevent large neural networks networks from overfitting is to employ some sort of regularization, e.g. weight norm penalty, dropout, early stopping and data augmentation. In this section we will focus on weight norm penalty as a regularization and derive a probabilistic interpretation for some of those.\n",
    "\n",
    "We start by restating the conditional probability of the output $\\mathbf{y}$ given the input $\\mathbf{x}$ and the networks parameters $\\boldsymbol{\\theta}$\n",
    "\n",
    "$p(\\mathbf{y}\\vert\\mathbf{x},\\boldsymbol{\\theta})=\\dfrac{1}{\\sqrt{(2\\pi)^{M}\\sigma^{2}}}\\mathrm{e}^{-\\dfrac{1}{2\\sigma^{2}}\\Vert\\boldsymbol{\\mathbf{y}-g_{\\boldsymbol{\\theta}}(\\mathbf{x})}\\Vert_{2}^{2}}$,\n",
    "\n",
    "which we used to derive the log likelihood. If we have some prior knowledge about the parameters of the neural network, which is given by a pdf $p(\\boldsymbol{\\theta})$ over the weights, we can use Bayes theorem to derive a posterior distribution\n",
    "\n",
    "$p(\\boldsymbol{\\theta}\\vert\\mathbf{x},\\mathbf{y})=\\dfrac{p(\\boldsymbol{\\theta},\\mathbf{y}\\vert\\mathbf{x})}{p(\\mathbf{y})}=\\dfrac{p(\\mathbf{y}\\vert\\mathbf{x},\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{p(\\mathbf{y})}$\n",
    "\n",
    "where $p(\\boldsymbol{\\theta})$ is the  prior over the networks parameters. Similar to the derivation at the beginning of the exercise, we can use this posterior distribution to derive a cost function for training the neural network. In this case, however, we are not maximizing the log likelihood but the posterior distribution over the weights, hence this approach is called Maximum A Posteori (MAP) estimation of the parameters. Mathematically we can formulate this as\n",
    "\n",
    "$\\boldsymbol{\\theta}^{\\star}=\\arg\\max_{\\boldsymbol{\\theta}}\\mathbb{E}\\left[p(\\boldsymbol{\\theta}\\vert\\mathbf{x},\\mathbf{y})\\right]=\\arg\\max_{\\boldsymbol{\\theta}}\\mathbb{E}\\left[\\ln{p(\\boldsymbol{\\theta}\\vert\\mathbf{x},\\mathbf{y})}\\right]=\\arg\\max_{\\boldsymbol{\\theta}}\\ln{p(\\boldsymbol{\\theta})}+\\mathbb{E}\\left[\\ln{p(\\mathbf{y}\\vert\\mathbf{x},\\boldsymbol{\\theta})}\\right]$,\n",
    "\n",
    "where we used the fact that applying a strictly increasing function, e.g. $\\ln{}$, does not change the position of the maximum of a cost function, ignored $p(\\mathbf{y})$, since it is independent of the network parameters and dropped the expectation operator for $\\ln{p(\\boldsymbol{\\theta})}$, since it is not depending on the random variable $\\mathbf{x}$. Comparing the MAP estimate of the parameters with the ML estimate we derived above, shows that the only difference is the addition of $\\ln{p(\\boldsymbol{\\theta})}$. This term is the regularization, i.e. the weight norm penalty. Depending on the distribution over $\\boldsymbol{\\theta}$ it can have different forms. If we choose a standard normal distribution, i.e. $\\boldsymbol{\\theta\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})}$,  we get\n",
    "\n",
    "$\\boldsymbol{\\theta}^{\\star}=\\arg\\min_{\\boldsymbol{\\theta}}\\lambda\\Vert\\boldsymbol{\\theta}\\Vert_{2}^{2}+\\dfrac{1}{N_{D}}\\sum_{i=1}^{N_{D}}\\Vert\\boldsymbol{\\mathbf{y}_{i}-g_{\\boldsymbol{\\theta}}(\\mathbf{x}_{i})}\\Vert_{2}^{2}$,\n",
    "\n",
    "where we have made the same simplifications as for the ML estimation and also introduced the parameter $\\lambda=\\sigma^{2}$, which is used to control the strength of the regularization. This form of regularization is commonly known as $l_{2}$-norm or weight decay regularization. Choosing a prior where the weights follow an i.i.d laplacian distribution, i.e. $p(\\boldsymbol{\\theta})=\\prod_{j}\\dfrac{1}{2}\\mathrm{e}^{\\vert\\theta_{j}\\vert}$, leads to\n",
    "\n",
    "$\\boldsymbol{\\theta}^{\\star}=\\arg\\min_{\\boldsymbol{\\theta}}\\lambda\\Vert\\boldsymbol{\\theta}\\Vert_{1}+\\dfrac{1}{N_{D}}\\sum_{i=1}^{N_{D}}\\Vert\\boldsymbol{\\mathbf{y}_{i}-g_{\\boldsymbol{\\theta}}(\\mathbf{x}_{i})}\\Vert_{2}^{2}$,\n",
    "\n",
    "where the strength of the regularization is again controlled by $\\lambda=\\sigma^{2}$. this type of regularization is known as  $l_{1}$-norm and it has the property to induce sparsity in the parameters of the network.\n",
    "\n",
    "With this theoretical background on regularization we can now implement it and observe it's effects on the regression problem covered in this exercise. For this we will define a model with a high capacity and train it for a extended time to provoke overfitting. For this, we will increase the number of hidden neurons in both hidden layers to $100$ and $50$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T2Fv2J-VK_vw"
   },
   "outputs": [],
   "source": [
    "class MyBigModel(object):\n",
    "    def __init__(self):\n",
    "        # Create model variables\n",
    "        self.W0 = tf.Variable(tf.random.normal([1, 100]), name=\"W0\")\n",
    "        self.b0 = tf.Variable(tf.zeros(100), name=\"b0\")\n",
    "        self.W1 = tf.Variable(tf.random.normal([100, 50]), name=\"W1\")\n",
    "        self.b1 = tf.Variable(tf.zeros(50), name=\"b1\")\n",
    "        self.W2 = tf.Variable(tf.random.normal([50, 1]), name=\"W2\")\n",
    "        self.b2 = tf.Variable(tf.zeros(1), name=\"b2\")\n",
    "        self.trainable_variables = [self.W0, self.b0, self.W1, self.b1, self.W2, self.b2]\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        # Compute forward pass\n",
    "        output = tf.reshape(inputs, [-1, 1])\n",
    "        output = tf.nn.tanh(tf.add(tf.matmul(output, self.W0), self.b0))\n",
    "        output = tf.nn.tanh(tf.add(tf.matmul(output, self.W1), self.b1))\n",
    "        output = tf.add(tf.matmul(output, self.W2), self.b2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y1XpaZTNLV-p"
   },
   "source": [
    "After creating one instance of this class we can again train it on our data set. We will also create a new optimizer for training this bigger model, since some optimizers adapt the learning rates for individual parameters during a training process and we do not want to train our bigger model with learning rates adopted from an earlier training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b7Yq-a1FLi0X"
   },
   "outputs": [],
   "source": [
    "big_mdl = MyBigModel()\n",
    "big_opt = tf.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dBrf0I68MnMy"
   },
   "source": [
    "Now we are ready to train this bigger model using the same training step and training loop. In order to provoke overfitting we also reduce the number of samples in the training data set a lot, increase the batch size and train for a more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 713856,
     "status": "ok",
     "timestamp": 1562073999283,
     "user": {
      "displayName": "Felix Wiewel",
      "photoUrl": "https://lh3.googleusercontent.com/-XBgpqQQzeMU/AAAAAAAAAAI/AAAAAAAAAZY/12qzl-EFXfM/s64/photo.jpg",
      "userId": "11509421729312238102"
     },
     "user_tz": -120
    },
    "id": "Nz_lXM8LMwXo",
    "outputId": "9fa737d2-f24c-4108-a421-ac6a8262d4ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train loss: 2.6967 Validation loss: 0.21089\n",
      "Epoch: 1 Train loss: 0.42084 Validation loss: 0.38211\n",
      "Epoch: 2 Train loss: 0.35196 Validation loss: 0.15229\n",
      "Epoch: 3 Train loss: 0.30138 Validation loss: 0.046661\n",
      "Epoch: 4 Train loss: 0.42687 Validation loss: 0.02591\n",
      "Epoch: 5 Train loss: 0.31673 Validation loss: 0.044473\n",
      "Epoch: 6 Train loss: 0.34327 Validation loss: 8.2299e-05\n",
      "Epoch: 7 Train loss: 0.34665 Validation loss: 0.00036658\n",
      "Epoch: 8 Train loss: 0.33916 Validation loss: 0.0002112\n",
      "Epoch: 9 Train loss: 0.33514 Validation loss: 2.8887e-05\n",
      "Epoch: 10 Train loss: 0.33435 Validation loss: 8.5043e-06\n",
      "Epoch: 11 Train loss: 0.33477 Validation loss: 2.5014e-05\n",
      "Epoch: 12 Train loss: 0.33506 Validation loss: 6.1983e-05\n",
      "Epoch: 13 Train loss: 0.33502 Validation loss: 0.00010406\n",
      "Epoch: 14 Train loss: 0.33478 Validation loss: 0.00014104\n",
      "Epoch: 15 Train loss: 0.3344 Validation loss: 0.00017057\n",
      "Epoch: 16 Train loss: 0.33394 Validation loss: 0.00019336\n",
      "Epoch: 17 Train loss: 0.33339 Validation loss: 0.00021035\n",
      "Epoch: 18 Train loss: 0.33276 Validation loss: 0.00022239\n",
      "Epoch: 19 Train loss: 0.33202 Validation loss: 0.00023068\n",
      "Epoch: 20 Train loss: 0.33114 Validation loss: 0.00023671\n",
      "Epoch: 21 Train loss: 0.33009 Validation loss: 0.00024257\n",
      "Epoch: 22 Train loss: 0.32881 Validation loss: 0.00025124\n",
      "Epoch: 23 Train loss: 0.32727 Validation loss: 0.00026591\n",
      "Epoch: 24 Train loss: 0.32538 Validation loss: 0.00029092\n",
      "Epoch: 25 Train loss: 0.32307 Validation loss: 0.00033159\n",
      "Epoch: 26 Train loss: 0.32026 Validation loss: 0.00039478\n",
      "Epoch: 27 Train loss: 0.31684 Validation loss: 0.00048909\n",
      "Epoch: 28 Train loss: 0.31271 Validation loss: 0.00062476\n",
      "Epoch: 29 Train loss: 0.30781 Validation loss: 0.0008101\n",
      "Epoch: 30 Train loss: 0.30217 Validation loss: 0.0010457\n",
      "Epoch: 31 Train loss: 0.29596 Validation loss: 0.0013178\n",
      "Epoch: 32 Train loss: 0.28946 Validation loss: 0.0016028\n",
      "Epoch: 33 Train loss: 0.2829 Validation loss: 0.001883\n",
      "Epoch: 34 Train loss: 0.27633 Validation loss: 0.0021594\n",
      "Epoch: 35 Train loss: 0.26971 Validation loss: 0.0024472\n",
      "Epoch: 36 Train loss: 0.26302 Validation loss: 0.0027597\n",
      "Epoch: 37 Train loss: 0.25636 Validation loss: 0.0031015\n",
      "Epoch: 38 Train loss: 0.24991 Validation loss: 0.0034673\n",
      "Epoch: 39 Train loss: 0.24393 Validation loss: 0.0038452\n",
      "Epoch: 40 Train loss: 0.23861 Validation loss: 0.0042246\n",
      "Epoch: 41 Train loss: 0.23402 Validation loss: 0.004599\n",
      "Epoch: 42 Train loss: 0.23012 Validation loss: 0.0049691\n",
      "Epoch: 43 Train loss: 0.22679 Validation loss: 0.0053364\n",
      "Epoch: 44 Train loss: 0.22388 Validation loss: 0.0057023\n",
      "Epoch: 45 Train loss: 0.22128 Validation loss: 0.0060659\n",
      "Epoch: 46 Train loss: 0.21891 Validation loss: 0.0064236\n",
      "Epoch: 47 Train loss: 0.21672 Validation loss: 0.0067726\n",
      "Epoch: 48 Train loss: 0.21468 Validation loss: 0.0071113\n",
      "Epoch: 49 Train loss: 0.21277 Validation loss: 0.0074377\n",
      "Epoch: 50 Train loss: 0.211 Validation loss: 0.0077512\n",
      "Epoch: 51 Train loss: 0.20935 Validation loss: 0.0080517\n",
      "Epoch: 52 Train loss: 0.20782 Validation loss: 0.0083389\n",
      "Epoch: 53 Train loss: 0.2064 Validation loss: 0.008613\n",
      "Epoch: 54 Train loss: 0.20507 Validation loss: 0.0088731\n",
      "Epoch: 55 Train loss: 0.20383 Validation loss: 0.0091199\n",
      "Epoch: 56 Train loss: 0.20267 Validation loss: 0.0093535\n",
      "Epoch: 57 Train loss: 0.20159 Validation loss: 0.0095743\n",
      "Epoch: 58 Train loss: 0.20056 Validation loss: 0.009782\n",
      "Epoch: 59 Train loss: 0.1996 Validation loss: 0.0099786\n",
      "Epoch: 60 Train loss: 0.19868 Validation loss: 0.010163\n",
      "Epoch: 61 Train loss: 0.19781 Validation loss: 0.010337\n",
      "Epoch: 62 Train loss: 0.19698 Validation loss: 0.010501\n",
      "Epoch: 63 Train loss: 0.19619 Validation loss: 0.010657\n",
      "Epoch: 64 Train loss: 0.19543 Validation loss: 0.010803\n",
      "Epoch: 65 Train loss: 0.19469 Validation loss: 0.010942\n",
      "Epoch: 66 Train loss: 0.19399 Validation loss: 0.011075\n",
      "Epoch: 67 Train loss: 0.19331 Validation loss: 0.011201\n",
      "Epoch: 68 Train loss: 0.19265 Validation loss: 0.011322\n",
      "Epoch: 69 Train loss: 0.19201 Validation loss: 0.011437\n",
      "Epoch: 70 Train loss: 0.1914 Validation loss: 0.011548\n",
      "Epoch: 71 Train loss: 0.19079 Validation loss: 0.011655\n",
      "Epoch: 72 Train loss: 0.19021 Validation loss: 0.011759\n",
      "Epoch: 73 Train loss: 0.18964 Validation loss: 0.011859\n",
      "Epoch: 74 Train loss: 0.18908 Validation loss: 0.011956\n",
      "Epoch: 75 Train loss: 0.18853 Validation loss: 0.012051\n",
      "Epoch: 76 Train loss: 0.188 Validation loss: 0.012143\n",
      "Epoch: 77 Train loss: 0.18748 Validation loss: 0.012233\n",
      "Epoch: 78 Train loss: 0.18696 Validation loss: 0.012321\n",
      "Epoch: 79 Train loss: 0.18646 Validation loss: 0.012408\n",
      "Epoch: 80 Train loss: 0.18597 Validation loss: 0.012492\n",
      "Epoch: 81 Train loss: 0.18548 Validation loss: 0.012576\n",
      "Epoch: 82 Train loss: 0.185 Validation loss: 0.012657\n",
      "Epoch: 83 Train loss: 0.18453 Validation loss: 0.012738\n",
      "Epoch: 84 Train loss: 0.18407 Validation loss: 0.012816\n",
      "Epoch: 85 Train loss: 0.18361 Validation loss: 0.012894\n",
      "Epoch: 86 Train loss: 0.18316 Validation loss: 0.012971\n",
      "Epoch: 87 Train loss: 0.18271 Validation loss: 0.013046\n",
      "Epoch: 88 Train loss: 0.18227 Validation loss: 0.013121\n",
      "Epoch: 89 Train loss: 0.18183 Validation loss: 0.013194\n",
      "Epoch: 90 Train loss: 0.1814 Validation loss: 0.013266\n",
      "Epoch: 91 Train loss: 0.18097 Validation loss: 0.013337\n",
      "Epoch: 92 Train loss: 0.18055 Validation loss: 0.013407\n",
      "Epoch: 93 Train loss: 0.18013 Validation loss: 0.013476\n",
      "Epoch: 94 Train loss: 0.17972 Validation loss: 0.013544\n",
      "Epoch: 95 Train loss: 0.17931 Validation loss: 0.01361\n",
      "Epoch: 96 Train loss: 0.1789 Validation loss: 0.013676\n",
      "Epoch: 97 Train loss: 0.1785 Validation loss: 0.01374\n",
      "Epoch: 98 Train loss: 0.1781 Validation loss: 0.013803\n",
      "Epoch: 99 Train loss: 0.1777 Validation loss: 0.013865\n",
      "Epoch: 100 Train loss: 0.17731 Validation loss: 0.013925\n",
      "Epoch: 101 Train loss: 0.17692 Validation loss: 0.013984\n",
      "Epoch: 102 Train loss: 0.17653 Validation loss: 0.014042\n",
      "Epoch: 103 Train loss: 0.17615 Validation loss: 0.014098\n",
      "Epoch: 104 Train loss: 0.17577 Validation loss: 0.014153\n",
      "Epoch: 105 Train loss: 0.17539 Validation loss: 0.014206\n",
      "Epoch: 106 Train loss: 0.17501 Validation loss: 0.014257\n",
      "Epoch: 107 Train loss: 0.17464 Validation loss: 0.014307\n",
      "Epoch: 108 Train loss: 0.17427 Validation loss: 0.014355\n",
      "Epoch: 109 Train loss: 0.1739 Validation loss: 0.014401\n",
      "Epoch: 110 Train loss: 0.17354 Validation loss: 0.014445\n",
      "Epoch: 111 Train loss: 0.17317 Validation loss: 0.014488\n",
      "Epoch: 112 Train loss: 0.17281 Validation loss: 0.014528\n",
      "Epoch: 113 Train loss: 0.17245 Validation loss: 0.014566\n",
      "Epoch: 114 Train loss: 0.1721 Validation loss: 0.014602\n",
      "Epoch: 115 Train loss: 0.17175 Validation loss: 0.014636\n",
      "Epoch: 116 Train loss: 0.1714 Validation loss: 0.014667\n",
      "Epoch: 117 Train loss: 0.17105 Validation loss: 0.014697\n",
      "Epoch: 118 Train loss: 0.1707 Validation loss: 0.014724\n",
      "Epoch: 119 Train loss: 0.17036 Validation loss: 0.014748\n",
      "Epoch: 120 Train loss: 0.17002 Validation loss: 0.01477\n",
      "Epoch: 121 Train loss: 0.16968 Validation loss: 0.014789\n",
      "Epoch: 122 Train loss: 0.16935 Validation loss: 0.014806\n",
      "Epoch: 123 Train loss: 0.16901 Validation loss: 0.01482\n",
      "Epoch: 124 Train loss: 0.16868 Validation loss: 0.014831\n",
      "Epoch: 125 Train loss: 0.16835 Validation loss: 0.014839\n",
      "Epoch: 126 Train loss: 0.16802 Validation loss: 0.014845\n",
      "Epoch: 127 Train loss: 0.1677 Validation loss: 0.014847\n",
      "Epoch: 128 Train loss: 0.16738 Validation loss: 0.014846\n",
      "Epoch: 129 Train loss: 0.16706 Validation loss: 0.014842\n",
      "Epoch: 130 Train loss: 0.16674 Validation loss: 0.014835\n",
      "Epoch: 131 Train loss: 0.16642 Validation loss: 0.014824\n",
      "Epoch: 132 Train loss: 0.16611 Validation loss: 0.01481\n",
      "Epoch: 133 Train loss: 0.1658 Validation loss: 0.014793\n",
      "Epoch: 134 Train loss: 0.16549 Validation loss: 0.014772\n",
      "Epoch: 135 Train loss: 0.16518 Validation loss: 0.014748\n",
      "Epoch: 136 Train loss: 0.16488 Validation loss: 0.01472\n",
      "Epoch: 137 Train loss: 0.16458 Validation loss: 0.014689\n",
      "Epoch: 138 Train loss: 0.16428 Validation loss: 0.014653\n",
      "Epoch: 139 Train loss: 0.16398 Validation loss: 0.014615\n",
      "Epoch: 140 Train loss: 0.16368 Validation loss: 0.014572\n",
      "Epoch: 141 Train loss: 0.16339 Validation loss: 0.014525\n",
      "Epoch: 142 Train loss: 0.16309 Validation loss: 0.014474\n",
      "Epoch: 143 Train loss: 0.16281 Validation loss: 0.01442\n",
      "Epoch: 144 Train loss: 0.16252 Validation loss: 0.014362\n",
      "Epoch: 145 Train loss: 0.16223 Validation loss: 0.014299\n",
      "Epoch: 146 Train loss: 0.16195 Validation loss: 0.014232\n",
      "Epoch: 147 Train loss: 0.16167 Validation loss: 0.014161\n",
      "Epoch: 148 Train loss: 0.16139 Validation loss: 0.014087\n",
      "Epoch: 149 Train loss: 0.16111 Validation loss: 0.014008\n",
      "Epoch: 150 Train loss: 0.16084 Validation loss: 0.013925\n",
      "Epoch: 151 Train loss: 0.16056 Validation loss: 0.013838\n",
      "Epoch: 152 Train loss: 0.16029 Validation loss: 0.013746\n",
      "Epoch: 153 Train loss: 0.16002 Validation loss: 0.013651\n",
      "Epoch: 154 Train loss: 0.15976 Validation loss: 0.013551\n",
      "Epoch: 155 Train loss: 0.15949 Validation loss: 0.013448\n",
      "Epoch: 156 Train loss: 0.15923 Validation loss: 0.013341\n",
      "Epoch: 157 Train loss: 0.15897 Validation loss: 0.01323\n",
      "Epoch: 158 Train loss: 0.15871 Validation loss: 0.013114\n",
      "Epoch: 159 Train loss: 0.15845 Validation loss: 0.012995\n",
      "Epoch: 160 Train loss: 0.1582 Validation loss: 0.012872\n",
      "Epoch: 161 Train loss: 0.15794 Validation loss: 0.012747\n",
      "Epoch: 162 Train loss: 0.15769 Validation loss: 0.012617\n",
      "Epoch: 163 Train loss: 0.15745 Validation loss: 0.012483\n",
      "Epoch: 164 Train loss: 0.1572 Validation loss: 0.012347\n",
      "Epoch: 165 Train loss: 0.15695 Validation loss: 0.012208\n",
      "Epoch: 166 Train loss: 0.15671 Validation loss: 0.012066\n",
      "Epoch: 167 Train loss: 0.15647 Validation loss: 0.011921\n",
      "Epoch: 168 Train loss: 0.15623 Validation loss: 0.011773\n",
      "Epoch: 169 Train loss: 0.15599 Validation loss: 0.011623\n",
      "Epoch: 170 Train loss: 0.15576 Validation loss: 0.011471\n",
      "Epoch: 171 Train loss: 0.15552 Validation loss: 0.011317\n",
      "Epoch: 172 Train loss: 0.15529 Validation loss: 0.011161\n",
      "Epoch: 173 Train loss: 0.15506 Validation loss: 0.011003\n",
      "Epoch: 174 Train loss: 0.15483 Validation loss: 0.010845\n",
      "Epoch: 175 Train loss: 0.1546 Validation loss: 0.010684\n",
      "Epoch: 176 Train loss: 0.15438 Validation loss: 0.010523\n",
      "Epoch: 177 Train loss: 0.15416 Validation loss: 0.010361\n",
      "Epoch: 178 Train loss: 0.15393 Validation loss: 0.010199\n",
      "Epoch: 179 Train loss: 0.15371 Validation loss: 0.010037\n",
      "Epoch: 180 Train loss: 0.15349 Validation loss: 0.0098738\n",
      "Epoch: 181 Train loss: 0.15328 Validation loss: 0.0097113\n",
      "Epoch: 182 Train loss: 0.15306 Validation loss: 0.0095492\n",
      "Epoch: 183 Train loss: 0.15284 Validation loss: 0.0093882\n",
      "Epoch: 184 Train loss: 0.15263 Validation loss: 0.0092272\n",
      "Epoch: 185 Train loss: 0.15242 Validation loss: 0.009068\n",
      "Epoch: 186 Train loss: 0.15221 Validation loss: 0.0089093\n",
      "Epoch: 187 Train loss: 0.152 Validation loss: 0.0087521\n",
      "Epoch: 188 Train loss: 0.15179 Validation loss: 0.0085961\n",
      "Epoch: 189 Train loss: 0.15158 Validation loss: 0.0084427\n",
      "Epoch: 190 Train loss: 0.15137 Validation loss: 0.0082903\n",
      "Epoch: 191 Train loss: 0.15117 Validation loss: 0.0081402\n",
      "Epoch: 192 Train loss: 0.15096 Validation loss: 0.0079927\n",
      "Epoch: 193 Train loss: 0.15076 Validation loss: 0.0078464\n",
      "Epoch: 194 Train loss: 0.15055 Validation loss: 0.0077031\n",
      "Epoch: 195 Train loss: 0.15035 Validation loss: 0.0075626\n",
      "Epoch: 196 Train loss: 0.15015 Validation loss: 0.0074243\n",
      "Epoch: 197 Train loss: 0.14995 Validation loss: 0.0072885\n",
      "Epoch: 198 Train loss: 0.14974 Validation loss: 0.0071562\n",
      "Epoch: 199 Train loss: 0.14954 Validation loss: 0.0070265\n",
      "Epoch: 200 Train loss: 0.14934 Validation loss: 0.0068994\n",
      "Epoch: 201 Train loss: 0.14915 Validation loss: 0.0067756\n",
      "Epoch: 202 Train loss: 0.14895 Validation loss: 0.0066544\n",
      "Epoch: 203 Train loss: 0.14875 Validation loss: 0.0065367\n",
      "Epoch: 204 Train loss: 0.14855 Validation loss: 0.0064218\n",
      "Epoch: 205 Train loss: 0.14835 Validation loss: 0.0063104\n",
      "Epoch: 206 Train loss: 0.14815 Validation loss: 0.0062024\n",
      "Epoch: 207 Train loss: 0.14796 Validation loss: 0.0060969\n",
      "Epoch: 208 Train loss: 0.14776 Validation loss: 0.0059955\n",
      "Epoch: 209 Train loss: 0.14756 Validation loss: 0.0058965\n",
      "Epoch: 210 Train loss: 0.14737 Validation loss: 0.0058013\n",
      "Epoch: 211 Train loss: 0.14717 Validation loss: 0.0057091\n",
      "Epoch: 212 Train loss: 0.14697 Validation loss: 0.0056206\n",
      "Epoch: 213 Train loss: 0.14678 Validation loss: 0.0055349\n",
      "Epoch: 214 Train loss: 0.14658 Validation loss: 0.0054525\n",
      "Epoch: 215 Train loss: 0.14639 Validation loss: 0.005373\n",
      "Epoch: 216 Train loss: 0.14619 Validation loss: 0.0052969\n",
      "Epoch: 217 Train loss: 0.14599 Validation loss: 0.0052239\n",
      "Epoch: 218 Train loss: 0.1458 Validation loss: 0.0051541\n",
      "Epoch: 219 Train loss: 0.1456 Validation loss: 0.0050872\n",
      "Epoch: 220 Train loss: 0.1454 Validation loss: 0.0050231\n",
      "Epoch: 221 Train loss: 0.14521 Validation loss: 0.0049623\n",
      "Epoch: 222 Train loss: 0.14501 Validation loss: 0.004904\n",
      "Epoch: 223 Train loss: 0.14481 Validation loss: 0.0048493\n",
      "Epoch: 224 Train loss: 0.14461 Validation loss: 0.0047966\n",
      "Epoch: 225 Train loss: 0.14442 Validation loss: 0.004747\n",
      "Epoch: 226 Train loss: 0.14422 Validation loss: 0.0046998\n",
      "Epoch: 227 Train loss: 0.14402 Validation loss: 0.0046554\n",
      "Epoch: 228 Train loss: 0.14382 Validation loss: 0.0046136\n",
      "Epoch: 229 Train loss: 0.14362 Validation loss: 0.0045744\n",
      "Epoch: 230 Train loss: 0.14342 Validation loss: 0.004537\n",
      "Epoch: 231 Train loss: 0.14322 Validation loss: 0.004502\n",
      "Epoch: 232 Train loss: 0.14302 Validation loss: 0.0044697\n",
      "Epoch: 233 Train loss: 0.14282 Validation loss: 0.0044394\n",
      "Epoch: 234 Train loss: 0.14262 Validation loss: 0.004411\n",
      "Epoch: 235 Train loss: 0.14242 Validation loss: 0.0043847\n",
      "Epoch: 236 Train loss: 0.14222 Validation loss: 0.0043602\n",
      "Epoch: 237 Train loss: 0.14201 Validation loss: 0.0043381\n",
      "Epoch: 238 Train loss: 0.14181 Validation loss: 0.0043168\n",
      "Epoch: 239 Train loss: 0.14161 Validation loss: 0.0042978\n",
      "Epoch: 240 Train loss: 0.1414 Validation loss: 0.0042803\n",
      "Epoch: 241 Train loss: 0.1412 Validation loss: 0.0042646\n",
      "Epoch: 242 Train loss: 0.14099 Validation loss: 0.0042499\n",
      "Epoch: 243 Train loss: 0.14079 Validation loss: 0.0042369\n",
      "Epoch: 244 Train loss: 0.14058 Validation loss: 0.0042253\n",
      "Epoch: 245 Train loss: 0.14038 Validation loss: 0.0042145\n",
      "Epoch: 246 Train loss: 0.14017 Validation loss: 0.0042053\n",
      "Epoch: 247 Train loss: 0.13996 Validation loss: 0.0041968\n",
      "Epoch: 248 Train loss: 0.13975 Validation loss: 0.0041897\n",
      "Epoch: 249 Train loss: 0.13954 Validation loss: 0.0041832\n",
      "Epoch: 250 Train loss: 0.13934 Validation loss: 0.0041778\n",
      "Epoch: 251 Train loss: 0.13913 Validation loss: 0.0041731\n",
      "Epoch: 252 Train loss: 0.13891 Validation loss: 0.0041694\n",
      "Epoch: 253 Train loss: 0.1387 Validation loss: 0.0041662\n",
      "Epoch: 254 Train loss: 0.13849 Validation loss: 0.0041639\n",
      "Epoch: 255 Train loss: 0.13828 Validation loss: 0.0041621\n",
      "Epoch: 256 Train loss: 0.13806 Validation loss: 0.0041611\n",
      "Epoch: 257 Train loss: 0.13785 Validation loss: 0.0041602\n",
      "Epoch: 258 Train loss: 0.13763 Validation loss: 0.0041604\n",
      "Epoch: 259 Train loss: 0.13741 Validation loss: 0.0041605\n",
      "Epoch: 260 Train loss: 0.13719 Validation loss: 0.004161\n",
      "Epoch: 261 Train loss: 0.13697 Validation loss: 0.0041622\n",
      "Epoch: 262 Train loss: 0.13675 Validation loss: 0.0041634\n",
      "Epoch: 263 Train loss: 0.13653 Validation loss: 0.0041655\n",
      "Epoch: 264 Train loss: 0.13631 Validation loss: 0.0041673\n",
      "Epoch: 265 Train loss: 0.13608 Validation loss: 0.0041696\n",
      "Epoch: 266 Train loss: 0.13585 Validation loss: 0.0041719\n",
      "Epoch: 267 Train loss: 0.13563 Validation loss: 0.0041746\n",
      "Epoch: 268 Train loss: 0.13539 Validation loss: 0.0041777\n",
      "Epoch: 269 Train loss: 0.13516 Validation loss: 0.0041807\n",
      "Epoch: 270 Train loss: 0.13493 Validation loss: 0.0041843\n",
      "Epoch: 271 Train loss: 0.13469 Validation loss: 0.0041878\n",
      "Epoch: 272 Train loss: 0.13445 Validation loss: 0.0041916\n",
      "Epoch: 273 Train loss: 0.13421 Validation loss: 0.0041958\n",
      "Epoch: 274 Train loss: 0.13396 Validation loss: 0.0042\n",
      "Epoch: 275 Train loss: 0.13371 Validation loss: 0.0042046\n",
      "Epoch: 276 Train loss: 0.13346 Validation loss: 0.0042091\n",
      "Epoch: 277 Train loss: 0.13321 Validation loss: 0.0042141\n",
      "Epoch: 278 Train loss: 0.13295 Validation loss: 0.0042191\n",
      "Epoch: 279 Train loss: 0.13269 Validation loss: 0.0042248\n",
      "Epoch: 280 Train loss: 0.13243 Validation loss: 0.0042306\n",
      "Epoch: 281 Train loss: 0.13216 Validation loss: 0.0042366\n",
      "Epoch: 282 Train loss: 0.13189 Validation loss: 0.0042428\n",
      "Epoch: 283 Train loss: 0.13161 Validation loss: 0.0042497\n",
      "Epoch: 284 Train loss: 0.13133 Validation loss: 0.0042569\n",
      "Epoch: 285 Train loss: 0.13105 Validation loss: 0.0042644\n",
      "Epoch: 286 Train loss: 0.13077 Validation loss: 0.0042721\n",
      "Epoch: 287 Train loss: 0.13048 Validation loss: 0.0042807\n",
      "Epoch: 288 Train loss: 0.13018 Validation loss: 0.0042899\n",
      "Epoch: 289 Train loss: 0.12989 Validation loss: 0.0042989\n",
      "Epoch: 290 Train loss: 0.12959 Validation loss: 0.0043087\n",
      "Epoch: 291 Train loss: 0.12929 Validation loss: 0.0043188\n",
      "Epoch: 292 Train loss: 0.12899 Validation loss: 0.0043288\n",
      "Epoch: 293 Train loss: 0.12869 Validation loss: 0.0043398\n",
      "Epoch: 294 Train loss: 0.12839 Validation loss: 0.0043507\n",
      "Epoch: 295 Train loss: 0.12809 Validation loss: 0.0043617\n",
      "Epoch: 296 Train loss: 0.12779 Validation loss: 0.0043728\n",
      "Epoch: 297 Train loss: 0.1275 Validation loss: 0.0043836\n",
      "Epoch: 298 Train loss: 0.1272 Validation loss: 0.0043945\n",
      "Epoch: 299 Train loss: 0.12692 Validation loss: 0.0044052\n",
      "Epoch: 300 Train loss: 0.12663 Validation loss: 0.0044152\n",
      "Epoch: 301 Train loss: 0.12636 Validation loss: 0.0044246\n",
      "Epoch: 302 Train loss: 0.12609 Validation loss: 0.0044331\n",
      "Epoch: 303 Train loss: 0.12583 Validation loss: 0.004441\n",
      "Epoch: 304 Train loss: 0.12557 Validation loss: 0.0044483\n",
      "Epoch: 305 Train loss: 0.12532 Validation loss: 0.0044541\n",
      "Epoch: 306 Train loss: 0.12508 Validation loss: 0.0044587\n",
      "Epoch: 307 Train loss: 0.12485 Validation loss: 0.0044627\n",
      "Epoch: 308 Train loss: 0.12462 Validation loss: 0.0044648\n",
      "Epoch: 309 Train loss: 0.1244 Validation loss: 0.0044666\n",
      "Epoch: 310 Train loss: 0.12419 Validation loss: 0.0044664\n",
      "Epoch: 311 Train loss: 0.12398 Validation loss: 0.0044654\n",
      "Epoch: 312 Train loss: 0.12378 Validation loss: 0.0044627\n",
      "Epoch: 313 Train loss: 0.12359 Validation loss: 0.0044596\n",
      "Epoch: 314 Train loss: 0.1234 Validation loss: 0.0044549\n",
      "Epoch: 315 Train loss: 0.12321 Validation loss: 0.0044495\n",
      "Epoch: 316 Train loss: 0.12303 Validation loss: 0.0044428\n",
      "Epoch: 317 Train loss: 0.12286 Validation loss: 0.0044352\n",
      "Epoch: 318 Train loss: 0.12269 Validation loss: 0.0044267\n",
      "Epoch: 319 Train loss: 0.12252 Validation loss: 0.004417\n",
      "Epoch: 320 Train loss: 0.12235 Validation loss: 0.0044065\n",
      "Epoch: 321 Train loss: 0.12219 Validation loss: 0.0043954\n",
      "Epoch: 322 Train loss: 0.12203 Validation loss: 0.0043837\n",
      "Epoch: 323 Train loss: 0.12187 Validation loss: 0.0043705\n",
      "Epoch: 324 Train loss: 0.12171 Validation loss: 0.0043569\n",
      "Epoch: 325 Train loss: 0.12156 Validation loss: 0.0043427\n",
      "Epoch: 326 Train loss: 0.12141 Validation loss: 0.0043275\n",
      "Epoch: 327 Train loss: 0.12125 Validation loss: 0.0043111\n",
      "Epoch: 328 Train loss: 0.1211 Validation loss: 0.0042941\n",
      "Epoch: 329 Train loss: 0.12096 Validation loss: 0.0042759\n",
      "Epoch: 330 Train loss: 0.12081 Validation loss: 0.0042569\n",
      "Epoch: 331 Train loss: 0.12066 Validation loss: 0.004236\n",
      "Epoch: 332 Train loss: 0.12052 Validation loss: 0.0042141\n",
      "Epoch: 333 Train loss: 0.12038 Validation loss: 0.0041905\n",
      "Epoch: 334 Train loss: 0.12024 Validation loss: 0.004164\n",
      "Epoch: 335 Train loss: 0.1201 Validation loss: 0.0041361\n",
      "Epoch: 336 Train loss: 0.11997 Validation loss: 0.0041054\n",
      "Epoch: 337 Train loss: 0.11984 Validation loss: 0.0040711\n",
      "Epoch: 338 Train loss: 0.11971 Validation loss: 0.0040321\n",
      "Epoch: 339 Train loss: 0.11959 Validation loss: 0.0039884\n",
      "Epoch: 340 Train loss: 0.11948 Validation loss: 0.003938\n",
      "Epoch: 341 Train loss: 0.11937 Validation loss: 0.0038795\n",
      "Epoch: 342 Train loss: 0.11927 Validation loss: 0.0038106\n",
      "Epoch: 343 Train loss: 0.11919 Validation loss: 0.0037276\n",
      "Epoch: 344 Train loss: 0.11913 Validation loss: 0.0036264\n",
      "Epoch: 345 Train loss: 0.1191 Validation loss: 0.0034991\n",
      "Epoch: 346 Train loss: 0.1191 Validation loss: 0.0033367\n",
      "Epoch: 347 Train loss: 0.11916 Validation loss: 0.0031236\n",
      "Epoch: 348 Train loss: 0.11932 Validation loss: 0.0028355\n",
      "Epoch: 349 Train loss: 0.11966 Validation loss: 0.0024371\n",
      "Epoch: 350 Train loss: 0.12036 Validation loss: 0.0018841\n",
      "Epoch: 351 Train loss: 0.12201 Validation loss: 0.0011671\n",
      "Epoch: 352 Train loss: 0.12636 Validation loss: 0.0004495\n",
      "Epoch: 353 Train loss: 0.1355 Validation loss: 6.0531e-05\n",
      "Epoch: 354 Train loss: 0.14549 Validation loss: 4.0204e-08\n",
      "Epoch: 355 Train loss: 0.15112 Validation loss: 9.25e-06\n",
      "Epoch: 356 Train loss: 0.15319 Validation loss: 1.1469e-05\n",
      "Epoch: 357 Train loss: 0.15337 Validation loss: 6.3487e-06\n",
      "Epoch: 358 Train loss: 0.15247 Validation loss: 1.2582e-06\n",
      "Epoch: 359 Train loss: 0.15097 Validation loss: 1.5915e-07\n",
      "Epoch: 360 Train loss: 0.14914 Validation loss: 3.2432e-06\n",
      "Epoch: 361 Train loss: 0.14717 Validation loss: 8.8357e-06\n",
      "Epoch: 362 Train loss: 0.14514 Validation loss: 1.4953e-05\n",
      "Epoch: 363 Train loss: 0.14311 Validation loss: 2.0018e-05\n",
      "Epoch: 364 Train loss: 0.14113 Validation loss: 2.3147e-05\n",
      "Epoch: 365 Train loss: 0.13921 Validation loss: 2.393e-05\n",
      "Epoch: 366 Train loss: 0.13736 Validation loss: 2.2504e-05\n",
      "Epoch: 367 Train loss: 0.13561 Validation loss: 1.9346e-05\n",
      "Epoch: 368 Train loss: 0.13394 Validation loss: 1.5021e-05\n",
      "Epoch: 369 Train loss: 0.13236 Validation loss: 1.0341e-05\n",
      "Epoch: 370 Train loss: 0.13086 Validation loss: 5.9099e-06\n",
      "Epoch: 371 Train loss: 0.12945 Validation loss: 2.4372e-06\n",
      "Epoch: 372 Train loss: 0.12811 Validation loss: 3.8596e-07\n",
      "Epoch: 373 Train loss: 0.12685 Validation loss: 1.3142e-07\n",
      "Epoch: 374 Train loss: 0.12565 Validation loss: 1.8854e-06\n",
      "Epoch: 375 Train loss: 0.12451 Validation loss: 5.731e-06\n",
      "Epoch: 376 Train loss: 0.12343 Validation loss: 1.159e-05\n",
      "Epoch: 377 Train loss: 0.12239 Validation loss: 1.9337e-05\n",
      "Epoch: 378 Train loss: 0.12139 Validation loss: 2.8723e-05\n",
      "Epoch: 379 Train loss: 0.12042 Validation loss: 3.9545e-05\n",
      "Epoch: 380 Train loss: 0.11949 Validation loss: 5.1428e-05\n",
      "Epoch: 381 Train loss: 0.11859 Validation loss: 6.4192e-05\n",
      "Epoch: 382 Train loss: 0.11771 Validation loss: 7.7387e-05\n",
      "Epoch: 383 Train loss: 0.11685 Validation loss: 9.0747e-05\n",
      "Epoch: 384 Train loss: 0.116 Validation loss: 0.00010412\n",
      "Epoch: 385 Train loss: 0.11517 Validation loss: 0.00011714\n",
      "Epoch: 386 Train loss: 0.11435 Validation loss: 0.0001299\n",
      "Epoch: 387 Train loss: 0.11354 Validation loss: 0.0001418\n",
      "Epoch: 388 Train loss: 0.11273 Validation loss: 0.00015308\n",
      "Epoch: 389 Train loss: 0.11194 Validation loss: 0.00016351\n",
      "Epoch: 390 Train loss: 0.11115 Validation loss: 0.00017312\n",
      "Epoch: 391 Train loss: 0.11038 Validation loss: 0.00018179\n",
      "Epoch: 392 Train loss: 0.10962 Validation loss: 0.00018971\n",
      "Epoch: 393 Train loss: 0.10887 Validation loss: 0.0001967\n",
      "Epoch: 394 Train loss: 0.10814 Validation loss: 0.00020278\n",
      "Epoch: 395 Train loss: 0.10741 Validation loss: 0.00020788\n",
      "Epoch: 396 Train loss: 0.1067 Validation loss: 0.00021163\n",
      "Epoch: 397 Train loss: 0.10598 Validation loss: 0.00021406\n",
      "Epoch: 398 Train loss: 0.10525 Validation loss: 0.00021495\n",
      "Epoch: 399 Train loss: 0.10451 Validation loss: 0.00021422\n",
      "Epoch: 400 Train loss: 0.10375 Validation loss: 0.00021207\n",
      "Epoch: 401 Train loss: 0.10297 Validation loss: 0.00020865\n",
      "Epoch: 402 Train loss: 0.10217 Validation loss: 0.00020411\n",
      "Epoch: 403 Train loss: 0.10136 Validation loss: 0.0001987\n",
      "Epoch: 404 Train loss: 0.10053 Validation loss: 0.00019251\n",
      "Epoch: 405 Train loss: 0.099699 Validation loss: 0.00018566\n",
      "Epoch: 406 Train loss: 0.098858 Validation loss: 0.00017826\n",
      "Epoch: 407 Train loss: 0.098011 Validation loss: 0.00017036\n",
      "Epoch: 408 Train loss: 0.097156 Validation loss: 0.00016204\n",
      "Epoch: 409 Train loss: 0.096295 Validation loss: 0.0001533\n",
      "Epoch: 410 Train loss: 0.095429 Validation loss: 0.00014424\n",
      "Epoch: 411 Train loss: 0.094555 Validation loss: 0.00013478\n",
      "Epoch: 412 Train loss: 0.093675 Validation loss: 0.00012516\n",
      "Epoch: 413 Train loss: 0.092789 Validation loss: 0.00011537\n",
      "Epoch: 414 Train loss: 0.091897 Validation loss: 0.0001054\n",
      "Epoch: 415 Train loss: 0.090997 Validation loss: 9.5421e-05\n",
      "Epoch: 416 Train loss: 0.090089 Validation loss: 8.55e-05\n",
      "Epoch: 417 Train loss: 0.089175 Validation loss: 7.565e-05\n",
      "Epoch: 418 Train loss: 0.088255 Validation loss: 6.6066e-05\n",
      "Epoch: 419 Train loss: 0.087326 Validation loss: 5.6709e-05\n",
      "Epoch: 420 Train loss: 0.086391 Validation loss: 4.7708e-05\n",
      "Epoch: 421 Train loss: 0.085448 Validation loss: 3.9225e-05\n",
      "Epoch: 422 Train loss: 0.0845 Validation loss: 3.1276e-05\n",
      "Epoch: 423 Train loss: 0.083544 Validation loss: 2.3978e-05\n",
      "Epoch: 424 Train loss: 0.082581 Validation loss: 1.7463e-05\n",
      "Epoch: 425 Train loss: 0.081613 Validation loss: 1.1817e-05\n",
      "Epoch: 426 Train loss: 0.08064 Validation loss: 7.1343e-06\n",
      "Epoch: 427 Train loss: 0.079661 Validation loss: 3.5395e-06\n",
      "Epoch: 428 Train loss: 0.078677 Validation loss: 1.1411e-06\n",
      "Epoch: 429 Train loss: 0.07769 Validation loss: 5.4203e-08\n",
      "Epoch: 430 Train loss: 0.076699 Validation loss: 3.9311e-07\n",
      "Epoch: 431 Train loss: 0.075703 Validation loss: 2.2906e-06\n",
      "Epoch: 432 Train loss: 0.074705 Validation loss: 5.8481e-06\n",
      "Epoch: 433 Train loss: 0.073704 Validation loss: 1.1209e-05\n",
      "Epoch: 434 Train loss: 0.072703 Validation loss: 1.8474e-05\n",
      "Epoch: 435 Train loss: 0.071701 Validation loss: 2.7797e-05\n",
      "Epoch: 436 Train loss: 0.070697 Validation loss: 3.9258e-05\n",
      "Epoch: 437 Train loss: 0.069695 Validation loss: 5.2987e-05\n",
      "Epoch: 438 Train loss: 0.068695 Validation loss: 6.9082e-05\n",
      "Epoch: 439 Train loss: 0.067696 Validation loss: 8.7727e-05\n",
      "Epoch: 440 Train loss: 0.066698 Validation loss: 0.00010895\n",
      "Epoch: 441 Train loss: 0.065704 Validation loss: 0.00013296\n",
      "Epoch: 442 Train loss: 0.064712 Validation loss: 0.00015982\n",
      "Epoch: 443 Train loss: 0.063725 Validation loss: 0.00018958\n",
      "Epoch: 444 Train loss: 0.062743 Validation loss: 0.00022223\n",
      "Epoch: 445 Train loss: 0.061767 Validation loss: 0.00025807\n",
      "Epoch: 446 Train loss: 0.060798 Validation loss: 0.00029702\n",
      "Epoch: 447 Train loss: 0.059834 Validation loss: 0.00033926\n",
      "Epoch: 448 Train loss: 0.058878 Validation loss: 0.00038472\n",
      "Epoch: 449 Train loss: 0.057928 Validation loss: 0.00043372\n",
      "Epoch: 450 Train loss: 0.056987 Validation loss: 0.00048581\n",
      "Epoch: 451 Train loss: 0.056055 Validation loss: 0.00054155\n",
      "Epoch: 452 Train loss: 0.055131 Validation loss: 0.00060085\n",
      "Epoch: 453 Train loss: 0.054216 Validation loss: 0.00066348\n",
      "Epoch: 454 Train loss: 0.053311 Validation loss: 0.00072953\n",
      "Epoch: 455 Train loss: 0.052417 Validation loss: 0.00079928\n",
      "Epoch: 456 Train loss: 0.051532 Validation loss: 0.00087247\n",
      "Epoch: 457 Train loss: 0.050657 Validation loss: 0.00094945\n",
      "Epoch: 458 Train loss: 0.049793 Validation loss: 0.0010295\n",
      "Epoch: 459 Train loss: 0.048941 Validation loss: 0.0011133\n",
      "Epoch: 460 Train loss: 0.0481 Validation loss: 0.0012003\n",
      "Epoch: 461 Train loss: 0.047269 Validation loss: 0.0012911\n",
      "Epoch: 462 Train loss: 0.04645 Validation loss: 0.001385\n",
      "Epoch: 463 Train loss: 0.045644 Validation loss: 0.0014823\n",
      "Epoch: 464 Train loss: 0.044848 Validation loss: 0.0015829\n",
      "Epoch: 465 Train loss: 0.044066 Validation loss: 0.0016866\n",
      "Epoch: 466 Train loss: 0.043294 Validation loss: 0.0017939\n",
      "Epoch: 467 Train loss: 0.042534 Validation loss: 0.0019042\n",
      "Epoch: 468 Train loss: 0.041786 Validation loss: 0.0020176\n",
      "Epoch: 469 Train loss: 0.041051 Validation loss: 0.0021341\n",
      "Epoch: 470 Train loss: 0.040326 Validation loss: 0.0022538\n",
      "Epoch: 471 Train loss: 0.039613 Validation loss: 0.0023759\n",
      "Epoch: 472 Train loss: 0.038913 Validation loss: 0.002501\n",
      "Epoch: 473 Train loss: 0.038224 Validation loss: 0.0026294\n",
      "Epoch: 474 Train loss: 0.037546 Validation loss: 0.00276\n",
      "Epoch: 475 Train loss: 0.036881 Validation loss: 0.0028935\n",
      "Epoch: 476 Train loss: 0.036227 Validation loss: 0.0030298\n",
      "Epoch: 477 Train loss: 0.035585 Validation loss: 0.0031682\n",
      "Epoch: 478 Train loss: 0.034955 Validation loss: 0.0033086\n",
      "Epoch: 479 Train loss: 0.034336 Validation loss: 0.0034521\n",
      "Epoch: 480 Train loss: 0.033728 Validation loss: 0.0035973\n",
      "Epoch: 481 Train loss: 0.033132 Validation loss: 0.003745\n",
      "Epoch: 482 Train loss: 0.032546 Validation loss: 0.0038949\n",
      "Epoch: 483 Train loss: 0.03197 Validation loss: 0.0040471\n",
      "Epoch: 484 Train loss: 0.031404 Validation loss: 0.0042015\n",
      "Epoch: 485 Train loss: 0.030849 Validation loss: 0.0043574\n",
      "Epoch: 486 Train loss: 0.030305 Validation loss: 0.0045155\n",
      "Epoch: 487 Train loss: 0.029771 Validation loss: 0.0046747\n",
      "Epoch: 488 Train loss: 0.029249 Validation loss: 0.004836\n",
      "Epoch: 489 Train loss: 0.028736 Validation loss: 0.0049989\n",
      "Epoch: 490 Train loss: 0.028233 Validation loss: 0.0051631\n",
      "Epoch: 491 Train loss: 0.02774 Validation loss: 0.0053291\n",
      "Epoch: 492 Train loss: 0.027256 Validation loss: 0.0054969\n",
      "Epoch: 493 Train loss: 0.026781 Validation loss: 0.0056654\n",
      "Epoch: 494 Train loss: 0.026315 Validation loss: 0.0058358\n",
      "Epoch: 495 Train loss: 0.025858 Validation loss: 0.0060073\n",
      "Epoch: 496 Train loss: 0.025411 Validation loss: 0.0061802\n",
      "Epoch: 497 Train loss: 0.024973 Validation loss: 0.0063535\n",
      "Epoch: 498 Train loss: 0.024543 Validation loss: 0.0065277\n",
      "Epoch: 499 Train loss: 0.024122 Validation loss: 0.0067037\n",
      "Epoch: 500 Train loss: 0.023709 Validation loss: 0.0068807\n",
      "Epoch: 501 Train loss: 0.023303 Validation loss: 0.0070588\n",
      "Epoch: 502 Train loss: 0.022907 Validation loss: 0.007237\n",
      "Epoch: 503 Train loss: 0.022518 Validation loss: 0.0074159\n",
      "Epoch: 504 Train loss: 0.022138 Validation loss: 0.0075956\n",
      "Epoch: 505 Train loss: 0.021765 Validation loss: 0.0077759\n",
      "Epoch: 506 Train loss: 0.021399 Validation loss: 0.0079578\n",
      "Epoch: 507 Train loss: 0.02104 Validation loss: 0.0081396\n",
      "Epoch: 508 Train loss: 0.020689 Validation loss: 0.0083219\n",
      "Epoch: 509 Train loss: 0.020345 Validation loss: 0.0085045\n",
      "Epoch: 510 Train loss: 0.020007 Validation loss: 0.008688\n",
      "Epoch: 511 Train loss: 0.019678 Validation loss: 0.0088716\n",
      "Epoch: 512 Train loss: 0.019354 Validation loss: 0.0090551\n",
      "Epoch: 513 Train loss: 0.019037 Validation loss: 0.0092392\n",
      "Epoch: 514 Train loss: 0.018727 Validation loss: 0.0094233\n",
      "Epoch: 515 Train loss: 0.018423 Validation loss: 0.0096076\n",
      "Epoch: 516 Train loss: 0.018125 Validation loss: 0.0097927\n",
      "Epoch: 517 Train loss: 0.017833 Validation loss: 0.0099781\n",
      "Epoch: 518 Train loss: 0.017546 Validation loss: 0.010163\n",
      "Epoch: 519 Train loss: 0.017266 Validation loss: 0.010347\n",
      "Epoch: 520 Train loss: 0.016992 Validation loss: 0.010533\n",
      "Epoch: 521 Train loss: 0.016723 Validation loss: 0.010718\n",
      "Epoch: 522 Train loss: 0.016459 Validation loss: 0.010903\n",
      "Epoch: 523 Train loss: 0.016202 Validation loss: 0.011087\n",
      "Epoch: 524 Train loss: 0.015949 Validation loss: 0.011272\n",
      "Epoch: 525 Train loss: 0.015701 Validation loss: 0.011457\n",
      "Epoch: 526 Train loss: 0.015459 Validation loss: 0.01164\n",
      "Epoch: 527 Train loss: 0.015222 Validation loss: 0.011825\n",
      "Epoch: 528 Train loss: 0.01499 Validation loss: 0.012008\n",
      "Epoch: 529 Train loss: 0.014762 Validation loss: 0.012192\n",
      "Epoch: 530 Train loss: 0.014539 Validation loss: 0.012375\n",
      "Epoch: 531 Train loss: 0.01432 Validation loss: 0.012558\n",
      "Epoch: 532 Train loss: 0.014106 Validation loss: 0.012739\n",
      "Epoch: 533 Train loss: 0.013897 Validation loss: 0.012922\n",
      "Epoch: 534 Train loss: 0.013691 Validation loss: 0.013104\n",
      "Epoch: 535 Train loss: 0.01349 Validation loss: 0.013285\n",
      "Epoch: 536 Train loss: 0.013293 Validation loss: 0.013466\n",
      "Epoch: 537 Train loss: 0.0131 Validation loss: 0.013646\n",
      "Epoch: 538 Train loss: 0.012911 Validation loss: 0.013826\n",
      "Epoch: 539 Train loss: 0.012727 Validation loss: 0.014004\n",
      "Epoch: 540 Train loss: 0.012546 Validation loss: 0.014182\n",
      "Epoch: 541 Train loss: 0.012369 Validation loss: 0.01436\n",
      "Epoch: 542 Train loss: 0.012195 Validation loss: 0.014538\n",
      "Epoch: 543 Train loss: 0.012025 Validation loss: 0.014714\n",
      "Epoch: 544 Train loss: 0.011858 Validation loss: 0.01489\n",
      "Epoch: 545 Train loss: 0.011695 Validation loss: 0.015066\n",
      "Epoch: 546 Train loss: 0.011535 Validation loss: 0.01524\n",
      "Epoch: 547 Train loss: 0.011379 Validation loss: 0.015415\n",
      "Epoch: 548 Train loss: 0.011225 Validation loss: 0.015588\n",
      "Epoch: 549 Train loss: 0.011075 Validation loss: 0.015761\n",
      "Epoch: 550 Train loss: 0.010928 Validation loss: 0.015933\n",
      "Epoch: 551 Train loss: 0.010784 Validation loss: 0.016104\n",
      "Epoch: 552 Train loss: 0.010643 Validation loss: 0.016275\n",
      "Epoch: 553 Train loss: 0.010504 Validation loss: 0.016446\n",
      "Epoch: 554 Train loss: 0.010369 Validation loss: 0.016614\n",
      "Epoch: 555 Train loss: 0.010236 Validation loss: 0.016782\n",
      "Epoch: 556 Train loss: 0.010107 Validation loss: 0.01695\n",
      "Epoch: 557 Train loss: 0.0099797 Validation loss: 0.017117\n",
      "Epoch: 558 Train loss: 0.0098558 Validation loss: 0.017281\n",
      "Epoch: 559 Train loss: 0.0097352 Validation loss: 0.017444\n",
      "Epoch: 560 Train loss: 0.0096171 Validation loss: 0.017607\n",
      "Epoch: 561 Train loss: 0.0095013 Validation loss: 0.017768\n",
      "Epoch: 562 Train loss: 0.0093877 Validation loss: 0.017929\n",
      "Epoch: 563 Train loss: 0.0092765 Validation loss: 0.01809\n",
      "Epoch: 564 Train loss: 0.0091675 Validation loss: 0.018249\n",
      "Epoch: 565 Train loss: 0.0090611 Validation loss: 0.018408\n",
      "Epoch: 566 Train loss: 0.0089566 Validation loss: 0.018565\n",
      "Epoch: 567 Train loss: 0.0088539 Validation loss: 0.018723\n",
      "Epoch: 568 Train loss: 0.0087536 Validation loss: 0.018879\n",
      "Epoch: 569 Train loss: 0.0086552 Validation loss: 0.019034\n",
      "Epoch: 570 Train loss: 0.0085592 Validation loss: 0.019188\n",
      "Epoch: 571 Train loss: 0.008465 Validation loss: 0.01934\n",
      "Epoch: 572 Train loss: 0.0083728 Validation loss: 0.019493\n",
      "Epoch: 573 Train loss: 0.0082826 Validation loss: 0.019644\n",
      "Epoch: 574 Train loss: 0.0081946 Validation loss: 0.019793\n",
      "Epoch: 575 Train loss: 0.0081087 Validation loss: 0.019942\n",
      "Epoch: 576 Train loss: 0.0080243 Validation loss: 0.02009\n",
      "Epoch: 577 Train loss: 0.0079419 Validation loss: 0.020236\n",
      "Epoch: 578 Train loss: 0.0078613 Validation loss: 0.020381\n",
      "Epoch: 579 Train loss: 0.0077822 Validation loss: 0.020526\n",
      "Epoch: 580 Train loss: 0.0077052 Validation loss: 0.020669\n",
      "Epoch: 581 Train loss: 0.0076298 Validation loss: 0.02081\n",
      "Epoch: 582 Train loss: 0.007556 Validation loss: 0.020952\n",
      "Epoch: 583 Train loss: 0.0074837 Validation loss: 0.021091\n",
      "Epoch: 584 Train loss: 0.007413 Validation loss: 0.021229\n",
      "Epoch: 585 Train loss: 0.0073438 Validation loss: 0.021368\n",
      "Epoch: 586 Train loss: 0.0072761 Validation loss: 0.021504\n",
      "Epoch: 587 Train loss: 0.00721 Validation loss: 0.021639\n",
      "Epoch: 588 Train loss: 0.0071454 Validation loss: 0.021773\n",
      "Epoch: 589 Train loss: 0.0070823 Validation loss: 0.021906\n",
      "Epoch: 590 Train loss: 0.0070202 Validation loss: 0.022039\n",
      "Epoch: 591 Train loss: 0.0069592 Validation loss: 0.02217\n",
      "Epoch: 592 Train loss: 0.0069001 Validation loss: 0.022299\n",
      "Epoch: 593 Train loss: 0.0068421 Validation loss: 0.022428\n",
      "Epoch: 594 Train loss: 0.0067856 Validation loss: 0.022555\n",
      "Epoch: 595 Train loss: 0.0067303 Validation loss: 0.022681\n",
      "Epoch: 596 Train loss: 0.0066761 Validation loss: 0.022807\n",
      "Epoch: 597 Train loss: 0.0066232 Validation loss: 0.02293\n",
      "Epoch: 598 Train loss: 0.0065715 Validation loss: 0.023053\n",
      "Epoch: 599 Train loss: 0.0065208 Validation loss: 0.023175\n",
      "Epoch: 600 Train loss: 0.0064712 Validation loss: 0.023296\n",
      "Epoch: 601 Train loss: 0.0064229 Validation loss: 0.023414\n",
      "Epoch: 602 Train loss: 0.0063757 Validation loss: 0.023532\n",
      "Epoch: 603 Train loss: 0.0063295 Validation loss: 0.023649\n",
      "Epoch: 604 Train loss: 0.0062844 Validation loss: 0.023764\n",
      "Epoch: 605 Train loss: 0.0062401 Validation loss: 0.023878\n",
      "Epoch: 606 Train loss: 0.0061971 Validation loss: 0.023991\n",
      "Epoch: 607 Train loss: 0.0061548 Validation loss: 0.024104\n",
      "Epoch: 608 Train loss: 0.0061135 Validation loss: 0.024215\n",
      "Epoch: 609 Train loss: 0.006073 Validation loss: 0.024325\n",
      "Epoch: 610 Train loss: 0.0060336 Validation loss: 0.024433\n",
      "Epoch: 611 Train loss: 0.0059947 Validation loss: 0.024543\n",
      "Epoch: 612 Train loss: 0.0059569 Validation loss: 0.024649\n",
      "Epoch: 613 Train loss: 0.0059196 Validation loss: 0.024756\n",
      "Epoch: 614 Train loss: 0.005883 Validation loss: 0.024861\n",
      "Epoch: 615 Train loss: 0.0058476 Validation loss: 0.024965\n",
      "Epoch: 616 Train loss: 0.0058129 Validation loss: 0.025068\n",
      "Epoch: 617 Train loss: 0.0057789 Validation loss: 0.025169\n",
      "Epoch: 618 Train loss: 0.0057457 Validation loss: 0.02527\n",
      "Epoch: 619 Train loss: 0.0057131 Validation loss: 0.02537\n",
      "Epoch: 620 Train loss: 0.0056815 Validation loss: 0.025467\n",
      "Epoch: 621 Train loss: 0.0056506 Validation loss: 0.025565\n",
      "Epoch: 622 Train loss: 0.0056203 Validation loss: 0.02566\n",
      "Epoch: 623 Train loss: 0.0055907 Validation loss: 0.025756\n",
      "Epoch: 624 Train loss: 0.0055616 Validation loss: 0.02585\n",
      "Epoch: 625 Train loss: 0.0055331 Validation loss: 0.025944\n",
      "Epoch: 626 Train loss: 0.0055052 Validation loss: 0.026036\n",
      "Epoch: 627 Train loss: 0.005478 Validation loss: 0.026127\n",
      "Epoch: 628 Train loss: 0.0054513 Validation loss: 0.026217\n",
      "Epoch: 629 Train loss: 0.0054252 Validation loss: 0.026306\n",
      "Epoch: 630 Train loss: 0.0053997 Validation loss: 0.026395\n",
      "Epoch: 631 Train loss: 0.0053747 Validation loss: 0.026483\n",
      "Epoch: 632 Train loss: 0.0053502 Validation loss: 0.026569\n",
      "Epoch: 633 Train loss: 0.0053261 Validation loss: 0.026655\n",
      "Epoch: 634 Train loss: 0.0053026 Validation loss: 0.026739\n",
      "Epoch: 635 Train loss: 0.0052798 Validation loss: 0.026823\n",
      "Epoch: 636 Train loss: 0.0052571 Validation loss: 0.026906\n",
      "Epoch: 637 Train loss: 0.0052351 Validation loss: 0.026988\n",
      "Epoch: 638 Train loss: 0.0052133 Validation loss: 0.02707\n",
      "Epoch: 639 Train loss: 0.0051919 Validation loss: 0.027152\n",
      "Epoch: 640 Train loss: 0.0051709 Validation loss: 0.027233\n",
      "Epoch: 641 Train loss: 0.0051502 Validation loss: 0.027313\n",
      "Epoch: 642 Train loss: 0.0051299 Validation loss: 0.027393\n",
      "Epoch: 643 Train loss: 0.00511 Validation loss: 0.027471\n",
      "Epoch: 644 Train loss: 0.0050905 Validation loss: 0.027549\n",
      "Epoch: 645 Train loss: 0.0050712 Validation loss: 0.027628\n",
      "Epoch: 646 Train loss: 0.0050522 Validation loss: 0.027706\n",
      "Epoch: 647 Train loss: 0.0050335 Validation loss: 0.027783\n",
      "Epoch: 648 Train loss: 0.0050152 Validation loss: 0.027859\n",
      "Epoch: 649 Train loss: 0.0049972 Validation loss: 0.027935\n",
      "Epoch: 650 Train loss: 0.0049795 Validation loss: 0.028011\n",
      "Epoch: 651 Train loss: 0.004962 Validation loss: 0.028086\n",
      "Epoch: 652 Train loss: 0.0049449 Validation loss: 0.02816\n",
      "Epoch: 653 Train loss: 0.0049281 Validation loss: 0.028234\n",
      "Epoch: 654 Train loss: 0.0049116 Validation loss: 0.028307\n",
      "Epoch: 655 Train loss: 0.0048955 Validation loss: 0.028379\n",
      "Epoch: 656 Train loss: 0.0048797 Validation loss: 0.02845\n",
      "Epoch: 657 Train loss: 0.0048641 Validation loss: 0.028522\n",
      "Epoch: 658 Train loss: 0.0048487 Validation loss: 0.028593\n",
      "Epoch: 659 Train loss: 0.0048335 Validation loss: 0.028664\n",
      "Epoch: 660 Train loss: 0.0048186 Validation loss: 0.028735\n",
      "Epoch: 661 Train loss: 0.0048038 Validation loss: 0.028805\n",
      "Epoch: 662 Train loss: 0.0047894 Validation loss: 0.028874\n",
      "Epoch: 663 Train loss: 0.0047752 Validation loss: 0.028943\n",
      "Epoch: 664 Train loss: 0.0047612 Validation loss: 0.029011\n",
      "Epoch: 665 Train loss: 0.0047474 Validation loss: 0.02908\n",
      "Epoch: 666 Train loss: 0.0047338 Validation loss: 0.029148\n",
      "Epoch: 667 Train loss: 0.0047205 Validation loss: 0.029215\n",
      "Epoch: 668 Train loss: 0.0047073 Validation loss: 0.029284\n",
      "Epoch: 669 Train loss: 0.0046941 Validation loss: 0.029351\n",
      "Epoch: 670 Train loss: 0.0046813 Validation loss: 0.029419\n",
      "Epoch: 671 Train loss: 0.0046687 Validation loss: 0.029484\n",
      "Epoch: 672 Train loss: 0.0046563 Validation loss: 0.029552\n",
      "Epoch: 673 Train loss: 0.0046439 Validation loss: 0.029619\n",
      "Epoch: 674 Train loss: 0.0046316 Validation loss: 0.029686\n",
      "Epoch: 675 Train loss: 0.0046197 Validation loss: 0.029751\n",
      "Epoch: 676 Train loss: 0.0046079 Validation loss: 0.029817\n",
      "Epoch: 677 Train loss: 0.0045961 Validation loss: 0.029885\n",
      "Epoch: 678 Train loss: 0.0045844 Validation loss: 0.029952\n",
      "Epoch: 679 Train loss: 0.0045729 Validation loss: 0.030019\n",
      "Epoch: 680 Train loss: 0.0045616 Validation loss: 0.030084\n",
      "Epoch: 681 Train loss: 0.0045505 Validation loss: 0.030151\n",
      "Epoch: 682 Train loss: 0.0045395 Validation loss: 0.030216\n",
      "Epoch: 683 Train loss: 0.0045288 Validation loss: 0.030281\n",
      "Epoch: 684 Train loss: 0.0045181 Validation loss: 0.030347\n",
      "Epoch: 685 Train loss: 0.0045075 Validation loss: 0.030412\n",
      "Epoch: 686 Train loss: 0.0044972 Validation loss: 0.030477\n",
      "Epoch: 687 Train loss: 0.004487 Validation loss: 0.030541\n",
      "Epoch: 688 Train loss: 0.0044769 Validation loss: 0.030606\n",
      "Epoch: 689 Train loss: 0.0044669 Validation loss: 0.030672\n",
      "Epoch: 690 Train loss: 0.004457 Validation loss: 0.030737\n",
      "Epoch: 691 Train loss: 0.0044473 Validation loss: 0.030802\n",
      "Epoch: 692 Train loss: 0.0044376 Validation loss: 0.030868\n",
      "Epoch: 693 Train loss: 0.004428 Validation loss: 0.030934\n",
      "Epoch: 694 Train loss: 0.0044186 Validation loss: 0.031\n",
      "Epoch: 695 Train loss: 0.0044093 Validation loss: 0.031066\n",
      "Epoch: 696 Train loss: 0.0044 Validation loss: 0.031131\n",
      "Epoch: 697 Train loss: 0.0043909 Validation loss: 0.031197\n",
      "Epoch: 698 Train loss: 0.004382 Validation loss: 0.031264\n",
      "Epoch: 699 Train loss: 0.0043732 Validation loss: 0.031329\n",
      "Epoch: 700 Train loss: 0.0043644 Validation loss: 0.031395\n",
      "Epoch: 701 Train loss: 0.0043558 Validation loss: 0.031463\n",
      "Epoch: 702 Train loss: 0.0043472 Validation loss: 0.031529\n",
      "Epoch: 703 Train loss: 0.0043388 Validation loss: 0.031597\n",
      "Epoch: 704 Train loss: 0.0043304 Validation loss: 0.031664\n",
      "Epoch: 705 Train loss: 0.0043222 Validation loss: 0.031731\n",
      "Epoch: 706 Train loss: 0.0043141 Validation loss: 0.031799\n",
      "Epoch: 707 Train loss: 0.004306 Validation loss: 0.031868\n",
      "Epoch: 708 Train loss: 0.0042981 Validation loss: 0.031936\n",
      "Epoch: 709 Train loss: 0.0042903 Validation loss: 0.032004\n",
      "Epoch: 710 Train loss: 0.0042826 Validation loss: 0.032073\n",
      "Epoch: 711 Train loss: 0.004275 Validation loss: 0.032141\n",
      "Epoch: 712 Train loss: 0.0042676 Validation loss: 0.032209\n",
      "Epoch: 713 Train loss: 0.0042602 Validation loss: 0.032279\n",
      "Epoch: 714 Train loss: 0.004253 Validation loss: 0.032349\n",
      "Epoch: 715 Train loss: 0.0042458 Validation loss: 0.032418\n",
      "Epoch: 716 Train loss: 0.0042388 Validation loss: 0.032487\n",
      "Epoch: 717 Train loss: 0.0042319 Validation loss: 0.032558\n",
      "Epoch: 718 Train loss: 0.0042251 Validation loss: 0.032628\n",
      "Epoch: 719 Train loss: 0.0042184 Validation loss: 0.032698\n",
      "Epoch: 720 Train loss: 0.0042119 Validation loss: 0.032767\n",
      "Epoch: 721 Train loss: 0.0042055 Validation loss: 0.032837\n",
      "Epoch: 722 Train loss: 0.0041992 Validation loss: 0.032907\n",
      "Epoch: 723 Train loss: 0.0041931 Validation loss: 0.032976\n",
      "Epoch: 724 Train loss: 0.004187 Validation loss: 0.033046\n",
      "Epoch: 725 Train loss: 0.0041811 Validation loss: 0.033117\n",
      "Epoch: 726 Train loss: 0.0041752 Validation loss: 0.033187\n",
      "Epoch: 727 Train loss: 0.0041696 Validation loss: 0.033256\n",
      "Epoch: 728 Train loss: 0.004164 Validation loss: 0.033326\n",
      "Epoch: 729 Train loss: 0.0041586 Validation loss: 0.033394\n",
      "Epoch: 730 Train loss: 0.0041533 Validation loss: 0.033464\n",
      "Epoch: 731 Train loss: 0.0041481 Validation loss: 0.033531\n",
      "Epoch: 732 Train loss: 0.0041431 Validation loss: 0.033599\n",
      "Epoch: 733 Train loss: 0.0041382 Validation loss: 0.033666\n",
      "Epoch: 734 Train loss: 0.0041334 Validation loss: 0.033733\n",
      "Epoch: 735 Train loss: 0.0041287 Validation loss: 0.033799\n",
      "Epoch: 736 Train loss: 0.0041242 Validation loss: 0.033864\n",
      "Epoch: 737 Train loss: 0.0041198 Validation loss: 0.033928\n",
      "Epoch: 738 Train loss: 0.0041155 Validation loss: 0.033993\n",
      "Epoch: 739 Train loss: 0.0041113 Validation loss: 0.034056\n",
      "Epoch: 740 Train loss: 0.0041073 Validation loss: 0.034117\n",
      "Epoch: 741 Train loss: 0.0041033 Validation loss: 0.034179\n",
      "Epoch: 742 Train loss: 0.0040994 Validation loss: 0.03424\n",
      "Epoch: 743 Train loss: 0.0040957 Validation loss: 0.034301\n",
      "Epoch: 744 Train loss: 0.004092 Validation loss: 0.034359\n",
      "Epoch: 745 Train loss: 0.0040885 Validation loss: 0.034417\n",
      "Epoch: 746 Train loss: 0.004085 Validation loss: 0.034471\n",
      "Epoch: 747 Train loss: 0.0040817 Validation loss: 0.034524\n",
      "Epoch: 748 Train loss: 0.0040784 Validation loss: 0.034577\n",
      "Epoch: 749 Train loss: 0.0040752 Validation loss: 0.034628\n",
      "Epoch: 750 Train loss: 0.0040721 Validation loss: 0.034678\n",
      "Epoch: 751 Train loss: 0.0040691 Validation loss: 0.034726\n",
      "Epoch: 752 Train loss: 0.0040661 Validation loss: 0.034772\n",
      "Epoch: 753 Train loss: 0.0040632 Validation loss: 0.034818\n",
      "Epoch: 754 Train loss: 0.0040603 Validation loss: 0.03486\n",
      "Epoch: 755 Train loss: 0.0040576 Validation loss: 0.034901\n",
      "Epoch: 756 Train loss: 0.0040548 Validation loss: 0.034942\n",
      "Epoch: 757 Train loss: 0.0040521 Validation loss: 0.03498\n",
      "Epoch: 758 Train loss: 0.0040495 Validation loss: 0.035016\n",
      "Epoch: 759 Train loss: 0.0040468 Validation loss: 0.035051\n",
      "Epoch: 760 Train loss: 0.0040443 Validation loss: 0.035083\n",
      "Epoch: 761 Train loss: 0.0040417 Validation loss: 0.035113\n",
      "Epoch: 762 Train loss: 0.0040392 Validation loss: 0.035143\n",
      "Epoch: 763 Train loss: 0.0040367 Validation loss: 0.035172\n",
      "Epoch: 764 Train loss: 0.0040342 Validation loss: 0.035198\n",
      "Epoch: 765 Train loss: 0.0040318 Validation loss: 0.035223\n",
      "Epoch: 766 Train loss: 0.0040293 Validation loss: 0.035247\n",
      "Epoch: 767 Train loss: 0.0040269 Validation loss: 0.03527\n",
      "Epoch: 768 Train loss: 0.0040245 Validation loss: 0.035291\n",
      "Epoch: 769 Train loss: 0.0040221 Validation loss: 0.035311\n",
      "Epoch: 770 Train loss: 0.0040198 Validation loss: 0.035328\n",
      "Epoch: 771 Train loss: 0.0040174 Validation loss: 0.035343\n",
      "Epoch: 772 Train loss: 0.004015 Validation loss: 0.035361\n",
      "Epoch: 773 Train loss: 0.0040127 Validation loss: 0.035375\n",
      "Epoch: 774 Train loss: 0.0040103 Validation loss: 0.035389\n",
      "Epoch: 775 Train loss: 0.004008 Validation loss: 0.035403\n",
      "Epoch: 776 Train loss: 0.0040057 Validation loss: 0.035413\n",
      "Epoch: 777 Train loss: 0.0040033 Validation loss: 0.035427\n",
      "Epoch: 778 Train loss: 0.004001 Validation loss: 0.035437\n",
      "Epoch: 779 Train loss: 0.0039987 Validation loss: 0.035448\n",
      "Epoch: 780 Train loss: 0.0039964 Validation loss: 0.035459\n",
      "Epoch: 781 Train loss: 0.0039941 Validation loss: 0.03547\n",
      "Epoch: 782 Train loss: 0.0039918 Validation loss: 0.035478\n",
      "Epoch: 783 Train loss: 0.0039895 Validation loss: 0.035487\n",
      "Epoch: 784 Train loss: 0.0039872 Validation loss: 0.035493\n",
      "Epoch: 785 Train loss: 0.0039849 Validation loss: 0.035502\n",
      "Epoch: 786 Train loss: 0.0039826 Validation loss: 0.035509\n",
      "Epoch: 787 Train loss: 0.0039803 Validation loss: 0.035518\n",
      "Epoch: 788 Train loss: 0.003978 Validation loss: 0.035525\n",
      "Epoch: 789 Train loss: 0.0039758 Validation loss: 0.035533\n",
      "Epoch: 790 Train loss: 0.0039735 Validation loss: 0.03554\n",
      "Epoch: 791 Train loss: 0.0039712 Validation loss: 0.035547\n",
      "Epoch: 792 Train loss: 0.003969 Validation loss: 0.035555\n",
      "Epoch: 793 Train loss: 0.0039667 Validation loss: 0.035562\n",
      "Epoch: 794 Train loss: 0.0039645 Validation loss: 0.035569\n",
      "Epoch: 795 Train loss: 0.0039622 Validation loss: 0.035576\n",
      "Epoch: 796 Train loss: 0.00396 Validation loss: 0.035582\n",
      "Epoch: 797 Train loss: 0.0039577 Validation loss: 0.035588\n",
      "Epoch: 798 Train loss: 0.0039555 Validation loss: 0.035595\n",
      "Epoch: 799 Train loss: 0.0039533 Validation loss: 0.035602\n",
      "Epoch: 800 Train loss: 0.003951 Validation loss: 0.035608\n",
      "Epoch: 801 Train loss: 0.0039488 Validation loss: 0.035614\n",
      "Epoch: 802 Train loss: 0.0039466 Validation loss: 0.035621\n",
      "Epoch: 803 Train loss: 0.0039443 Validation loss: 0.035626\n",
      "Epoch: 804 Train loss: 0.0039421 Validation loss: 0.035633\n",
      "Epoch: 805 Train loss: 0.0039399 Validation loss: 0.03564\n",
      "Epoch: 806 Train loss: 0.0039377 Validation loss: 0.035646\n",
      "Epoch: 807 Train loss: 0.0039355 Validation loss: 0.035653\n",
      "Epoch: 808 Train loss: 0.0039333 Validation loss: 0.035659\n",
      "Epoch: 809 Train loss: 0.0039311 Validation loss: 0.035666\n",
      "Epoch: 810 Train loss: 0.0039289 Validation loss: 0.035672\n",
      "Epoch: 811 Train loss: 0.0039267 Validation loss: 0.035678\n",
      "Epoch: 812 Train loss: 0.0039245 Validation loss: 0.035683\n",
      "Epoch: 813 Train loss: 0.0039223 Validation loss: 0.035689\n",
      "Epoch: 814 Train loss: 0.0039202 Validation loss: 0.035696\n",
      "Epoch: 815 Train loss: 0.003918 Validation loss: 0.035703\n",
      "Epoch: 816 Train loss: 0.0039158 Validation loss: 0.035707\n",
      "Epoch: 817 Train loss: 0.0039136 Validation loss: 0.035713\n",
      "Epoch: 818 Train loss: 0.0039115 Validation loss: 0.035719\n",
      "Epoch: 819 Train loss: 0.0039093 Validation loss: 0.035725\n",
      "Epoch: 820 Train loss: 0.0039071 Validation loss: 0.035732\n",
      "Epoch: 821 Train loss: 0.003905 Validation loss: 0.035737\n",
      "Epoch: 822 Train loss: 0.0039028 Validation loss: 0.035742\n",
      "Epoch: 823 Train loss: 0.0039007 Validation loss: 0.035749\n",
      "Epoch: 824 Train loss: 0.0038985 Validation loss: 0.035753\n",
      "Epoch: 825 Train loss: 0.0038964 Validation loss: 0.035761\n",
      "Epoch: 826 Train loss: 0.0038942 Validation loss: 0.035765\n",
      "Epoch: 827 Train loss: 0.0038921 Validation loss: 0.035771\n",
      "Epoch: 828 Train loss: 0.0038899 Validation loss: 0.035776\n",
      "Epoch: 829 Train loss: 0.0038878 Validation loss: 0.035782\n",
      "Epoch: 830 Train loss: 0.0038857 Validation loss: 0.035788\n",
      "Epoch: 831 Train loss: 0.0038835 Validation loss: 0.035794\n",
      "Epoch: 832 Train loss: 0.0038814 Validation loss: 0.035798\n",
      "Epoch: 833 Train loss: 0.0038793 Validation loss: 0.035804\n",
      "Epoch: 834 Train loss: 0.0038771 Validation loss: 0.03581\n",
      "Epoch: 835 Train loss: 0.003875 Validation loss: 0.035814\n",
      "Epoch: 836 Train loss: 0.0038729 Validation loss: 0.03582\n",
      "Epoch: 837 Train loss: 0.0038708 Validation loss: 0.035826\n",
      "Epoch: 838 Train loss: 0.0038687 Validation loss: 0.035831\n",
      "Epoch: 839 Train loss: 0.0038666 Validation loss: 0.035837\n",
      "Epoch: 840 Train loss: 0.0038645 Validation loss: 0.035841\n",
      "Epoch: 841 Train loss: 0.0038624 Validation loss: 0.035846\n",
      "Epoch: 842 Train loss: 0.0038603 Validation loss: 0.035852\n",
      "Epoch: 843 Train loss: 0.0038582 Validation loss: 0.035856\n",
      "Epoch: 844 Train loss: 0.0038561 Validation loss: 0.035861\n",
      "Epoch: 845 Train loss: 0.003854 Validation loss: 0.035867\n",
      "Epoch: 846 Train loss: 0.0038519 Validation loss: 0.035871\n",
      "Epoch: 847 Train loss: 0.0038498 Validation loss: 0.035876\n",
      "Epoch: 848 Train loss: 0.0038477 Validation loss: 0.035882\n",
      "Epoch: 849 Train loss: 0.0038456 Validation loss: 0.035887\n",
      "Epoch: 850 Train loss: 0.0038435 Validation loss: 0.035892\n",
      "Epoch: 851 Train loss: 0.0038415 Validation loss: 0.035897\n",
      "Epoch: 852 Train loss: 0.0038394 Validation loss: 0.035901\n",
      "Epoch: 853 Train loss: 0.0038373 Validation loss: 0.035907\n",
      "Epoch: 854 Train loss: 0.0038353 Validation loss: 0.03591\n",
      "Epoch: 855 Train loss: 0.0038332 Validation loss: 0.035915\n",
      "Epoch: 856 Train loss: 0.0038311 Validation loss: 0.03592\n",
      "Epoch: 857 Train loss: 0.0038291 Validation loss: 0.035925\n",
      "Epoch: 858 Train loss: 0.003827 Validation loss: 0.03593\n",
      "Epoch: 859 Train loss: 0.003825 Validation loss: 0.035934\n",
      "Epoch: 860 Train loss: 0.0038229 Validation loss: 0.035939\n",
      "Epoch: 861 Train loss: 0.0038209 Validation loss: 0.035944\n",
      "Epoch: 862 Train loss: 0.0038188 Validation loss: 0.035948\n",
      "Epoch: 863 Train loss: 0.0038168 Validation loss: 0.035952\n",
      "Epoch: 864 Train loss: 0.0038147 Validation loss: 0.035957\n",
      "Epoch: 865 Train loss: 0.0038127 Validation loss: 0.035962\n",
      "Epoch: 866 Train loss: 0.0038106 Validation loss: 0.035966\n",
      "Epoch: 867 Train loss: 0.0038086 Validation loss: 0.03597\n",
      "Epoch: 868 Train loss: 0.0038066 Validation loss: 0.035975\n",
      "Epoch: 869 Train loss: 0.0038045 Validation loss: 0.035978\n",
      "Epoch: 870 Train loss: 0.0038025 Validation loss: 0.035983\n",
      "Epoch: 871 Train loss: 0.0038005 Validation loss: 0.035986\n",
      "Epoch: 872 Train loss: 0.0037985 Validation loss: 0.03599\n",
      "Epoch: 873 Train loss: 0.0037964 Validation loss: 0.035995\n",
      "Epoch: 874 Train loss: 0.0037944 Validation loss: 0.035999\n",
      "Epoch: 875 Train loss: 0.0037924 Validation loss: 0.036005\n",
      "Epoch: 876 Train loss: 0.0037904 Validation loss: 0.036009\n",
      "Epoch: 877 Train loss: 0.0037884 Validation loss: 0.036011\n",
      "Epoch: 878 Train loss: 0.0037864 Validation loss: 0.036016\n",
      "Epoch: 879 Train loss: 0.0037844 Validation loss: 0.036019\n",
      "Epoch: 880 Train loss: 0.0037824 Validation loss: 0.036024\n",
      "Epoch: 881 Train loss: 0.0037804 Validation loss: 0.036027\n",
      "Epoch: 882 Train loss: 0.0037784 Validation loss: 0.036031\n",
      "Epoch: 883 Train loss: 0.0037764 Validation loss: 0.036033\n",
      "Epoch: 884 Train loss: 0.0037744 Validation loss: 0.036037\n",
      "Epoch: 885 Train loss: 0.0037724 Validation loss: 0.036042\n",
      "Epoch: 886 Train loss: 0.0037704 Validation loss: 0.036045\n",
      "Epoch: 887 Train loss: 0.0037684 Validation loss: 0.036049\n",
      "Epoch: 888 Train loss: 0.0037664 Validation loss: 0.036052\n",
      "Epoch: 889 Train loss: 0.0037645 Validation loss: 0.036054\n",
      "Epoch: 890 Train loss: 0.0037625 Validation loss: 0.036058\n",
      "Epoch: 891 Train loss: 0.0037605 Validation loss: 0.036061\n",
      "Epoch: 892 Train loss: 0.0037585 Validation loss: 0.036063\n",
      "Epoch: 893 Train loss: 0.0037565 Validation loss: 0.036067\n",
      "Epoch: 894 Train loss: 0.0037546 Validation loss: 0.03607\n",
      "Epoch: 895 Train loss: 0.0037526 Validation loss: 0.036073\n",
      "Epoch: 896 Train loss: 0.0037507 Validation loss: 0.036076\n",
      "Epoch: 897 Train loss: 0.0037487 Validation loss: 0.036081\n",
      "Epoch: 898 Train loss: 0.0037467 Validation loss: 0.036083\n",
      "Epoch: 899 Train loss: 0.0037448 Validation loss: 0.036086\n",
      "Epoch: 900 Train loss: 0.0037428 Validation loss: 0.036089\n",
      "Epoch: 901 Train loss: 0.0037409 Validation loss: 0.036092\n",
      "Epoch: 902 Train loss: 0.0037389 Validation loss: 0.036094\n",
      "Epoch: 903 Train loss: 0.003737 Validation loss: 0.036096\n",
      "Epoch: 904 Train loss: 0.003735 Validation loss: 0.0361\n",
      "Epoch: 905 Train loss: 0.0037331 Validation loss: 0.036103\n",
      "Epoch: 906 Train loss: 0.0037311 Validation loss: 0.036104\n",
      "Epoch: 907 Train loss: 0.0037292 Validation loss: 0.036107\n",
      "Epoch: 908 Train loss: 0.0037273 Validation loss: 0.03611\n",
      "Epoch: 909 Train loss: 0.0037253 Validation loss: 0.036113\n",
      "Epoch: 910 Train loss: 0.0037234 Validation loss: 0.036115\n",
      "Epoch: 911 Train loss: 0.0037215 Validation loss: 0.036118\n",
      "Epoch: 912 Train loss: 0.0037195 Validation loss: 0.03612\n",
      "Epoch: 913 Train loss: 0.0037176 Validation loss: 0.036123\n",
      "Epoch: 914 Train loss: 0.0037157 Validation loss: 0.036125\n",
      "Epoch: 915 Train loss: 0.0037138 Validation loss: 0.036127\n",
      "Epoch: 916 Train loss: 0.0037119 Validation loss: 0.03613\n",
      "Epoch: 917 Train loss: 0.0037099 Validation loss: 0.036132\n",
      "Epoch: 918 Train loss: 0.003708 Validation loss: 0.036134\n",
      "Epoch: 919 Train loss: 0.0037061 Validation loss: 0.036137\n",
      "Epoch: 920 Train loss: 0.0037042 Validation loss: 0.036139\n",
      "Epoch: 921 Train loss: 0.0037023 Validation loss: 0.03614\n",
      "Epoch: 922 Train loss: 0.0037004 Validation loss: 0.036143\n",
      "Epoch: 923 Train loss: 0.0036985 Validation loss: 0.036145\n",
      "Epoch: 924 Train loss: 0.0036966 Validation loss: 0.036147\n",
      "Epoch: 925 Train loss: 0.0036947 Validation loss: 0.036148\n",
      "Epoch: 926 Train loss: 0.0036928 Validation loss: 0.036151\n",
      "Epoch: 927 Train loss: 0.0036909 Validation loss: 0.036153\n",
      "Epoch: 928 Train loss: 0.003689 Validation loss: 0.036154\n",
      "Epoch: 929 Train loss: 0.0036871 Validation loss: 0.036156\n",
      "Epoch: 930 Train loss: 0.0036852 Validation loss: 0.036157\n",
      "Epoch: 931 Train loss: 0.0036833 Validation loss: 0.036161\n",
      "Epoch: 932 Train loss: 0.0036815 Validation loss: 0.036162\n",
      "Epoch: 933 Train loss: 0.0036796 Validation loss: 0.036162\n",
      "Epoch: 934 Train loss: 0.0036777 Validation loss: 0.036164\n",
      "Epoch: 935 Train loss: 0.0036758 Validation loss: 0.036166\n",
      "Epoch: 936 Train loss: 0.0036739 Validation loss: 0.036166\n",
      "Epoch: 937 Train loss: 0.0036721 Validation loss: 0.036168\n",
      "Epoch: 938 Train loss: 0.0036702 Validation loss: 0.03617\n",
      "Epoch: 939 Train loss: 0.0036683 Validation loss: 0.036172\n",
      "Epoch: 940 Train loss: 0.0036665 Validation loss: 0.036172\n",
      "Epoch: 941 Train loss: 0.0036646 Validation loss: 0.036174\n",
      "Epoch: 942 Train loss: 0.0036627 Validation loss: 0.036176\n",
      "Epoch: 943 Train loss: 0.0036609 Validation loss: 0.036176\n",
      "Epoch: 944 Train loss: 0.003659 Validation loss: 0.036178\n",
      "Epoch: 945 Train loss: 0.0036571 Validation loss: 0.036179\n",
      "Epoch: 946 Train loss: 0.0036553 Validation loss: 0.03618\n",
      "Epoch: 947 Train loss: 0.0036534 Validation loss: 0.036182\n",
      "Epoch: 948 Train loss: 0.0036516 Validation loss: 0.036184\n",
      "Epoch: 949 Train loss: 0.0036497 Validation loss: 0.036185\n",
      "Epoch: 950 Train loss: 0.0036479 Validation loss: 0.036186\n",
      "Epoch: 951 Train loss: 0.003646 Validation loss: 0.036187\n",
      "Epoch: 952 Train loss: 0.0036442 Validation loss: 0.036188\n",
      "Epoch: 953 Train loss: 0.0036424 Validation loss: 0.036188\n",
      "Epoch: 954 Train loss: 0.0036405 Validation loss: 0.036189\n",
      "Epoch: 955 Train loss: 0.0036387 Validation loss: 0.036188\n",
      "Epoch: 956 Train loss: 0.0036368 Validation loss: 0.03619\n",
      "Epoch: 957 Train loss: 0.003635 Validation loss: 0.03619\n",
      "Epoch: 958 Train loss: 0.0036332 Validation loss: 0.036191\n",
      "Epoch: 959 Train loss: 0.0036313 Validation loss: 0.036192\n",
      "Epoch: 960 Train loss: 0.0036295 Validation loss: 0.036191\n",
      "Epoch: 961 Train loss: 0.0036277 Validation loss: 0.036191\n",
      "Epoch: 962 Train loss: 0.0036259 Validation loss: 0.036191\n",
      "Epoch: 963 Train loss: 0.003624 Validation loss: 0.036191\n",
      "Epoch: 964 Train loss: 0.0036222 Validation loss: 0.036193\n",
      "Epoch: 965 Train loss: 0.0036204 Validation loss: 0.036192\n",
      "Epoch: 966 Train loss: 0.0036186 Validation loss: 0.036194\n",
      "Epoch: 967 Train loss: 0.0036168 Validation loss: 0.036193\n",
      "Epoch: 968 Train loss: 0.003615 Validation loss: 0.036194\n",
      "Epoch: 969 Train loss: 0.0036132 Validation loss: 0.036195\n",
      "Epoch: 970 Train loss: 0.0036113 Validation loss: 0.036195\n",
      "Epoch: 971 Train loss: 0.0036095 Validation loss: 0.036195\n",
      "Epoch: 972 Train loss: 0.0036077 Validation loss: 0.036195\n",
      "Epoch: 973 Train loss: 0.0036059 Validation loss: 0.036195\n",
      "Epoch: 974 Train loss: 0.0036041 Validation loss: 0.036196\n",
      "Epoch: 975 Train loss: 0.0036023 Validation loss: 0.036195\n",
      "Epoch: 976 Train loss: 0.0036005 Validation loss: 0.036195\n",
      "Epoch: 977 Train loss: 0.0035987 Validation loss: 0.036195\n",
      "Epoch: 978 Train loss: 0.0035969 Validation loss: 0.036196\n",
      "Epoch: 979 Train loss: 0.0035951 Validation loss: 0.036194\n",
      "Epoch: 980 Train loss: 0.0035933 Validation loss: 0.036194\n",
      "Epoch: 981 Train loss: 0.0035915 Validation loss: 0.036194\n",
      "Epoch: 982 Train loss: 0.0035897 Validation loss: 0.036193\n",
      "Epoch: 983 Train loss: 0.003588 Validation loss: 0.036193\n",
      "Epoch: 984 Train loss: 0.0035862 Validation loss: 0.036193\n",
      "Epoch: 985 Train loss: 0.0035844 Validation loss: 0.036192\n",
      "Epoch: 986 Train loss: 0.0035826 Validation loss: 0.036191\n",
      "Epoch: 987 Train loss: 0.0035808 Validation loss: 0.03619\n",
      "Epoch: 988 Train loss: 0.003579 Validation loss: 0.036187\n",
      "Epoch: 989 Train loss: 0.0035773 Validation loss: 0.036188\n",
      "Epoch: 990 Train loss: 0.0035755 Validation loss: 0.036187\n",
      "Epoch: 991 Train loss: 0.0035737 Validation loss: 0.036187\n",
      "Epoch: 992 Train loss: 0.0035719 Validation loss: 0.036186\n",
      "Epoch: 993 Train loss: 0.0035702 Validation loss: 0.036186\n",
      "Epoch: 994 Train loss: 0.0035684 Validation loss: 0.036186\n",
      "Epoch: 995 Train loss: 0.0035666 Validation loss: 0.036184\n",
      "Epoch: 996 Train loss: 0.0035649 Validation loss: 0.036185\n",
      "Epoch: 997 Train loss: 0.0035631 Validation loss: 0.036183\n",
      "Epoch: 998 Train loss: 0.0035613 Validation loss: 0.036183\n",
      "Epoch: 999 Train loss: 0.0035595 Validation loss: 0.036181\n"
     ]
    }
   ],
   "source": [
    "N_train_samples_overfit = 30\n",
    "N_epochs = 1000\n",
    "batch_size = 30\n",
    "\n",
    "sel_idx = np.arange(0, N_train_samples)\n",
    "sel_idx = np.random.choice(sel_idx, N_train_samples_overfit)\n",
    "x_train_overfit = x_train[sel_idx]\n",
    "y_train_overfit = y_train[sel_idx]\n",
    "\n",
    "train_overfit_ds = tf.data.Dataset.from_tensor_slices((x_train_overfit, y_train_overfit)).shuffle(N_train_samples_overfit).batch(batch_size).repeat()\n",
    "\n",
    "epoch = 0\n",
    "train_iters = 0\n",
    "train_loss = 0.0\n",
    "for x_t, y_t in train_overfit_ds:\n",
    "    train_loss += train_step(big_mdl, big_opt, x_t, y_t)\n",
    "    train_iters += 1\n",
    "    if (train_iters >= int(N_train_samples/batch_size)):\n",
    "        for x_v, y_v in validation_ds:\n",
    "            y_pred = big_mdl(x_v)\n",
    "            validation_loss = tf.reduce_mean(tf.square(y_v-y_pred))\n",
    "        print(\"Epoch: {} Train loss: {:.5} Validation loss: {:.5}\".format(epoch, train_loss/train_iters, validation_loss))\n",
    "        train_iters = 0\n",
    "        train_loss = 0.0\n",
    "        epoch += 1\n",
    "    if (epoch == N_epochs):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gkaUDmA8Ps6w"
   },
   "source": [
    "Predicting with this model shows overfitting. For recognizing overfitting a comparison of the validation and training loss is very useful. If the training loss decreases during training while the validation loss consistently increases, the model you are training is probably overfitting. Plotting the models prediction and the target also shows that there is a significant discrepancy between the target and the prediction of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 714163,
     "status": "ok",
     "timestamp": 1562073999608,
     "user": {
      "displayName": "Felix Wiewel",
      "photoUrl": "https://lh3.googleusercontent.com/-XBgpqQQzeMU/AAAAAAAAAAI/AAAAAAAAAZY/12qzl-EFXfM/s64/photo.jpg",
      "userId": "11509421729312238102"
     },
     "user_tz": -120
    },
    "id": "3Sq-9xhWPxI4",
    "outputId": "fa8abb3f-11ed-4623-8f8f-3f546ced3855"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM7MC+h9bjXZmwlrU8QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = big_mdl(x)\n",
    "plt.scatter(x_train_overfit, y_train_overfit)\n",
    "plt.plot(x, y_true)\n",
    "plt.plot(x, y_pred.numpy())\n",
    "plt.legend([\"Target\", \"Prediction\", \"Training samples\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZ36J2EN85b9"
   },
   "source": [
    "In order to implement a regularization we need to modify the loss function. Since the loss function in this exercise is computed during the training step, we define a new training step with a regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sLbcWwlt9Jwl"
   },
   "outputs": [],
   "source": [
    "def regularized_train_step(model, optimizer, x, y, lmbd):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        y_pred = model(x)\n",
    "        loss_val = tf.reduce_mean(tf.square(y-y_pred))\n",
    "        regul_val = tf.reduce_sum([tf.reduce_sum(tf.square(w)) for w in model.trainable_variables])\n",
    "        total_loss = tf.add(loss_val, lmbd*regul_val)\n",
    "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ExK4FIw89s0M"
   },
   "source": [
    "We can now set the strength of the regularization and retrain the big model with a regularization. We create another instance of the big model in order to compare the big model with and without regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1085821,
     "status": "ok",
     "timestamp": 1562074371290,
     "user": {
      "displayName": "Felix Wiewel",
      "photoUrl": "https://lh3.googleusercontent.com/-XBgpqQQzeMU/AAAAAAAAAAI/AAAAAAAAAZY/12qzl-EFXfM/s64/photo.jpg",
      "userId": "11509421729312238102"
     },
     "user_tz": -120
    },
    "id": "_PUUBGi496Vt",
    "outputId": "11e6637c-f0a9-42f5-ff57-74f248ec1e16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train loss: 1.0037 Validation loss: 0.13126\n",
      "Epoch: 1 Train loss: 0.27035 Validation loss: 0.065601\n",
      "Epoch: 2 Train loss: 0.20512 Validation loss: 0.045645\n",
      "Epoch: 3 Train loss: 0.1552 Validation loss: 0.05122\n",
      "Epoch: 4 Train loss: 0.12476 Validation loss: 0.065461\n",
      "Epoch: 5 Train loss: 0.13 Validation loss: 0.21363\n",
      "Epoch: 6 Train loss: 0.29464 Validation loss: 0.14423\n",
      "Epoch: 7 Train loss: 0.13695 Validation loss: 0.077842\n",
      "Epoch: 8 Train loss: 0.1022 Validation loss: 0.057304\n",
      "Epoch: 9 Train loss: 0.095723 Validation loss: 0.050884\n",
      "Epoch: 10 Train loss: 0.093442 Validation loss: 0.048443\n",
      "Epoch: 11 Train loss: 0.09161 Validation loss: 0.046768\n",
      "Epoch: 12 Train loss: 0.089949 Validation loss: 0.04528\n",
      "Epoch: 13 Train loss: 0.088426 Validation loss: 0.043942\n",
      "Epoch: 14 Train loss: 0.087021 Validation loss: 0.042761\n",
      "Epoch: 15 Train loss: 0.085719 Validation loss: 0.041726\n",
      "Epoch: 16 Train loss: 0.084506 Validation loss: 0.040821\n",
      "Epoch: 17 Train loss: 0.083372 Validation loss: 0.04003\n",
      "Epoch: 18 Train loss: 0.082309 Validation loss: 0.039335\n",
      "Epoch: 19 Train loss: 0.08131 Validation loss: 0.03872\n",
      "Epoch: 20 Train loss: 0.080369 Validation loss: 0.038172\n",
      "Epoch: 21 Train loss: 0.079482 Validation loss: 0.037677\n",
      "Epoch: 22 Train loss: 0.078645 Validation loss: 0.037228\n",
      "Epoch: 23 Train loss: 0.077856 Validation loss: 0.036813\n",
      "Epoch: 24 Train loss: 0.077112 Validation loss: 0.036426\n",
      "Epoch: 25 Train loss: 0.07641 Validation loss: 0.036063\n",
      "Epoch: 26 Train loss: 0.075749 Validation loss: 0.035717\n",
      "Epoch: 27 Train loss: 0.075126 Validation loss: 0.035385\n",
      "Epoch: 28 Train loss: 0.074537 Validation loss: 0.035065\n",
      "Epoch: 29 Train loss: 0.073982 Validation loss: 0.034755\n",
      "Epoch: 30 Train loss: 0.073458 Validation loss: 0.034455\n",
      "Epoch: 31 Train loss: 0.072963 Validation loss: 0.034164\n",
      "Epoch: 32 Train loss: 0.072493 Validation loss: 0.033882\n",
      "Epoch: 33 Train loss: 0.072048 Validation loss: 0.03361\n",
      "Epoch: 34 Train loss: 0.071626 Validation loss: 0.033348\n",
      "Epoch: 35 Train loss: 0.071223 Validation loss: 0.033096\n",
      "Epoch: 36 Train loss: 0.07084 Validation loss: 0.032855\n",
      "Epoch: 37 Train loss: 0.070474 Validation loss: 0.032625\n",
      "Epoch: 38 Train loss: 0.070124 Validation loss: 0.032408\n",
      "Epoch: 39 Train loss: 0.069788 Validation loss: 0.032202\n",
      "Epoch: 40 Train loss: 0.069466 Validation loss: 0.032009\n",
      "Epoch: 41 Train loss: 0.069156 Validation loss: 0.031829\n",
      "Epoch: 42 Train loss: 0.068858 Validation loss: 0.03166\n",
      "Epoch: 43 Train loss: 0.06857 Validation loss: 0.031503\n",
      "Epoch: 44 Train loss: 0.068292 Validation loss: 0.031358\n",
      "Epoch: 45 Train loss: 0.068024 Validation loss: 0.031224\n",
      "Epoch: 46 Train loss: 0.067763 Validation loss: 0.031101\n",
      "Epoch: 47 Train loss: 0.067511 Validation loss: 0.030989\n",
      "Epoch: 48 Train loss: 0.067266 Validation loss: 0.030887\n",
      "Epoch: 49 Train loss: 0.067027 Validation loss: 0.030793\n",
      "Epoch: 50 Train loss: 0.066795 Validation loss: 0.030709\n",
      "Epoch: 51 Train loss: 0.066569 Validation loss: 0.030634\n",
      "Epoch: 52 Train loss: 0.066349 Validation loss: 0.030566\n",
      "Epoch: 53 Train loss: 0.066134 Validation loss: 0.030505\n",
      "Epoch: 54 Train loss: 0.065924 Validation loss: 0.030452\n",
      "Epoch: 55 Train loss: 0.065718 Validation loss: 0.030405\n",
      "Epoch: 56 Train loss: 0.065517 Validation loss: 0.030363\n",
      "Epoch: 57 Train loss: 0.06532 Validation loss: 0.030328\n",
      "Epoch: 58 Train loss: 0.065127 Validation loss: 0.030297\n",
      "Epoch: 59 Train loss: 0.064938 Validation loss: 0.030271\n",
      "Epoch: 60 Train loss: 0.064752 Validation loss: 0.03025\n",
      "Epoch: 61 Train loss: 0.064569 Validation loss: 0.030232\n",
      "Epoch: 62 Train loss: 0.06439 Validation loss: 0.030218\n",
      "Epoch: 63 Train loss: 0.064213 Validation loss: 0.030208\n",
      "Epoch: 64 Train loss: 0.064039 Validation loss: 0.0302\n",
      "Epoch: 65 Train loss: 0.063868 Validation loss: 0.030196\n",
      "Epoch: 66 Train loss: 0.063699 Validation loss: 0.030194\n",
      "Epoch: 67 Train loss: 0.063532 Validation loss: 0.030195\n",
      "Epoch: 68 Train loss: 0.063368 Validation loss: 0.030197\n",
      "Epoch: 69 Train loss: 0.063206 Validation loss: 0.030203\n",
      "Epoch: 70 Train loss: 0.063046 Validation loss: 0.030209\n",
      "Epoch: 71 Train loss: 0.062887 Validation loss: 0.030218\n",
      "Epoch: 72 Train loss: 0.062731 Validation loss: 0.030228\n",
      "Epoch: 73 Train loss: 0.062576 Validation loss: 0.03024\n",
      "Epoch: 74 Train loss: 0.062422 Validation loss: 0.030254\n",
      "Epoch: 75 Train loss: 0.062271 Validation loss: 0.030269\n",
      "Epoch: 76 Train loss: 0.06212 Validation loss: 0.030285\n",
      "Epoch: 77 Train loss: 0.061971 Validation loss: 0.030303\n",
      "Epoch: 78 Train loss: 0.061824 Validation loss: 0.030322\n",
      "Epoch: 79 Train loss: 0.061677 Validation loss: 0.030342\n",
      "Epoch: 80 Train loss: 0.061532 Validation loss: 0.030363\n",
      "Epoch: 81 Train loss: 0.061388 Validation loss: 0.030386\n",
      "Epoch: 82 Train loss: 0.061244 Validation loss: 0.03041\n",
      "Epoch: 83 Train loss: 0.061102 Validation loss: 0.030435\n",
      "Epoch: 84 Train loss: 0.060961 Validation loss: 0.030461\n",
      "Epoch: 85 Train loss: 0.060821 Validation loss: 0.030487\n",
      "Epoch: 86 Train loss: 0.060681 Validation loss: 0.030516\n",
      "Epoch: 87 Train loss: 0.060542 Validation loss: 0.030545\n",
      "Epoch: 88 Train loss: 0.060404 Validation loss: 0.030576\n",
      "Epoch: 89 Train loss: 0.060267 Validation loss: 0.030607\n",
      "Epoch: 90 Train loss: 0.06013 Validation loss: 0.030639\n",
      "Epoch: 91 Train loss: 0.059993 Validation loss: 0.030673\n",
      "Epoch: 92 Train loss: 0.059858 Validation loss: 0.030707\n",
      "Epoch: 93 Train loss: 0.059722 Validation loss: 0.030743\n",
      "Epoch: 94 Train loss: 0.059588 Validation loss: 0.030779\n",
      "Epoch: 95 Train loss: 0.059453 Validation loss: 0.030817\n",
      "Epoch: 96 Train loss: 0.059319 Validation loss: 0.030855\n",
      "Epoch: 97 Train loss: 0.059185 Validation loss: 0.030895\n",
      "Epoch: 98 Train loss: 0.059051 Validation loss: 0.030936\n",
      "Epoch: 99 Train loss: 0.058918 Validation loss: 0.030978\n",
      "Epoch: 100 Train loss: 0.058784 Validation loss: 0.03102\n",
      "Epoch: 101 Train loss: 0.058651 Validation loss: 0.031063\n",
      "Epoch: 102 Train loss: 0.058517 Validation loss: 0.031108\n",
      "Epoch: 103 Train loss: 0.058384 Validation loss: 0.031153\n",
      "Epoch: 104 Train loss: 0.05825 Validation loss: 0.031199\n",
      "Epoch: 105 Train loss: 0.058116 Validation loss: 0.031246\n",
      "Epoch: 106 Train loss: 0.057982 Validation loss: 0.031294\n",
      "Epoch: 107 Train loss: 0.057848 Validation loss: 0.031343\n",
      "Epoch: 108 Train loss: 0.057713 Validation loss: 0.031393\n",
      "Epoch: 109 Train loss: 0.057578 Validation loss: 0.031443\n",
      "Epoch: 110 Train loss: 0.057442 Validation loss: 0.031495\n",
      "Epoch: 111 Train loss: 0.057306 Validation loss: 0.031548\n",
      "Epoch: 112 Train loss: 0.057169 Validation loss: 0.031602\n",
      "Epoch: 113 Train loss: 0.057032 Validation loss: 0.031658\n",
      "Epoch: 114 Train loss: 0.056894 Validation loss: 0.031715\n",
      "Epoch: 115 Train loss: 0.056756 Validation loss: 0.031774\n",
      "Epoch: 116 Train loss: 0.056618 Validation loss: 0.031834\n",
      "Epoch: 117 Train loss: 0.056479 Validation loss: 0.031896\n",
      "Epoch: 118 Train loss: 0.05634 Validation loss: 0.031961\n",
      "Epoch: 119 Train loss: 0.0562 Validation loss: 0.032028\n",
      "Epoch: 120 Train loss: 0.056061 Validation loss: 0.032099\n",
      "Epoch: 121 Train loss: 0.055922 Validation loss: 0.032172\n",
      "Epoch: 122 Train loss: 0.055783 Validation loss: 0.032248\n",
      "Epoch: 123 Train loss: 0.055645 Validation loss: 0.032328\n",
      "Epoch: 124 Train loss: 0.055508 Validation loss: 0.032411\n",
      "Epoch: 125 Train loss: 0.055371 Validation loss: 0.032498\n",
      "Epoch: 126 Train loss: 0.055235 Validation loss: 0.032589\n",
      "Epoch: 127 Train loss: 0.0551 Validation loss: 0.032683\n",
      "Epoch: 128 Train loss: 0.054967 Validation loss: 0.032781\n",
      "Epoch: 129 Train loss: 0.054834 Validation loss: 0.032883\n",
      "Epoch: 130 Train loss: 0.054703 Validation loss: 0.032988\n",
      "Epoch: 131 Train loss: 0.054574 Validation loss: 0.033097\n",
      "Epoch: 132 Train loss: 0.054445 Validation loss: 0.033208\n",
      "Epoch: 133 Train loss: 0.054318 Validation loss: 0.033323\n",
      "Epoch: 134 Train loss: 0.054193 Validation loss: 0.03344\n",
      "Epoch: 135 Train loss: 0.054068 Validation loss: 0.033559\n",
      "Epoch: 136 Train loss: 0.053945 Validation loss: 0.033681\n",
      "Epoch: 137 Train loss: 0.053823 Validation loss: 0.033804\n",
      "Epoch: 138 Train loss: 0.053702 Validation loss: 0.03393\n",
      "Epoch: 139 Train loss: 0.053582 Validation loss: 0.034056\n",
      "Epoch: 140 Train loss: 0.053464 Validation loss: 0.034185\n",
      "Epoch: 141 Train loss: 0.053346 Validation loss: 0.034314\n",
      "Epoch: 142 Train loss: 0.053229 Validation loss: 0.034444\n",
      "Epoch: 143 Train loss: 0.053112 Validation loss: 0.034574\n",
      "Epoch: 144 Train loss: 0.052997 Validation loss: 0.034705\n",
      "Epoch: 145 Train loss: 0.052882 Validation loss: 0.034837\n",
      "Epoch: 146 Train loss: 0.052768 Validation loss: 0.034968\n",
      "Epoch: 147 Train loss: 0.052655 Validation loss: 0.0351\n",
      "Epoch: 148 Train loss: 0.052542 Validation loss: 0.035232\n",
      "Epoch: 149 Train loss: 0.05243 Validation loss: 0.035363\n",
      "Epoch: 150 Train loss: 0.052319 Validation loss: 0.035494\n",
      "Epoch: 151 Train loss: 0.052208 Validation loss: 0.035625\n",
      "Epoch: 152 Train loss: 0.052097 Validation loss: 0.035755\n",
      "Epoch: 153 Train loss: 0.051987 Validation loss: 0.035884\n",
      "Epoch: 154 Train loss: 0.051877 Validation loss: 0.036013\n",
      "Epoch: 155 Train loss: 0.051768 Validation loss: 0.03614\n",
      "Epoch: 156 Train loss: 0.051659 Validation loss: 0.036267\n",
      "Epoch: 157 Train loss: 0.051551 Validation loss: 0.036392\n",
      "Epoch: 158 Train loss: 0.051443 Validation loss: 0.036516\n",
      "Epoch: 159 Train loss: 0.051335 Validation loss: 0.03664\n",
      "Epoch: 160 Train loss: 0.051228 Validation loss: 0.036761\n",
      "Epoch: 161 Train loss: 0.051121 Validation loss: 0.036882\n",
      "Epoch: 162 Train loss: 0.051014 Validation loss: 0.037001\n",
      "Epoch: 163 Train loss: 0.050908 Validation loss: 0.037118\n",
      "Epoch: 164 Train loss: 0.050802 Validation loss: 0.037233\n",
      "Epoch: 165 Train loss: 0.050696 Validation loss: 0.037346\n",
      "Epoch: 166 Train loss: 0.050591 Validation loss: 0.037458\n",
      "Epoch: 167 Train loss: 0.050486 Validation loss: 0.037567\n",
      "Epoch: 168 Train loss: 0.050381 Validation loss: 0.037675\n",
      "Epoch: 169 Train loss: 0.050276 Validation loss: 0.03778\n",
      "Epoch: 170 Train loss: 0.050172 Validation loss: 0.037883\n",
      "Epoch: 171 Train loss: 0.050068 Validation loss: 0.037983\n",
      "Epoch: 172 Train loss: 0.049964 Validation loss: 0.038081\n",
      "Epoch: 173 Train loss: 0.04986 Validation loss: 0.038177\n",
      "Epoch: 174 Train loss: 0.049757 Validation loss: 0.03827\n",
      "Epoch: 175 Train loss: 0.049653 Validation loss: 0.038359\n",
      "Epoch: 176 Train loss: 0.04955 Validation loss: 0.038447\n",
      "Epoch: 177 Train loss: 0.049448 Validation loss: 0.038532\n",
      "Epoch: 178 Train loss: 0.049345 Validation loss: 0.038613\n",
      "Epoch: 179 Train loss: 0.049243 Validation loss: 0.038692\n",
      "Epoch: 180 Train loss: 0.049141 Validation loss: 0.038767\n",
      "Epoch: 181 Train loss: 0.049039 Validation loss: 0.03884\n",
      "Epoch: 182 Train loss: 0.048938 Validation loss: 0.038909\n",
      "Epoch: 183 Train loss: 0.048836 Validation loss: 0.038974\n",
      "Epoch: 184 Train loss: 0.048735 Validation loss: 0.039037\n",
      "Epoch: 185 Train loss: 0.048634 Validation loss: 0.039096\n",
      "Epoch: 186 Train loss: 0.048534 Validation loss: 0.039151\n",
      "Epoch: 187 Train loss: 0.048433 Validation loss: 0.039203\n",
      "Epoch: 188 Train loss: 0.048333 Validation loss: 0.039251\n",
      "Epoch: 189 Train loss: 0.048234 Validation loss: 0.039296\n",
      "Epoch: 190 Train loss: 0.048134 Validation loss: 0.039337\n",
      "Epoch: 191 Train loss: 0.048035 Validation loss: 0.039374\n",
      "Epoch: 192 Train loss: 0.047936 Validation loss: 0.039407\n",
      "Epoch: 193 Train loss: 0.047837 Validation loss: 0.039437\n",
      "Epoch: 194 Train loss: 0.047739 Validation loss: 0.039463\n",
      "Epoch: 195 Train loss: 0.04764 Validation loss: 0.039485\n",
      "Epoch: 196 Train loss: 0.047543 Validation loss: 0.039503\n",
      "Epoch: 197 Train loss: 0.047445 Validation loss: 0.039517\n",
      "Epoch: 198 Train loss: 0.047348 Validation loss: 0.039528\n",
      "Epoch: 199 Train loss: 0.047251 Validation loss: 0.039535\n",
      "Epoch: 200 Train loss: 0.047155 Validation loss: 0.039538\n",
      "Epoch: 201 Train loss: 0.047058 Validation loss: 0.039537\n",
      "Epoch: 202 Train loss: 0.046962 Validation loss: 0.039533\n",
      "Epoch: 203 Train loss: 0.046867 Validation loss: 0.039525\n",
      "Epoch: 204 Train loss: 0.046772 Validation loss: 0.039513\n",
      "Epoch: 205 Train loss: 0.046677 Validation loss: 0.039498\n",
      "Epoch: 206 Train loss: 0.046583 Validation loss: 0.039479\n",
      "Epoch: 207 Train loss: 0.046489 Validation loss: 0.039457\n",
      "Epoch: 208 Train loss: 0.046395 Validation loss: 0.039431\n",
      "Epoch: 209 Train loss: 0.046302 Validation loss: 0.039403\n",
      "Epoch: 210 Train loss: 0.046209 Validation loss: 0.039371\n",
      "Epoch: 211 Train loss: 0.046117 Validation loss: 0.039336\n",
      "Epoch: 212 Train loss: 0.046025 Validation loss: 0.039299\n",
      "Epoch: 213 Train loss: 0.045933 Validation loss: 0.039258\n",
      "Epoch: 214 Train loss: 0.045842 Validation loss: 0.039214\n",
      "Epoch: 215 Train loss: 0.045752 Validation loss: 0.039168\n",
      "Epoch: 216 Train loss: 0.045662 Validation loss: 0.03912\n",
      "Epoch: 217 Train loss: 0.045572 Validation loss: 0.039068\n",
      "Epoch: 218 Train loss: 0.045483 Validation loss: 0.039015\n",
      "Epoch: 219 Train loss: 0.045394 Validation loss: 0.038959\n",
      "Epoch: 220 Train loss: 0.045306 Validation loss: 0.038902\n",
      "Epoch: 221 Train loss: 0.045219 Validation loss: 0.038842\n",
      "Epoch: 222 Train loss: 0.045131 Validation loss: 0.038781\n",
      "Epoch: 223 Train loss: 0.045045 Validation loss: 0.038718\n",
      "Epoch: 224 Train loss: 0.044959 Validation loss: 0.038652\n",
      "Epoch: 225 Train loss: 0.044874 Validation loss: 0.038586\n",
      "Epoch: 226 Train loss: 0.044789 Validation loss: 0.038518\n",
      "Epoch: 227 Train loss: 0.044704 Validation loss: 0.038449\n",
      "Epoch: 228 Train loss: 0.044621 Validation loss: 0.038378\n",
      "Epoch: 229 Train loss: 0.044538 Validation loss: 0.038307\n",
      "Epoch: 230 Train loss: 0.044455 Validation loss: 0.038234\n",
      "Epoch: 231 Train loss: 0.044373 Validation loss: 0.038161\n",
      "Epoch: 232 Train loss: 0.044292 Validation loss: 0.038087\n",
      "Epoch: 233 Train loss: 0.044212 Validation loss: 0.038012\n",
      "Epoch: 234 Train loss: 0.044132 Validation loss: 0.037937\n",
      "Epoch: 235 Train loss: 0.044052 Validation loss: 0.037861\n",
      "Epoch: 236 Train loss: 0.043974 Validation loss: 0.037784\n",
      "Epoch: 237 Train loss: 0.043895 Validation loss: 0.037708\n",
      "Epoch: 238 Train loss: 0.043818 Validation loss: 0.037631\n",
      "Epoch: 239 Train loss: 0.043741 Validation loss: 0.037554\n",
      "Epoch: 240 Train loss: 0.043665 Validation loss: 0.037477\n",
      "Epoch: 241 Train loss: 0.04359 Validation loss: 0.037399\n",
      "Epoch: 242 Train loss: 0.043515 Validation loss: 0.037322\n",
      "Epoch: 243 Train loss: 0.043441 Validation loss: 0.037246\n",
      "Epoch: 244 Train loss: 0.043367 Validation loss: 0.037169\n",
      "Epoch: 245 Train loss: 0.043294 Validation loss: 0.037093\n",
      "Epoch: 246 Train loss: 0.043222 Validation loss: 0.037017\n",
      "Epoch: 247 Train loss: 0.04315 Validation loss: 0.036941\n",
      "Epoch: 248 Train loss: 0.043079 Validation loss: 0.036867\n",
      "Epoch: 249 Train loss: 0.043008 Validation loss: 0.036792\n",
      "Epoch: 250 Train loss: 0.042939 Validation loss: 0.036719\n",
      "Epoch: 251 Train loss: 0.042869 Validation loss: 0.036646\n",
      "Epoch: 252 Train loss: 0.042801 Validation loss: 0.036574\n",
      "Epoch: 253 Train loss: 0.042733 Validation loss: 0.036502\n",
      "Epoch: 254 Train loss: 0.042665 Validation loss: 0.036432\n",
      "Epoch: 255 Train loss: 0.042598 Validation loss: 0.036363\n",
      "Epoch: 256 Train loss: 0.042531 Validation loss: 0.036295\n",
      "Epoch: 257 Train loss: 0.042466 Validation loss: 0.036227\n",
      "Epoch: 258 Train loss: 0.0424 Validation loss: 0.036161\n",
      "Epoch: 259 Train loss: 0.042335 Validation loss: 0.036096\n",
      "Epoch: 260 Train loss: 0.042271 Validation loss: 0.036033\n",
      "Epoch: 261 Train loss: 0.042207 Validation loss: 0.03597\n",
      "Epoch: 262 Train loss: 0.042143 Validation loss: 0.035909\n",
      "Epoch: 263 Train loss: 0.04208 Validation loss: 0.035849\n",
      "Epoch: 264 Train loss: 0.042018 Validation loss: 0.035791\n",
      "Epoch: 265 Train loss: 0.041956 Validation loss: 0.035734\n",
      "Epoch: 266 Train loss: 0.041894 Validation loss: 0.035679\n",
      "Epoch: 267 Train loss: 0.041833 Validation loss: 0.035625\n",
      "Epoch: 268 Train loss: 0.041772 Validation loss: 0.035572\n",
      "Epoch: 269 Train loss: 0.041712 Validation loss: 0.035522\n",
      "Epoch: 270 Train loss: 0.041652 Validation loss: 0.035472\n",
      "Epoch: 271 Train loss: 0.041592 Validation loss: 0.035424\n",
      "Epoch: 272 Train loss: 0.041533 Validation loss: 0.035378\n",
      "Epoch: 273 Train loss: 0.041474 Validation loss: 0.035333\n",
      "Epoch: 274 Train loss: 0.041415 Validation loss: 0.03529\n",
      "Epoch: 275 Train loss: 0.041357 Validation loss: 0.035248\n",
      "Epoch: 276 Train loss: 0.0413 Validation loss: 0.035207\n",
      "Epoch: 277 Train loss: 0.041242 Validation loss: 0.035168\n",
      "Epoch: 278 Train loss: 0.041185 Validation loss: 0.03513\n",
      "Epoch: 279 Train loss: 0.041128 Validation loss: 0.035093\n",
      "Epoch: 280 Train loss: 0.041072 Validation loss: 0.035058\n",
      "Epoch: 281 Train loss: 0.041016 Validation loss: 0.035024\n",
      "Epoch: 282 Train loss: 0.04096 Validation loss: 0.03499\n",
      "Epoch: 283 Train loss: 0.040905 Validation loss: 0.034957\n",
      "Epoch: 284 Train loss: 0.04085 Validation loss: 0.034925\n",
      "Epoch: 285 Train loss: 0.040796 Validation loss: 0.034894\n",
      "Epoch: 286 Train loss: 0.040742 Validation loss: 0.034863\n",
      "Epoch: 287 Train loss: 0.040688 Validation loss: 0.034832\n",
      "Epoch: 288 Train loss: 0.040635 Validation loss: 0.034801\n",
      "Epoch: 289 Train loss: 0.040582 Validation loss: 0.03477\n",
      "Epoch: 290 Train loss: 0.04053 Validation loss: 0.034738\n",
      "Epoch: 291 Train loss: 0.040478 Validation loss: 0.034706\n",
      "Epoch: 292 Train loss: 0.040427 Validation loss: 0.034673\n",
      "Epoch: 293 Train loss: 0.040376 Validation loss: 0.034639\n",
      "Epoch: 294 Train loss: 0.040326 Validation loss: 0.034603\n",
      "Epoch: 295 Train loss: 0.040276 Validation loss: 0.034567\n",
      "Epoch: 296 Train loss: 0.040226 Validation loss: 0.034528\n",
      "Epoch: 297 Train loss: 0.040178 Validation loss: 0.034488\n",
      "Epoch: 298 Train loss: 0.040129 Validation loss: 0.034446\n",
      "Epoch: 299 Train loss: 0.040082 Validation loss: 0.034401\n",
      "Epoch: 300 Train loss: 0.040035 Validation loss: 0.034354\n",
      "Epoch: 301 Train loss: 0.039988 Validation loss: 0.034304\n",
      "Epoch: 302 Train loss: 0.039942 Validation loss: 0.034251\n",
      "Epoch: 303 Train loss: 0.039897 Validation loss: 0.034196\n",
      "Epoch: 304 Train loss: 0.039852 Validation loss: 0.034138\n",
      "Epoch: 305 Train loss: 0.039808 Validation loss: 0.034076\n",
      "Epoch: 306 Train loss: 0.039765 Validation loss: 0.034011\n",
      "Epoch: 307 Train loss: 0.039722 Validation loss: 0.033944\n",
      "Epoch: 308 Train loss: 0.039679 Validation loss: 0.033873\n",
      "Epoch: 309 Train loss: 0.039637 Validation loss: 0.033798\n",
      "Epoch: 310 Train loss: 0.039596 Validation loss: 0.033721\n",
      "Epoch: 311 Train loss: 0.039555 Validation loss: 0.03364\n",
      "Epoch: 312 Train loss: 0.039515 Validation loss: 0.033557\n",
      "Epoch: 313 Train loss: 0.039475 Validation loss: 0.03347\n",
      "Epoch: 314 Train loss: 0.039436 Validation loss: 0.03338\n",
      "Epoch: 315 Train loss: 0.039398 Validation loss: 0.033288\n",
      "Epoch: 316 Train loss: 0.03936 Validation loss: 0.033193\n",
      "Epoch: 317 Train loss: 0.039322 Validation loss: 0.033095\n",
      "Epoch: 318 Train loss: 0.039285 Validation loss: 0.032995\n",
      "Epoch: 319 Train loss: 0.039248 Validation loss: 0.032892\n",
      "Epoch: 320 Train loss: 0.039212 Validation loss: 0.032788\n",
      "Epoch: 321 Train loss: 0.039176 Validation loss: 0.032681\n",
      "Epoch: 322 Train loss: 0.039141 Validation loss: 0.032572\n",
      "Epoch: 323 Train loss: 0.039106 Validation loss: 0.032462\n",
      "Epoch: 324 Train loss: 0.039072 Validation loss: 0.03235\n",
      "Epoch: 325 Train loss: 0.039038 Validation loss: 0.032237\n",
      "Epoch: 326 Train loss: 0.039004 Validation loss: 0.032122\n",
      "Epoch: 327 Train loss: 0.038971 Validation loss: 0.032006\n",
      "Epoch: 328 Train loss: 0.038938 Validation loss: 0.031889\n",
      "Epoch: 329 Train loss: 0.038906 Validation loss: 0.031772\n",
      "Epoch: 330 Train loss: 0.038874 Validation loss: 0.031653\n",
      "Epoch: 331 Train loss: 0.038843 Validation loss: 0.031534\n",
      "Epoch: 332 Train loss: 0.038812 Validation loss: 0.031415\n",
      "Epoch: 333 Train loss: 0.038781 Validation loss: 0.031295\n",
      "Epoch: 334 Train loss: 0.038751 Validation loss: 0.031174\n",
      "Epoch: 335 Train loss: 0.038721 Validation loss: 0.031054\n",
      "Epoch: 336 Train loss: 0.038692 Validation loss: 0.030933\n",
      "Epoch: 337 Train loss: 0.038663 Validation loss: 0.030813\n",
      "Epoch: 338 Train loss: 0.038634 Validation loss: 0.030692\n",
      "Epoch: 339 Train loss: 0.038606 Validation loss: 0.030571\n",
      "Epoch: 340 Train loss: 0.038578 Validation loss: 0.030451\n",
      "Epoch: 341 Train loss: 0.038551 Validation loss: 0.030331\n",
      "Epoch: 342 Train loss: 0.038524 Validation loss: 0.030212\n",
      "Epoch: 343 Train loss: 0.038497 Validation loss: 0.030092\n",
      "Epoch: 344 Train loss: 0.038471 Validation loss: 0.029974\n",
      "Epoch: 345 Train loss: 0.038445 Validation loss: 0.029855\n",
      "Epoch: 346 Train loss: 0.038419 Validation loss: 0.029737\n",
      "Epoch: 347 Train loss: 0.038394 Validation loss: 0.02962\n",
      "Epoch: 348 Train loss: 0.038369 Validation loss: 0.029503\n",
      "Epoch: 349 Train loss: 0.038345 Validation loss: 0.029388\n",
      "Epoch: 350 Train loss: 0.038321 Validation loss: 0.029272\n",
      "Epoch: 351 Train loss: 0.038298 Validation loss: 0.029158\n",
      "Epoch: 352 Train loss: 0.038275 Validation loss: 0.029044\n",
      "Epoch: 353 Train loss: 0.038252 Validation loss: 0.028931\n",
      "Epoch: 354 Train loss: 0.038229 Validation loss: 0.028818\n",
      "Epoch: 355 Train loss: 0.038207 Validation loss: 0.028706\n",
      "Epoch: 356 Train loss: 0.038186 Validation loss: 0.028595\n",
      "Epoch: 357 Train loss: 0.038165 Validation loss: 0.028486\n",
      "Epoch: 358 Train loss: 0.038144 Validation loss: 0.028376\n",
      "Epoch: 359 Train loss: 0.038123 Validation loss: 0.028268\n",
      "Epoch: 360 Train loss: 0.038103 Validation loss: 0.02816\n",
      "Epoch: 361 Train loss: 0.038084 Validation loss: 0.028053\n",
      "Epoch: 362 Train loss: 0.038065 Validation loss: 0.027947\n",
      "Epoch: 363 Train loss: 0.038046 Validation loss: 0.027842\n",
      "Epoch: 364 Train loss: 0.038027 Validation loss: 0.027738\n",
      "Epoch: 365 Train loss: 0.038009 Validation loss: 0.027634\n",
      "Epoch: 366 Train loss: 0.037991 Validation loss: 0.027532\n",
      "Epoch: 367 Train loss: 0.037974 Validation loss: 0.02743\n",
      "Epoch: 368 Train loss: 0.037957 Validation loss: 0.027328\n",
      "Epoch: 369 Train loss: 0.03794 Validation loss: 0.027228\n",
      "Epoch: 370 Train loss: 0.037924 Validation loss: 0.027128\n",
      "Epoch: 371 Train loss: 0.037908 Validation loss: 0.02703\n",
      "Epoch: 372 Train loss: 0.037892 Validation loss: 0.026931\n",
      "Epoch: 373 Train loss: 0.037877 Validation loss: 0.026834\n",
      "Epoch: 374 Train loss: 0.037862 Validation loss: 0.026738\n",
      "Epoch: 375 Train loss: 0.037848 Validation loss: 0.026642\n",
      "Epoch: 376 Train loss: 0.037833 Validation loss: 0.026547\n",
      "Epoch: 377 Train loss: 0.037819 Validation loss: 0.026452\n",
      "Epoch: 378 Train loss: 0.037806 Validation loss: 0.026358\n",
      "Epoch: 379 Train loss: 0.037792 Validation loss: 0.026265\n",
      "Epoch: 380 Train loss: 0.037779 Validation loss: 0.026173\n",
      "Epoch: 381 Train loss: 0.037767 Validation loss: 0.026081\n",
      "Epoch: 382 Train loss: 0.037754 Validation loss: 0.02599\n",
      "Epoch: 383 Train loss: 0.037742 Validation loss: 0.0259\n",
      "Epoch: 384 Train loss: 0.03773 Validation loss: 0.02581\n",
      "Epoch: 385 Train loss: 0.037719 Validation loss: 0.025721\n",
      "Epoch: 386 Train loss: 0.037707 Validation loss: 0.025632\n",
      "Epoch: 387 Train loss: 0.037696 Validation loss: 0.025544\n",
      "Epoch: 388 Train loss: 0.037685 Validation loss: 0.025457\n",
      "Epoch: 389 Train loss: 0.037675 Validation loss: 0.02537\n",
      "Epoch: 390 Train loss: 0.037664 Validation loss: 0.025283\n",
      "Epoch: 391 Train loss: 0.037654 Validation loss: 0.025197\n",
      "Epoch: 392 Train loss: 0.037644 Validation loss: 0.025112\n",
      "Epoch: 393 Train loss: 0.037635 Validation loss: 0.025027\n",
      "Epoch: 394 Train loss: 0.037625 Validation loss: 0.024943\n",
      "Epoch: 395 Train loss: 0.037616 Validation loss: 0.02486\n",
      "Epoch: 396 Train loss: 0.037607 Validation loss: 0.024776\n",
      "Epoch: 397 Train loss: 0.037598 Validation loss: 0.024694\n",
      "Epoch: 398 Train loss: 0.037589 Validation loss: 0.024612\n",
      "Epoch: 399 Train loss: 0.037581 Validation loss: 0.02453\n",
      "Epoch: 400 Train loss: 0.037572 Validation loss: 0.024449\n",
      "Epoch: 401 Train loss: 0.037564 Validation loss: 0.024368\n",
      "Epoch: 402 Train loss: 0.037556 Validation loss: 0.024288\n",
      "Epoch: 403 Train loss: 0.037548 Validation loss: 0.024208\n",
      "Epoch: 404 Train loss: 0.037541 Validation loss: 0.024129\n",
      "Epoch: 405 Train loss: 0.037533 Validation loss: 0.02405\n",
      "Epoch: 406 Train loss: 0.037525 Validation loss: 0.023971\n",
      "Epoch: 407 Train loss: 0.037518 Validation loss: 0.023893\n",
      "Epoch: 408 Train loss: 0.037511 Validation loss: 0.023816\n",
      "Epoch: 409 Train loss: 0.037504 Validation loss: 0.023739\n",
      "Epoch: 410 Train loss: 0.037497 Validation loss: 0.023662\n",
      "Epoch: 411 Train loss: 0.03749 Validation loss: 0.023586\n",
      "Epoch: 412 Train loss: 0.037483 Validation loss: 0.023511\n",
      "Epoch: 413 Train loss: 0.037477 Validation loss: 0.023436\n",
      "Epoch: 414 Train loss: 0.03747 Validation loss: 0.023361\n",
      "Epoch: 415 Train loss: 0.037464 Validation loss: 0.023286\n",
      "Epoch: 416 Train loss: 0.037457 Validation loss: 0.023213\n",
      "Epoch: 417 Train loss: 0.037451 Validation loss: 0.023139\n",
      "Epoch: 418 Train loss: 0.037445 Validation loss: 0.023067\n",
      "Epoch: 419 Train loss: 0.037439 Validation loss: 0.022994\n",
      "Epoch: 420 Train loss: 0.037433 Validation loss: 0.022922\n",
      "Epoch: 421 Train loss: 0.037427 Validation loss: 0.022851\n",
      "Epoch: 422 Train loss: 0.037421 Validation loss: 0.02278\n",
      "Epoch: 423 Train loss: 0.037415 Validation loss: 0.022709\n",
      "Epoch: 424 Train loss: 0.037409 Validation loss: 0.02264\n",
      "Epoch: 425 Train loss: 0.037403 Validation loss: 0.02257\n",
      "Epoch: 426 Train loss: 0.037397 Validation loss: 0.022501\n",
      "Epoch: 427 Train loss: 0.037392 Validation loss: 0.022432\n",
      "Epoch: 428 Train loss: 0.037386 Validation loss: 0.022364\n",
      "Epoch: 429 Train loss: 0.03738 Validation loss: 0.022297\n",
      "Epoch: 430 Train loss: 0.037375 Validation loss: 0.02223\n",
      "Epoch: 431 Train loss: 0.037369 Validation loss: 0.022163\n",
      "Epoch: 432 Train loss: 0.037363 Validation loss: 0.022097\n",
      "Epoch: 433 Train loss: 0.037358 Validation loss: 0.022032\n",
      "Epoch: 434 Train loss: 0.037352 Validation loss: 0.021967\n",
      "Epoch: 435 Train loss: 0.037347 Validation loss: 0.021902\n",
      "Epoch: 436 Train loss: 0.037341 Validation loss: 0.021838\n",
      "Epoch: 437 Train loss: 0.037336 Validation loss: 0.021775\n",
      "Epoch: 438 Train loss: 0.03733 Validation loss: 0.021712\n",
      "Epoch: 439 Train loss: 0.037325 Validation loss: 0.02165\n",
      "Epoch: 440 Train loss: 0.037319 Validation loss: 0.021588\n",
      "Epoch: 441 Train loss: 0.037313 Validation loss: 0.021527\n",
      "Epoch: 442 Train loss: 0.037308 Validation loss: 0.021466\n",
      "Epoch: 443 Train loss: 0.037302 Validation loss: 0.021406\n",
      "Epoch: 444 Train loss: 0.037297 Validation loss: 0.021346\n",
      "Epoch: 445 Train loss: 0.037291 Validation loss: 0.021287\n",
      "Epoch: 446 Train loss: 0.037285 Validation loss: 0.021228\n",
      "Epoch: 447 Train loss: 0.03728 Validation loss: 0.02117\n",
      "Epoch: 448 Train loss: 0.037274 Validation loss: 0.021113\n",
      "Epoch: 449 Train loss: 0.037268 Validation loss: 0.021056\n",
      "Epoch: 450 Train loss: 0.037263 Validation loss: 0.021\n",
      "Epoch: 451 Train loss: 0.037257 Validation loss: 0.020944\n",
      "Epoch: 452 Train loss: 0.037251 Validation loss: 0.020889\n",
      "Epoch: 453 Train loss: 0.037245 Validation loss: 0.020835\n",
      "Epoch: 454 Train loss: 0.03724 Validation loss: 0.02078\n",
      "Epoch: 455 Train loss: 0.037234 Validation loss: 0.020727\n",
      "Epoch: 456 Train loss: 0.037228 Validation loss: 0.020674\n",
      "Epoch: 457 Train loss: 0.037222 Validation loss: 0.020622\n",
      "Epoch: 458 Train loss: 0.037216 Validation loss: 0.02057\n",
      "Epoch: 459 Train loss: 0.03721 Validation loss: 0.020519\n",
      "Epoch: 460 Train loss: 0.037204 Validation loss: 0.020469\n",
      "Epoch: 461 Train loss: 0.037197 Validation loss: 0.020419\n",
      "Epoch: 462 Train loss: 0.037191 Validation loss: 0.02037\n",
      "Epoch: 463 Train loss: 0.037185 Validation loss: 0.020321\n",
      "Epoch: 464 Train loss: 0.037179 Validation loss: 0.020273\n",
      "Epoch: 465 Train loss: 0.037172 Validation loss: 0.020225\n",
      "Epoch: 466 Train loss: 0.037166 Validation loss: 0.020178\n",
      "Epoch: 467 Train loss: 0.03716 Validation loss: 0.020132\n",
      "Epoch: 468 Train loss: 0.037153 Validation loss: 0.020086\n",
      "Epoch: 469 Train loss: 0.037147 Validation loss: 0.020041\n",
      "Epoch: 470 Train loss: 0.03714 Validation loss: 0.019996\n",
      "Epoch: 471 Train loss: 0.037133 Validation loss: 0.019952\n",
      "Epoch: 472 Train loss: 0.037127 Validation loss: 0.019909\n",
      "Epoch: 473 Train loss: 0.03712 Validation loss: 0.019866\n",
      "Epoch: 474 Train loss: 0.037113 Validation loss: 0.019824\n",
      "Epoch: 475 Train loss: 0.037106 Validation loss: 0.019782\n",
      "Epoch: 476 Train loss: 0.0371 Validation loss: 0.019741\n",
      "Epoch: 477 Train loss: 0.037093 Validation loss: 0.019701\n",
      "Epoch: 478 Train loss: 0.037086 Validation loss: 0.01966\n",
      "Epoch: 479 Train loss: 0.037079 Validation loss: 0.019621\n",
      "Epoch: 480 Train loss: 0.037072 Validation loss: 0.019582\n",
      "Epoch: 481 Train loss: 0.037065 Validation loss: 0.019544\n",
      "Epoch: 482 Train loss: 0.037058 Validation loss: 0.019506\n",
      "Epoch: 483 Train loss: 0.037051 Validation loss: 0.019469\n",
      "Epoch: 484 Train loss: 0.037044 Validation loss: 0.019432\n",
      "Epoch: 485 Train loss: 0.037037 Validation loss: 0.019396\n",
      "Epoch: 486 Train loss: 0.037029 Validation loss: 0.01936\n",
      "Epoch: 487 Train loss: 0.037022 Validation loss: 0.019325\n",
      "Epoch: 488 Train loss: 0.037015 Validation loss: 0.01929\n",
      "Epoch: 489 Train loss: 0.037008 Validation loss: 0.019256\n",
      "Epoch: 490 Train loss: 0.037 Validation loss: 0.019222\n",
      "Epoch: 491 Train loss: 0.036993 Validation loss: 0.019189\n",
      "Epoch: 492 Train loss: 0.036986 Validation loss: 0.019156\n",
      "Epoch: 493 Train loss: 0.036979 Validation loss: 0.019124\n",
      "Epoch: 494 Train loss: 0.036971 Validation loss: 0.019092\n",
      "Epoch: 495 Train loss: 0.036964 Validation loss: 0.01906\n",
      "Epoch: 496 Train loss: 0.036957 Validation loss: 0.019029\n",
      "Epoch: 497 Train loss: 0.03695 Validation loss: 0.018999\n",
      "Epoch: 498 Train loss: 0.036942 Validation loss: 0.018968\n",
      "Epoch: 499 Train loss: 0.036935 Validation loss: 0.018938\n",
      "Epoch: 500 Train loss: 0.036928 Validation loss: 0.018909\n",
      "Epoch: 501 Train loss: 0.03692 Validation loss: 0.01888\n",
      "Epoch: 502 Train loss: 0.036913 Validation loss: 0.018851\n",
      "Epoch: 503 Train loss: 0.036906 Validation loss: 0.018823\n",
      "Epoch: 504 Train loss: 0.036899 Validation loss: 0.018795\n",
      "Epoch: 505 Train loss: 0.036892 Validation loss: 0.018767\n",
      "Epoch: 506 Train loss: 0.036884 Validation loss: 0.018739\n",
      "Epoch: 507 Train loss: 0.036877 Validation loss: 0.018712\n",
      "Epoch: 508 Train loss: 0.03687 Validation loss: 0.018685\n",
      "Epoch: 509 Train loss: 0.036863 Validation loss: 0.018659\n",
      "Epoch: 510 Train loss: 0.036856 Validation loss: 0.018633\n",
      "Epoch: 511 Train loss: 0.036849 Validation loss: 0.018607\n",
      "Epoch: 512 Train loss: 0.036842 Validation loss: 0.018581\n",
      "Epoch: 513 Train loss: 0.036835 Validation loss: 0.018555\n",
      "Epoch: 514 Train loss: 0.036828 Validation loss: 0.01853\n",
      "Epoch: 515 Train loss: 0.036821 Validation loss: 0.018505\n",
      "Epoch: 516 Train loss: 0.036814 Validation loss: 0.01848\n",
      "Epoch: 517 Train loss: 0.036808 Validation loss: 0.018456\n",
      "Epoch: 518 Train loss: 0.036801 Validation loss: 0.018431\n",
      "Epoch: 519 Train loss: 0.036794 Validation loss: 0.018407\n",
      "Epoch: 520 Train loss: 0.036787 Validation loss: 0.018383\n",
      "Epoch: 521 Train loss: 0.036781 Validation loss: 0.01836\n",
      "Epoch: 522 Train loss: 0.036774 Validation loss: 0.018336\n",
      "Epoch: 523 Train loss: 0.036768 Validation loss: 0.018313\n",
      "Epoch: 524 Train loss: 0.036761 Validation loss: 0.01829\n",
      "Epoch: 525 Train loss: 0.036755 Validation loss: 0.018267\n",
      "Epoch: 526 Train loss: 0.036748 Validation loss: 0.018244\n",
      "Epoch: 527 Train loss: 0.036742 Validation loss: 0.018221\n",
      "Epoch: 528 Train loss: 0.036736 Validation loss: 0.018199\n",
      "Epoch: 529 Train loss: 0.036729 Validation loss: 0.018176\n",
      "Epoch: 530 Train loss: 0.036723 Validation loss: 0.018154\n",
      "Epoch: 531 Train loss: 0.036717 Validation loss: 0.018132\n",
      "Epoch: 532 Train loss: 0.036711 Validation loss: 0.018111\n",
      "Epoch: 533 Train loss: 0.036705 Validation loss: 0.018089\n",
      "Epoch: 534 Train loss: 0.036699 Validation loss: 0.018068\n",
      "Epoch: 535 Train loss: 0.036693 Validation loss: 0.018047\n",
      "Epoch: 536 Train loss: 0.036687 Validation loss: 0.018026\n",
      "Epoch: 537 Train loss: 0.036681 Validation loss: 0.018005\n",
      "Epoch: 538 Train loss: 0.036675 Validation loss: 0.017984\n",
      "Epoch: 539 Train loss: 0.036669 Validation loss: 0.017964\n",
      "Epoch: 540 Train loss: 0.036664 Validation loss: 0.017944\n",
      "Epoch: 541 Train loss: 0.036658 Validation loss: 0.017924\n",
      "Epoch: 542 Train loss: 0.036652 Validation loss: 0.017904\n",
      "Epoch: 543 Train loss: 0.036647 Validation loss: 0.017884\n",
      "Epoch: 544 Train loss: 0.036641 Validation loss: 0.017865\n",
      "Epoch: 545 Train loss: 0.036636 Validation loss: 0.017846\n",
      "Epoch: 546 Train loss: 0.03663 Validation loss: 0.017827\n",
      "Epoch: 547 Train loss: 0.036625 Validation loss: 0.017808\n",
      "Epoch: 548 Train loss: 0.03662 Validation loss: 0.01779\n",
      "Epoch: 549 Train loss: 0.036614 Validation loss: 0.017772\n",
      "Epoch: 550 Train loss: 0.036609 Validation loss: 0.017753\n",
      "Epoch: 551 Train loss: 0.036604 Validation loss: 0.017736\n",
      "Epoch: 552 Train loss: 0.036599 Validation loss: 0.017718\n",
      "Epoch: 553 Train loss: 0.036594 Validation loss: 0.017701\n",
      "Epoch: 554 Train loss: 0.036589 Validation loss: 0.017683\n",
      "Epoch: 555 Train loss: 0.036584 Validation loss: 0.017666\n",
      "Epoch: 556 Train loss: 0.036579 Validation loss: 0.01765\n",
      "Epoch: 557 Train loss: 0.036574 Validation loss: 0.017634\n",
      "Epoch: 558 Train loss: 0.036569 Validation loss: 0.017617\n",
      "Epoch: 559 Train loss: 0.036564 Validation loss: 0.017602\n",
      "Epoch: 560 Train loss: 0.03656 Validation loss: 0.017586\n",
      "Epoch: 561 Train loss: 0.036555 Validation loss: 0.017571\n",
      "Epoch: 562 Train loss: 0.03655 Validation loss: 0.017556\n",
      "Epoch: 563 Train loss: 0.036546 Validation loss: 0.017541\n",
      "Epoch: 564 Train loss: 0.036541 Validation loss: 0.017526\n",
      "Epoch: 565 Train loss: 0.036537 Validation loss: 0.017512\n",
      "Epoch: 566 Train loss: 0.036532 Validation loss: 0.017498\n",
      "Epoch: 567 Train loss: 0.036528 Validation loss: 0.017484\n",
      "Epoch: 568 Train loss: 0.036524 Validation loss: 0.017471\n",
      "Epoch: 569 Train loss: 0.036519 Validation loss: 0.017458\n",
      "Epoch: 570 Train loss: 0.036515 Validation loss: 0.017445\n",
      "Epoch: 571 Train loss: 0.036511 Validation loss: 0.017433\n",
      "Epoch: 572 Train loss: 0.036507 Validation loss: 0.01742\n",
      "Epoch: 573 Train loss: 0.036503 Validation loss: 0.017408\n",
      "Epoch: 574 Train loss: 0.036499 Validation loss: 0.017397\n",
      "Epoch: 575 Train loss: 0.036495 Validation loss: 0.017386\n",
      "Epoch: 576 Train loss: 0.036491 Validation loss: 0.017375\n",
      "Epoch: 577 Train loss: 0.036487 Validation loss: 0.017364\n",
      "Epoch: 578 Train loss: 0.036483 Validation loss: 0.017354\n",
      "Epoch: 579 Train loss: 0.03648 Validation loss: 0.017343\n",
      "Epoch: 580 Train loss: 0.036476 Validation loss: 0.017334\n",
      "Epoch: 581 Train loss: 0.036472 Validation loss: 0.017324\n",
      "Epoch: 582 Train loss: 0.036469 Validation loss: 0.017315\n",
      "Epoch: 583 Train loss: 0.036465 Validation loss: 0.017306\n",
      "Epoch: 584 Train loss: 0.036462 Validation loss: 0.017298\n",
      "Epoch: 585 Train loss: 0.036458 Validation loss: 0.01729\n",
      "Epoch: 586 Train loss: 0.036455 Validation loss: 0.017282\n",
      "Epoch: 587 Train loss: 0.036451 Validation loss: 0.017274\n",
      "Epoch: 588 Train loss: 0.036448 Validation loss: 0.017267\n",
      "Epoch: 589 Train loss: 0.036445 Validation loss: 0.01726\n",
      "Epoch: 590 Train loss: 0.036442 Validation loss: 0.017254\n",
      "Epoch: 591 Train loss: 0.036438 Validation loss: 0.017248\n",
      "Epoch: 592 Train loss: 0.036435 Validation loss: 0.017242\n",
      "Epoch: 593 Train loss: 0.036432 Validation loss: 0.017236\n",
      "Epoch: 594 Train loss: 0.036429 Validation loss: 0.017231\n",
      "Epoch: 595 Train loss: 0.036426 Validation loss: 0.017226\n",
      "Epoch: 596 Train loss: 0.036423 Validation loss: 0.017222\n",
      "Epoch: 597 Train loss: 0.036421 Validation loss: 0.017218\n",
      "Epoch: 598 Train loss: 0.036418 Validation loss: 0.017214\n",
      "Epoch: 599 Train loss: 0.036415 Validation loss: 0.017211\n",
      "Epoch: 600 Train loss: 0.036412 Validation loss: 0.017207\n",
      "Epoch: 601 Train loss: 0.03641 Validation loss: 0.017205\n",
      "Epoch: 602 Train loss: 0.036407 Validation loss: 0.017202\n",
      "Epoch: 603 Train loss: 0.036405 Validation loss: 0.0172\n",
      "Epoch: 604 Train loss: 0.036402 Validation loss: 0.017199\n",
      "Epoch: 605 Train loss: 0.0364 Validation loss: 0.017197\n",
      "Epoch: 606 Train loss: 0.036397 Validation loss: 0.017196\n",
      "Epoch: 607 Train loss: 0.036395 Validation loss: 0.017196\n",
      "Epoch: 608 Train loss: 0.036392 Validation loss: 0.017196\n",
      "Epoch: 609 Train loss: 0.03639 Validation loss: 0.017196\n",
      "Epoch: 610 Train loss: 0.036388 Validation loss: 0.017197\n",
      "Epoch: 611 Train loss: 0.036386 Validation loss: 0.017197\n",
      "Epoch: 612 Train loss: 0.036384 Validation loss: 0.017199\n",
      "Epoch: 613 Train loss: 0.036382 Validation loss: 0.017201\n",
      "Epoch: 614 Train loss: 0.036379 Validation loss: 0.017203\n",
      "Epoch: 615 Train loss: 0.036377 Validation loss: 0.017205\n",
      "Epoch: 616 Train loss: 0.036376 Validation loss: 0.017208\n",
      "Epoch: 617 Train loss: 0.036374 Validation loss: 0.017211\n",
      "Epoch: 618 Train loss: 0.036372 Validation loss: 0.017215\n",
      "Epoch: 619 Train loss: 0.03637 Validation loss: 0.017219\n",
      "Epoch: 620 Train loss: 0.036368 Validation loss: 0.017224\n",
      "Epoch: 621 Train loss: 0.036366 Validation loss: 0.017228\n",
      "Epoch: 622 Train loss: 0.036365 Validation loss: 0.017234\n",
      "Epoch: 623 Train loss: 0.036363 Validation loss: 0.017239\n",
      "Epoch: 624 Train loss: 0.036362 Validation loss: 0.017245\n",
      "Epoch: 625 Train loss: 0.03636 Validation loss: 0.017252\n",
      "Epoch: 626 Train loss: 0.036358 Validation loss: 0.017259\n",
      "Epoch: 627 Train loss: 0.036357 Validation loss: 0.017266\n",
      "Epoch: 628 Train loss: 0.036356 Validation loss: 0.017273\n",
      "Epoch: 629 Train loss: 0.036354 Validation loss: 0.017281\n",
      "Epoch: 630 Train loss: 0.036353 Validation loss: 0.01729\n",
      "Epoch: 631 Train loss: 0.036351 Validation loss: 0.017299\n",
      "Epoch: 632 Train loss: 0.03635 Validation loss: 0.017308\n",
      "Epoch: 633 Train loss: 0.036349 Validation loss: 0.017318\n",
      "Epoch: 634 Train loss: 0.036348 Validation loss: 0.017328\n",
      "Epoch: 635 Train loss: 0.036347 Validation loss: 0.017338\n",
      "Epoch: 636 Train loss: 0.036345 Validation loss: 0.017349\n",
      "Epoch: 637 Train loss: 0.036344 Validation loss: 0.01736\n",
      "Epoch: 638 Train loss: 0.036343 Validation loss: 0.017372\n",
      "Epoch: 639 Train loss: 0.036342 Validation loss: 0.017384\n",
      "Epoch: 640 Train loss: 0.036341 Validation loss: 0.017396\n",
      "Epoch: 641 Train loss: 0.03634 Validation loss: 0.017409\n",
      "Epoch: 642 Train loss: 0.03634 Validation loss: 0.017422\n",
      "Epoch: 643 Train loss: 0.036339 Validation loss: 0.017435\n",
      "Epoch: 644 Train loss: 0.036338 Validation loss: 0.017449\n",
      "Epoch: 645 Train loss: 0.036337 Validation loss: 0.017464\n",
      "Epoch: 646 Train loss: 0.036336 Validation loss: 0.017478\n",
      "Epoch: 647 Train loss: 0.036336 Validation loss: 0.017493\n",
      "Epoch: 648 Train loss: 0.036335 Validation loss: 0.017508\n",
      "Epoch: 649 Train loss: 0.036334 Validation loss: 0.017524\n",
      "Epoch: 650 Train loss: 0.036334 Validation loss: 0.01754\n",
      "Epoch: 651 Train loss: 0.036333 Validation loss: 0.017557\n",
      "Epoch: 652 Train loss: 0.036333 Validation loss: 0.017573\n",
      "Epoch: 653 Train loss: 0.036332 Validation loss: 0.01759\n",
      "Epoch: 654 Train loss: 0.036332 Validation loss: 0.017608\n",
      "Epoch: 655 Train loss: 0.036331 Validation loss: 0.017625\n",
      "Epoch: 656 Train loss: 0.036331 Validation loss: 0.017643\n",
      "Epoch: 657 Train loss: 0.03633 Validation loss: 0.017662\n",
      "Epoch: 658 Train loss: 0.03633 Validation loss: 0.01768\n",
      "Epoch: 659 Train loss: 0.03633 Validation loss: 0.017699\n",
      "Epoch: 660 Train loss: 0.03633 Validation loss: 0.017719\n",
      "Epoch: 661 Train loss: 0.036329 Validation loss: 0.017738\n",
      "Epoch: 662 Train loss: 0.036329 Validation loss: 0.017758\n",
      "Epoch: 663 Train loss: 0.036329 Validation loss: 0.017778\n",
      "Epoch: 664 Train loss: 0.036329 Validation loss: 0.017799\n",
      "Epoch: 665 Train loss: 0.036329 Validation loss: 0.017819\n",
      "Epoch: 666 Train loss: 0.036329 Validation loss: 0.01784\n",
      "Epoch: 667 Train loss: 0.036329 Validation loss: 0.017862\n",
      "Epoch: 668 Train loss: 0.036328 Validation loss: 0.017883\n",
      "Epoch: 669 Train loss: 0.036329 Validation loss: 0.017905\n",
      "Epoch: 670 Train loss: 0.036329 Validation loss: 0.017927\n",
      "Epoch: 671 Train loss: 0.036329 Validation loss: 0.017949\n",
      "Epoch: 672 Train loss: 0.036329 Validation loss: 0.017972\n",
      "Epoch: 673 Train loss: 0.036329 Validation loss: 0.017994\n",
      "Epoch: 674 Train loss: 0.036329 Validation loss: 0.018017\n",
      "Epoch: 675 Train loss: 0.036329 Validation loss: 0.018041\n",
      "Epoch: 676 Train loss: 0.036329 Validation loss: 0.018064\n",
      "Epoch: 677 Train loss: 0.03633 Validation loss: 0.018088\n",
      "Epoch: 678 Train loss: 0.03633 Validation loss: 0.018112\n",
      "Epoch: 679 Train loss: 0.03633 Validation loss: 0.018135\n",
      "Epoch: 680 Train loss: 0.036331 Validation loss: 0.01816\n",
      "Epoch: 681 Train loss: 0.036331 Validation loss: 0.018184\n",
      "Epoch: 682 Train loss: 0.036331 Validation loss: 0.018209\n",
      "Epoch: 683 Train loss: 0.036332 Validation loss: 0.018234\n",
      "Epoch: 684 Train loss: 0.036332 Validation loss: 0.018259\n",
      "Epoch: 685 Train loss: 0.036333 Validation loss: 0.018284\n",
      "Epoch: 686 Train loss: 0.036333 Validation loss: 0.01831\n",
      "Epoch: 687 Train loss: 0.036334 Validation loss: 0.018335\n",
      "Epoch: 688 Train loss: 0.036334 Validation loss: 0.018361\n",
      "Epoch: 689 Train loss: 0.036335 Validation loss: 0.018387\n",
      "Epoch: 690 Train loss: 0.036336 Validation loss: 0.018413\n",
      "Epoch: 691 Train loss: 0.036336 Validation loss: 0.018439\n",
      "Epoch: 692 Train loss: 0.036337 Validation loss: 0.018466\n",
      "Epoch: 693 Train loss: 0.036338 Validation loss: 0.018492\n",
      "Epoch: 694 Train loss: 0.036338 Validation loss: 0.018519\n",
      "Epoch: 695 Train loss: 0.036339 Validation loss: 0.018546\n",
      "Epoch: 696 Train loss: 0.03634 Validation loss: 0.018573\n",
      "Epoch: 697 Train loss: 0.036341 Validation loss: 0.0186\n",
      "Epoch: 698 Train loss: 0.036341 Validation loss: 0.018627\n",
      "Epoch: 699 Train loss: 0.036342 Validation loss: 0.018655\n",
      "Epoch: 700 Train loss: 0.036343 Validation loss: 0.018682\n",
      "Epoch: 701 Train loss: 0.036344 Validation loss: 0.01871\n",
      "Epoch: 702 Train loss: 0.036345 Validation loss: 0.018738\n",
      "Epoch: 703 Train loss: 0.036346 Validation loss: 0.018766\n",
      "Epoch: 704 Train loss: 0.036347 Validation loss: 0.018794\n",
      "Epoch: 705 Train loss: 0.036348 Validation loss: 0.018822\n",
      "Epoch: 706 Train loss: 0.036349 Validation loss: 0.018851\n",
      "Epoch: 707 Train loss: 0.03635 Validation loss: 0.018879\n",
      "Epoch: 708 Train loss: 0.036351 Validation loss: 0.018908\n",
      "Epoch: 709 Train loss: 0.036352 Validation loss: 0.018937\n",
      "Epoch: 710 Train loss: 0.036353 Validation loss: 0.018966\n",
      "Epoch: 711 Train loss: 0.036354 Validation loss: 0.018995\n",
      "Epoch: 712 Train loss: 0.036355 Validation loss: 0.019024\n",
      "Epoch: 713 Train loss: 0.036356 Validation loss: 0.019053\n",
      "Epoch: 714 Train loss: 0.036357 Validation loss: 0.019082\n",
      "Epoch: 715 Train loss: 0.036358 Validation loss: 0.019112\n",
      "Epoch: 716 Train loss: 0.036359 Validation loss: 0.019141\n",
      "Epoch: 717 Train loss: 0.03636 Validation loss: 0.019171\n",
      "Epoch: 718 Train loss: 0.036361 Validation loss: 0.019201\n",
      "Epoch: 719 Train loss: 0.036362 Validation loss: 0.019231\n",
      "Epoch: 720 Train loss: 0.036363 Validation loss: 0.019261\n",
      "Epoch: 721 Train loss: 0.036364 Validation loss: 0.019291\n",
      "Epoch: 722 Train loss: 0.036365 Validation loss: 0.019322\n",
      "Epoch: 723 Train loss: 0.036366 Validation loss: 0.019352\n",
      "Epoch: 724 Train loss: 0.036368 Validation loss: 0.019383\n",
      "Epoch: 725 Train loss: 0.036369 Validation loss: 0.019413\n",
      "Epoch: 726 Train loss: 0.03637 Validation loss: 0.019444\n",
      "Epoch: 727 Train loss: 0.036371 Validation loss: 0.019475\n",
      "Epoch: 728 Train loss: 0.036372 Validation loss: 0.019506\n",
      "Epoch: 729 Train loss: 0.036373 Validation loss: 0.019537\n",
      "Epoch: 730 Train loss: 0.036374 Validation loss: 0.019568\n",
      "Epoch: 731 Train loss: 0.036375 Validation loss: 0.0196\n",
      "Epoch: 732 Train loss: 0.036376 Validation loss: 0.019631\n",
      "Epoch: 733 Train loss: 0.036377 Validation loss: 0.019663\n",
      "Epoch: 734 Train loss: 0.036378 Validation loss: 0.019695\n",
      "Epoch: 735 Train loss: 0.036378 Validation loss: 0.019727\n",
      "Epoch: 736 Train loss: 0.036379 Validation loss: 0.019759\n",
      "Epoch: 737 Train loss: 0.03638 Validation loss: 0.019791\n",
      "Epoch: 738 Train loss: 0.036381 Validation loss: 0.019823\n",
      "Epoch: 739 Train loss: 0.036382 Validation loss: 0.019855\n",
      "Epoch: 740 Train loss: 0.036382 Validation loss: 0.019888\n",
      "Epoch: 741 Train loss: 0.036383 Validation loss: 0.01992\n",
      "Epoch: 742 Train loss: 0.036384 Validation loss: 0.019953\n",
      "Epoch: 743 Train loss: 0.036384 Validation loss: 0.019986\n",
      "Epoch: 744 Train loss: 0.036385 Validation loss: 0.020019\n",
      "Epoch: 745 Train loss: 0.036386 Validation loss: 0.020052\n",
      "Epoch: 746 Train loss: 0.036386 Validation loss: 0.020085\n",
      "Epoch: 747 Train loss: 0.036387 Validation loss: 0.020119\n",
      "Epoch: 748 Train loss: 0.036387 Validation loss: 0.020152\n",
      "Epoch: 749 Train loss: 0.036387 Validation loss: 0.020186\n",
      "Epoch: 750 Train loss: 0.036388 Validation loss: 0.02022\n",
      "Epoch: 751 Train loss: 0.036388 Validation loss: 0.020254\n",
      "Epoch: 752 Train loss: 0.036388 Validation loss: 0.020288\n",
      "Epoch: 753 Train loss: 0.036388 Validation loss: 0.020322\n",
      "Epoch: 754 Train loss: 0.036388 Validation loss: 0.020357\n",
      "Epoch: 755 Train loss: 0.036388 Validation loss: 0.020391\n",
      "Epoch: 756 Train loss: 0.036388 Validation loss: 0.020426\n",
      "Epoch: 757 Train loss: 0.036388 Validation loss: 0.020461\n",
      "Epoch: 758 Train loss: 0.036388 Validation loss: 0.020496\n",
      "Epoch: 759 Train loss: 0.036387 Validation loss: 0.020531\n",
      "Epoch: 760 Train loss: 0.036387 Validation loss: 0.020566\n",
      "Epoch: 761 Train loss: 0.036386 Validation loss: 0.020602\n",
      "Epoch: 762 Train loss: 0.036386 Validation loss: 0.020637\n",
      "Epoch: 763 Train loss: 0.036385 Validation loss: 0.020673\n",
      "Epoch: 764 Train loss: 0.036384 Validation loss: 0.020709\n",
      "Epoch: 765 Train loss: 0.036383 Validation loss: 0.020745\n",
      "Epoch: 766 Train loss: 0.036382 Validation loss: 0.020781\n",
      "Epoch: 767 Train loss: 0.036381 Validation loss: 0.020817\n",
      "Epoch: 768 Train loss: 0.03638 Validation loss: 0.020854\n",
      "Epoch: 769 Train loss: 0.036378 Validation loss: 0.020891\n",
      "Epoch: 770 Train loss: 0.036377 Validation loss: 0.020927\n",
      "Epoch: 771 Train loss: 0.036375 Validation loss: 0.020964\n",
      "Epoch: 772 Train loss: 0.036374 Validation loss: 0.021002\n",
      "Epoch: 773 Train loss: 0.036372 Validation loss: 0.021039\n",
      "Epoch: 774 Train loss: 0.03637 Validation loss: 0.021076\n",
      "Epoch: 775 Train loss: 0.036368 Validation loss: 0.021114\n",
      "Epoch: 776 Train loss: 0.036365 Validation loss: 0.021151\n",
      "Epoch: 777 Train loss: 0.036363 Validation loss: 0.021189\n",
      "Epoch: 778 Train loss: 0.03636 Validation loss: 0.021227\n",
      "Epoch: 779 Train loss: 0.036358 Validation loss: 0.021265\n",
      "Epoch: 780 Train loss: 0.036355 Validation loss: 0.021303\n",
      "Epoch: 781 Train loss: 0.036352 Validation loss: 0.021342\n",
      "Epoch: 782 Train loss: 0.036348 Validation loss: 0.02138\n",
      "Epoch: 783 Train loss: 0.036345 Validation loss: 0.021419\n",
      "Epoch: 784 Train loss: 0.036341 Validation loss: 0.021457\n",
      "Epoch: 785 Train loss: 0.036337 Validation loss: 0.021496\n",
      "Epoch: 786 Train loss: 0.036333 Validation loss: 0.021535\n",
      "Epoch: 787 Train loss: 0.036329 Validation loss: 0.021574\n",
      "Epoch: 788 Train loss: 0.036324 Validation loss: 0.021613\n",
      "Epoch: 789 Train loss: 0.03632 Validation loss: 0.021652\n",
      "Epoch: 790 Train loss: 0.036315 Validation loss: 0.021691\n",
      "Epoch: 791 Train loss: 0.03631 Validation loss: 0.021731\n",
      "Epoch: 792 Train loss: 0.036304 Validation loss: 0.02177\n",
      "Epoch: 793 Train loss: 0.036299 Validation loss: 0.021809\n",
      "Epoch: 794 Train loss: 0.036293 Validation loss: 0.021848\n",
      "Epoch: 795 Train loss: 0.036287 Validation loss: 0.021888\n",
      "Epoch: 796 Train loss: 0.03628 Validation loss: 0.021927\n",
      "Epoch: 797 Train loss: 0.036274 Validation loss: 0.021966\n",
      "Epoch: 798 Train loss: 0.036267 Validation loss: 0.022006\n",
      "Epoch: 799 Train loss: 0.036259 Validation loss: 0.022045\n",
      "Epoch: 800 Train loss: 0.036252 Validation loss: 0.022084\n",
      "Epoch: 801 Train loss: 0.036244 Validation loss: 0.022123\n",
      "Epoch: 802 Train loss: 0.036236 Validation loss: 0.022162\n",
      "Epoch: 803 Train loss: 0.036228 Validation loss: 0.022201\n",
      "Epoch: 804 Train loss: 0.036219 Validation loss: 0.02224\n",
      "Epoch: 805 Train loss: 0.03621 Validation loss: 0.022279\n",
      "Epoch: 806 Train loss: 0.036201 Validation loss: 0.022317\n",
      "Epoch: 807 Train loss: 0.036191 Validation loss: 0.022356\n",
      "Epoch: 808 Train loss: 0.036181 Validation loss: 0.022394\n",
      "Epoch: 809 Train loss: 0.036171 Validation loss: 0.022432\n",
      "Epoch: 810 Train loss: 0.03616 Validation loss: 0.022469\n",
      "Epoch: 811 Train loss: 0.036149 Validation loss: 0.022507\n",
      "Epoch: 812 Train loss: 0.036138 Validation loss: 0.022544\n",
      "Epoch: 813 Train loss: 0.036127 Validation loss: 0.022582\n",
      "Epoch: 814 Train loss: 0.036115 Validation loss: 0.022618\n",
      "Epoch: 815 Train loss: 0.036103 Validation loss: 0.022655\n",
      "Epoch: 816 Train loss: 0.036091 Validation loss: 0.022691\n",
      "Epoch: 817 Train loss: 0.036078 Validation loss: 0.022727\n",
      "Epoch: 818 Train loss: 0.036065 Validation loss: 0.022762\n",
      "Epoch: 819 Train loss: 0.036052 Validation loss: 0.022798\n",
      "Epoch: 820 Train loss: 0.036039 Validation loss: 0.022833\n",
      "Epoch: 821 Train loss: 0.036025 Validation loss: 0.022868\n",
      "Epoch: 822 Train loss: 0.036011 Validation loss: 0.022903\n",
      "Epoch: 823 Train loss: 0.035997 Validation loss: 0.022937\n",
      "Epoch: 824 Train loss: 0.035983 Validation loss: 0.022971\n",
      "Epoch: 825 Train loss: 0.035969 Validation loss: 0.023005\n",
      "Epoch: 826 Train loss: 0.035954 Validation loss: 0.023039\n",
      "Epoch: 827 Train loss: 0.03594 Validation loss: 0.023072\n",
      "Epoch: 828 Train loss: 0.035925 Validation loss: 0.023106\n",
      "Epoch: 829 Train loss: 0.035911 Validation loss: 0.023139\n",
      "Epoch: 830 Train loss: 0.035896 Validation loss: 0.023172\n",
      "Epoch: 831 Train loss: 0.035882 Validation loss: 0.023206\n",
      "Epoch: 832 Train loss: 0.035867 Validation loss: 0.023239\n",
      "Epoch: 833 Train loss: 0.035853 Validation loss: 0.023272\n",
      "Epoch: 834 Train loss: 0.035839 Validation loss: 0.023305\n",
      "Epoch: 835 Train loss: 0.035824 Validation loss: 0.023339\n",
      "Epoch: 836 Train loss: 0.035811 Validation loss: 0.023373\n",
      "Epoch: 837 Train loss: 0.035797 Validation loss: 0.023407\n",
      "Epoch: 838 Train loss: 0.035783 Validation loss: 0.023442\n",
      "Epoch: 839 Train loss: 0.03577 Validation loss: 0.023477\n",
      "Epoch: 840 Train loss: 0.035757 Validation loss: 0.023512\n",
      "Epoch: 841 Train loss: 0.035745 Validation loss: 0.023548\n",
      "Epoch: 842 Train loss: 0.035732 Validation loss: 0.023585\n",
      "Epoch: 843 Train loss: 0.03572 Validation loss: 0.023622\n",
      "Epoch: 844 Train loss: 0.035709 Validation loss: 0.023659\n",
      "Epoch: 845 Train loss: 0.035698 Validation loss: 0.023698\n",
      "Epoch: 846 Train loss: 0.035687 Validation loss: 0.023738\n",
      "Epoch: 847 Train loss: 0.035677 Validation loss: 0.023778\n",
      "Epoch: 848 Train loss: 0.035667 Validation loss: 0.023819\n",
      "Epoch: 849 Train loss: 0.035658 Validation loss: 0.023861\n",
      "Epoch: 850 Train loss: 0.035649 Validation loss: 0.023903\n",
      "Epoch: 851 Train loss: 0.035641 Validation loss: 0.023947\n",
      "Epoch: 852 Train loss: 0.035633 Validation loss: 0.023992\n",
      "Epoch: 853 Train loss: 0.035626 Validation loss: 0.024038\n",
      "Epoch: 854 Train loss: 0.035619 Validation loss: 0.024084\n",
      "Epoch: 855 Train loss: 0.035612 Validation loss: 0.024132\n",
      "Epoch: 856 Train loss: 0.035606 Validation loss: 0.024181\n",
      "Epoch: 857 Train loss: 0.035601 Validation loss: 0.024231\n",
      "Epoch: 858 Train loss: 0.035596 Validation loss: 0.024281\n",
      "Epoch: 859 Train loss: 0.035592 Validation loss: 0.024333\n",
      "Epoch: 860 Train loss: 0.035588 Validation loss: 0.024386\n",
      "Epoch: 861 Train loss: 0.035584 Validation loss: 0.02444\n",
      "Epoch: 862 Train loss: 0.035581 Validation loss: 0.024494\n",
      "Epoch: 863 Train loss: 0.035579 Validation loss: 0.02455\n",
      "Epoch: 864 Train loss: 0.035577 Validation loss: 0.024607\n",
      "Epoch: 865 Train loss: 0.035575 Validation loss: 0.024664\n",
      "Epoch: 866 Train loss: 0.035574 Validation loss: 0.024723\n",
      "Epoch: 867 Train loss: 0.035573 Validation loss: 0.024782\n",
      "Epoch: 868 Train loss: 0.035573 Validation loss: 0.024842\n",
      "Epoch: 869 Train loss: 0.035573 Validation loss: 0.024903\n",
      "Epoch: 870 Train loss: 0.035573 Validation loss: 0.024965\n",
      "Epoch: 871 Train loss: 0.035574 Validation loss: 0.025027\n",
      "Epoch: 872 Train loss: 0.035575 Validation loss: 0.02509\n",
      "Epoch: 873 Train loss: 0.035577 Validation loss: 0.025154\n",
      "Epoch: 874 Train loss: 0.035579 Validation loss: 0.025218\n",
      "Epoch: 875 Train loss: 0.035581 Validation loss: 0.025283\n",
      "Epoch: 876 Train loss: 0.035584 Validation loss: 0.025348\n",
      "Epoch: 877 Train loss: 0.035587 Validation loss: 0.025414\n",
      "Epoch: 878 Train loss: 0.03559 Validation loss: 0.02548\n",
      "Epoch: 879 Train loss: 0.035594 Validation loss: 0.025547\n",
      "Epoch: 880 Train loss: 0.035598 Validation loss: 0.025614\n",
      "Epoch: 881 Train loss: 0.035602 Validation loss: 0.025682\n",
      "Epoch: 882 Train loss: 0.035607 Validation loss: 0.02575\n",
      "Epoch: 883 Train loss: 0.035612 Validation loss: 0.025818\n",
      "Epoch: 884 Train loss: 0.035617 Validation loss: 0.025887\n",
      "Epoch: 885 Train loss: 0.035622 Validation loss: 0.025955\n",
      "Epoch: 886 Train loss: 0.035628 Validation loss: 0.026025\n",
      "Epoch: 887 Train loss: 0.035634 Validation loss: 0.026094\n",
      "Epoch: 888 Train loss: 0.03564 Validation loss: 0.026163\n",
      "Epoch: 889 Train loss: 0.035647 Validation loss: 0.026232\n",
      "Epoch: 890 Train loss: 0.035654 Validation loss: 0.026302\n",
      "Epoch: 891 Train loss: 0.035661 Validation loss: 0.026372\n",
      "Epoch: 892 Train loss: 0.035668 Validation loss: 0.026441\n",
      "Epoch: 893 Train loss: 0.035676 Validation loss: 0.026511\n",
      "Epoch: 894 Train loss: 0.035683 Validation loss: 0.026581\n",
      "Epoch: 895 Train loss: 0.035691 Validation loss: 0.02665\n",
      "Epoch: 896 Train loss: 0.0357 Validation loss: 0.02672\n",
      "Epoch: 897 Train loss: 0.035708 Validation loss: 0.026789\n",
      "Epoch: 898 Train loss: 0.035717 Validation loss: 0.026859\n",
      "Epoch: 899 Train loss: 0.035726 Validation loss: 0.026928\n",
      "Epoch: 900 Train loss: 0.035735 Validation loss: 0.026998\n",
      "Epoch: 901 Train loss: 0.035744 Validation loss: 0.027067\n",
      "Epoch: 902 Train loss: 0.035753 Validation loss: 0.027136\n",
      "Epoch: 903 Train loss: 0.035763 Validation loss: 0.027205\n",
      "Epoch: 904 Train loss: 0.035773 Validation loss: 0.027274\n",
      "Epoch: 905 Train loss: 0.035783 Validation loss: 0.027342\n",
      "Epoch: 906 Train loss: 0.035793 Validation loss: 0.02741\n",
      "Epoch: 907 Train loss: 0.035804 Validation loss: 0.027478\n",
      "Epoch: 908 Train loss: 0.035814 Validation loss: 0.027546\n",
      "Epoch: 909 Train loss: 0.035825 Validation loss: 0.027614\n",
      "Epoch: 910 Train loss: 0.035836 Validation loss: 0.027681\n",
      "Epoch: 911 Train loss: 0.035847 Validation loss: 0.027748\n",
      "Epoch: 912 Train loss: 0.035859 Validation loss: 0.027815\n",
      "Epoch: 913 Train loss: 0.03587 Validation loss: 0.027882\n",
      "Epoch: 914 Train loss: 0.035882 Validation loss: 0.027948\n",
      "Epoch: 915 Train loss: 0.035893 Validation loss: 0.028014\n",
      "Epoch: 916 Train loss: 0.035905 Validation loss: 0.028079\n",
      "Epoch: 917 Train loss: 0.035918 Validation loss: 0.028145\n",
      "Epoch: 918 Train loss: 0.03593 Validation loss: 0.02821\n",
      "Epoch: 919 Train loss: 0.035942 Validation loss: 0.028274\n",
      "Epoch: 920 Train loss: 0.035955 Validation loss: 0.028339\n",
      "Epoch: 921 Train loss: 0.035967 Validation loss: 0.028403\n",
      "Epoch: 922 Train loss: 0.03598 Validation loss: 0.028467\n",
      "Epoch: 923 Train loss: 0.035993 Validation loss: 0.02853\n",
      "Epoch: 924 Train loss: 0.036006 Validation loss: 0.028593\n",
      "Epoch: 925 Train loss: 0.036019 Validation loss: 0.028655\n",
      "Epoch: 926 Train loss: 0.036033 Validation loss: 0.028718\n",
      "Epoch: 927 Train loss: 0.036046 Validation loss: 0.02878\n",
      "Epoch: 928 Train loss: 0.03606 Validation loss: 0.028841\n",
      "Epoch: 929 Train loss: 0.036073 Validation loss: 0.028903\n",
      "Epoch: 930 Train loss: 0.036087 Validation loss: 0.028964\n",
      "Epoch: 931 Train loss: 0.036101 Validation loss: 0.029024\n",
      "Epoch: 932 Train loss: 0.036115 Validation loss: 0.029084\n",
      "Epoch: 933 Train loss: 0.036129 Validation loss: 0.029144\n",
      "Epoch: 934 Train loss: 0.036144 Validation loss: 0.029204\n",
      "Epoch: 935 Train loss: 0.036158 Validation loss: 0.029263\n",
      "Epoch: 936 Train loss: 0.036173 Validation loss: 0.029321\n",
      "Epoch: 937 Train loss: 0.036187 Validation loss: 0.02938\n",
      "Epoch: 938 Train loss: 0.036202 Validation loss: 0.029438\n",
      "Epoch: 939 Train loss: 0.036217 Validation loss: 0.029496\n",
      "Epoch: 940 Train loss: 0.036231 Validation loss: 0.029553\n",
      "Epoch: 941 Train loss: 0.036246 Validation loss: 0.02961\n",
      "Epoch: 942 Train loss: 0.036261 Validation loss: 0.029666\n",
      "Epoch: 943 Train loss: 0.036277 Validation loss: 0.029723\n",
      "Epoch: 944 Train loss: 0.036292 Validation loss: 0.029778\n",
      "Epoch: 945 Train loss: 0.036307 Validation loss: 0.029834\n",
      "Epoch: 946 Train loss: 0.036323 Validation loss: 0.029889\n",
      "Epoch: 947 Train loss: 0.036338 Validation loss: 0.029944\n",
      "Epoch: 948 Train loss: 0.036354 Validation loss: 0.029998\n",
      "Epoch: 949 Train loss: 0.036369 Validation loss: 0.030052\n",
      "Epoch: 950 Train loss: 0.036385 Validation loss: 0.030106\n",
      "Epoch: 951 Train loss: 0.036401 Validation loss: 0.030159\n",
      "Epoch: 952 Train loss: 0.036417 Validation loss: 0.030212\n",
      "Epoch: 953 Train loss: 0.036433 Validation loss: 0.030265\n",
      "Epoch: 954 Train loss: 0.036449 Validation loss: 0.030317\n",
      "Epoch: 955 Train loss: 0.036465 Validation loss: 0.030369\n",
      "Epoch: 956 Train loss: 0.036481 Validation loss: 0.030421\n",
      "Epoch: 957 Train loss: 0.036497 Validation loss: 0.030472\n",
      "Epoch: 958 Train loss: 0.036513 Validation loss: 0.030523\n",
      "Epoch: 959 Train loss: 0.03653 Validation loss: 0.030574\n",
      "Epoch: 960 Train loss: 0.036546 Validation loss: 0.030624\n",
      "Epoch: 961 Train loss: 0.036562 Validation loss: 0.030674\n",
      "Epoch: 962 Train loss: 0.036579 Validation loss: 0.030724\n",
      "Epoch: 963 Train loss: 0.036596 Validation loss: 0.030773\n",
      "Epoch: 964 Train loss: 0.036612 Validation loss: 0.030822\n",
      "Epoch: 965 Train loss: 0.036629 Validation loss: 0.030871\n",
      "Epoch: 966 Train loss: 0.036646 Validation loss: 0.030919\n",
      "Epoch: 967 Train loss: 0.036662 Validation loss: 0.030968\n",
      "Epoch: 968 Train loss: 0.036679 Validation loss: 0.031015\n",
      "Epoch: 969 Train loss: 0.036696 Validation loss: 0.031063\n",
      "Epoch: 970 Train loss: 0.036713 Validation loss: 0.03111\n",
      "Epoch: 971 Train loss: 0.03673 Validation loss: 0.031157\n",
      "Epoch: 972 Train loss: 0.036747 Validation loss: 0.031203\n",
      "Epoch: 973 Train loss: 0.036764 Validation loss: 0.031249\n",
      "Epoch: 974 Train loss: 0.036781 Validation loss: 0.031295\n",
      "Epoch: 975 Train loss: 0.036798 Validation loss: 0.031341\n",
      "Epoch: 976 Train loss: 0.036815 Validation loss: 0.031387\n",
      "Epoch: 977 Train loss: 0.036832 Validation loss: 0.031432\n",
      "Epoch: 978 Train loss: 0.03685 Validation loss: 0.031477\n",
      "Epoch: 979 Train loss: 0.036867 Validation loss: 0.031521\n",
      "Epoch: 980 Train loss: 0.036884 Validation loss: 0.031565\n",
      "Epoch: 981 Train loss: 0.036901 Validation loss: 0.031609\n",
      "Epoch: 982 Train loss: 0.036919 Validation loss: 0.031653\n",
      "Epoch: 983 Train loss: 0.036936 Validation loss: 0.031696\n",
      "Epoch: 984 Train loss: 0.036953 Validation loss: 0.03174\n",
      "Epoch: 985 Train loss: 0.036971 Validation loss: 0.031782\n",
      "Epoch: 986 Train loss: 0.036988 Validation loss: 0.031825\n",
      "Epoch: 987 Train loss: 0.037006 Validation loss: 0.031868\n",
      "Epoch: 988 Train loss: 0.037023 Validation loss: 0.03191\n",
      "Epoch: 989 Train loss: 0.037041 Validation loss: 0.031952\n",
      "Epoch: 990 Train loss: 0.037058 Validation loss: 0.031993\n",
      "Epoch: 991 Train loss: 0.037076 Validation loss: 0.032035\n",
      "Epoch: 992 Train loss: 0.037094 Validation loss: 0.032076\n",
      "Epoch: 993 Train loss: 0.037111 Validation loss: 0.032117\n",
      "Epoch: 994 Train loss: 0.037129 Validation loss: 0.032157\n",
      "Epoch: 995 Train loss: 0.037146 Validation loss: 0.032198\n",
      "Epoch: 996 Train loss: 0.037164 Validation loss: 0.032238\n",
      "Epoch: 997 Train loss: 0.037182 Validation loss: 0.032278\n",
      "Epoch: 998 Train loss: 0.0372 Validation loss: 0.032317\n",
      "Epoch: 999 Train loss: 0.037217 Validation loss: 0.032357\n"
     ]
    }
   ],
   "source": [
    "lmbd = 0.005\n",
    "\n",
    "big_reg_mdl = MyBigModel()\n",
    "big_opt = tf.optimizers.SGD(learning_rate)\n",
    "\n",
    "epoch = 0\n",
    "train_iters = 0\n",
    "train_loss = 0.0\n",
    "for x_t, y_t in train_overfit_ds:\n",
    "    train_loss += regularized_train_step(big_reg_mdl, big_opt, x_t, y_t, lmbd)\n",
    "    train_iters += 1\n",
    "    if (train_iters >= int(N_train_samples/batch_size)):\n",
    "        for x_v, y_v in validation_ds:\n",
    "            y_pred = big_reg_mdl(x_v)\n",
    "            validation_loss = tf.reduce_mean(tf.square(y_v-y_pred))\n",
    "        print(\"Epoch: {} Train loss: {:.5} Validation loss: {:.5}\".format(epoch, train_loss/train_iters, validation_loss))\n",
    "        train_iters = 0\n",
    "        train_loss = 0.0\n",
    "        train_reg = 0.0\n",
    "        epoch += 1\n",
    "    if (epoch == N_epochs):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ylr1C2ovWXbO"
   },
   "source": [
    "During the training of the regularized model we can already notice, that, although there is still a difference between training and validation loss, the validation loss decreases as the training loss dreases. The effect of the regularization becomes even more evident if we plot the predictions of the regularized model and the overfitting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1086474,
     "status": "ok",
     "timestamp": 1562074371953,
     "user": {
      "displayName": "Felix Wiewel",
      "photoUrl": "https://lh3.googleusercontent.com/-XBgpqQQzeMU/AAAAAAAAAAI/AAAAAAAAAZY/12qzl-EFXfM/s64/photo.jpg",
      "userId": "11509421729312238102"
     },
     "user_tz": -120
    },
    "id": "UdMRV0vAWLmm",
    "outputId": "68ce20e5-75bc-4990-8c3e-a14f4116e6b4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = big_reg_mdl(x)\n",
    "y_pred_overfit = big_mdl(x)\n",
    "plt.scatter(x_train_overfit, y_train_overfit)\n",
    "plt.plot(x, y_true)\n",
    "plt.plot(x, y_pred.numpy())\n",
    "plt.plot(x, y_pred_overfit.numpy())\n",
    "plt.legend([\"Target\", \"Regularization\", \"No regularization\", \"Training samples\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sePQBaGqZzT6"
   },
   "source": [
    "The model with regularization seems to follow the overall trend of the data, while the model without any regularization very precisely fits the training samples. This is espacilly evident in the interval $\\left[0,0.5\\right]$, where the prediction of the unregularized model shows an oscillating behavior. Such oscillations are however not present in the ground truth and therefore undesirable. The regularized model on the other hand is not as flexible as the unregularized model and therefore does not fit the target function well in the interval $\\left[2.25, 3.0\\right]$.\n",
    "\n",
    "## Conclusion\n",
    "In this exercise we revisited the mathematical background for a simple regression task and covered it's practical implementation in Tensorflow 2. We also explored the phenomenon of overfitting and derived different regularizations from a probabilistic perspective. This exercise covers a very simple task with a very basic neural architecture and is intended as a primer for the second part of the regression exercise, which is dealing with a bigger and more realistic problem. In this second part we will consider the problem of regressing the age of a person from a potrait picture."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Simple_regression_solution.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
